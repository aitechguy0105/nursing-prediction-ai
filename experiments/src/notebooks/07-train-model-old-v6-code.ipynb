{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib --quiet\n",
    "!pip install plotly-express --quiet\n",
    "!pip install -U kaleido"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Restart kernel after the above packages are installed"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import json\n",
    "import pickle\n",
    "import sys\n",
    "import timeit\n",
    "from datetime import timedelta, datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# import hyperopt\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "from optuna.samplers import TPESampler\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "import mlflow\n",
    "from mlflow import log_metric, log_param, log_artifact, set_tag\n",
    "\n",
    "sys.path.insert(0, '/src')\n",
    "from shared.constants import CLIENT, VECTOR_MODELS, HYPER_PARAMETER_TUNING\n",
    "from shared.utils import get_client_class\n",
    "from dataclasses import dataclass\n",
    "from typing import Any\n",
    "\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "import plotly.offline as pyo\n",
    "\n",
    "pyo.init_notebook_mode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========= Set the CONFIG & HYPER_PARAMETER_TUNING in constants.py =========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clientClass = get_client_class(client=CLIENT)\n",
    "EXPERIMENT_DATES = getattr(clientClass(), 'get_experiment_dates')()\n",
    "\n",
    "# starting training from day 31 so that cumsum window 2,7,14,30 are all initial correct.\n",
    "EXPERIMENT_DATES['train_start_date'] = str((pd.to_datetime(EXPERIMENT_DATES['train_start_date']) +  pd.DateOffset(days=30)).date())\n",
    "\n",
    "\n",
    "TRAINING_DATA='Avante'   # trained on which data? e.g. avante + champion\n",
    "SELECTED_MODEL_VERSION = 'saiva-3-day-hosp-v6'    # e.g. v3, v4 or v6 model\n",
    "\n",
    "# Name used to filter models in AWS quicksight & also used as ML Flow experiment name\n",
    "MODEL_DESCRIPTION = 'avante-3-day-hosp-V6-test'  \n",
    "\n",
    "base_models = []\n",
    "model_config = {}\n",
    "\n",
    "\n",
    "# Change train_end_date, validation_start_date for Tuning run\n",
    "if not HYPER_PARAMETER_TUNING:\n",
    "    DESCRIPTION_STRING = 'TRAIN+VALID'\n",
    "    EXPERIMENT_DATES['train_end_date'] = (datetime.strptime(EXPERIMENT_DATES['validation_end_date'], '%Y-%m-%d') - timedelta(days=2)).strftime('%Y-%m-%d')\n",
    "    EXPERIMENT_DATES['validation_start_date'] = (datetime.strptime(EXPERIMENT_DATES['validation_end_date'], '%Y-%m-%d') - timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "else:\n",
    "    DESCRIPTION_STRING = 'TUNING'\n",
    "\n",
    "    \n",
    "def get_training_runs():\n",
    "    if HYPER_PARAMETER_TUNING:\n",
    "        return 9\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "print(HYPER_PARAMETER_TUNING)  \n",
    "print(CLIENT)\n",
    "EXPERIMENT_DATES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ============ Initialise MLFlow Experiment ============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an ML-flow experiment\n",
    "mlflow.set_tracking_uri('http://mlflow.saiva-dev')\n",
    "\n",
    "# Experiment name which appears in ML flow\n",
    "mlflow.set_experiment(MODEL_DESCRIPTION)\n",
    "\n",
    "EXPERIMENT = mlflow.get_experiment_by_name(MODEL_DESCRIPTION)\n",
    "EXPERIMENT_ID = EXPERIMENT.experiment_id\n",
    "\n",
    "print(f'Experiment ID: {EXPERIMENT_ID}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ========== Read Best parameters from previous MLFlow runs ==========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = mlflow.search_runs(experiment_ids=EXPERIMENT_ID)\n",
    "\n",
    "try:\n",
    "    best_run_config = df.sort_values(by=['params.p__best_score'], ascending = False).to_dict('records')[0]\n",
    "    BEST_AUC = best_run_config['params.p__best_score']\n",
    "    LEARNING_RATE = best_run_config['params.hp__learning_rate']\n",
    "    NUM_ITERATIONS = best_run_config['params.p__best_iteration']  # choose the best_iteration based on early_stopping\n",
    "    BEST_ITERATION = best_run_config['params.000_DESCRIPTION']\n",
    "except:\n",
    "    LEARNING_RATE = None\n",
    "    NUM_ITERATIONS = None\n",
    "    BEST_ITERATION = None\n",
    "    BEST_AUC = None\n",
    "    \n",
    "print(f'Best Tunning Iteration is {BEST_ITERATION} as its having AUC = {BEST_AUC}')\n",
    "print(NUM_ITERATIONS)\n",
    "print(LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hyper_parameters(trial):\n",
    "    lgb_param_space = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'verbose': 1,\n",
    "        'n_jobs': -1,\n",
    "        'is_unbalance': True\n",
    "        \n",
    "#     'is_unbalance': False,\n",
    "#     'learning_rate': 0.05,\n",
    "#     'num_iterations': 419,\n",
    "#     'scale_pos_weight': 14,\n",
    "#     'max_depth': 3,\n",
    "#     'num_leaves': 63, \n",
    "#     'bagging_fraction': 0.9069490690145419,\n",
    "#     'feature_fraction': 0.6775345127383569,\n",
    "#     'early_stopping_round': None,\n",
    "        \n",
    "#         'is_unbalance': False,\n",
    "#         'bagging_fraction': trial.suggest_float(\"bagging_fraction\", 0.4, 1),\n",
    "#         'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1),\n",
    "#         'learning_rate': trial.suggest_float(\"learning_rate\", 0.01, 0.05),\n",
    "#         'num_iterations': trial.suggest_int('num_iterations', 150, 500),\n",
    "#         'max_depth': int(trial.suggest_uniform('max_depth', 0, 63)),\n",
    "#         'scale_pos_weight': int(trial.suggest_uniform('scale_pos_weight', 1, 100)),\n",
    "#         'num_leaves': int(trial.suggest_categorical('num_leaves', [15, 31, 63, 127, 255, 511, 1023, 2047, 4095])), \n",
    "#         'early_stopping_round': int(trial.suggest_categorical('early_stopping_round', [10,20,50,100])),\n",
    "    }\n",
    "\n",
    "    if HYPER_PARAMETER_TUNING:\n",
    "        lgb_param_space['learning_rate'] = trial.suggest_float('learning_rate', 0.01, 0.05)\n",
    "        lgb_param_space['num_iterations'] = trial.suggest_int('num_iterations',108,405)  # aliases: n_estimators\n",
    "        lgb_param_space['early_stopping_round'] = 10\n",
    "    else:\n",
    "        lgb_param_space['learning_rate'] = float(LEARNING_RATE)\n",
    "        lgb_param_space['num_iterations'] = int(NUM_ITERATIONS)\n",
    "        lgb_param_space['early_stopping_round'] = None\n",
    "\n",
    "    return lgb_param_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## =================== CONFIG ENDS ======================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "processed_path = Path('/data/processed')\n",
    "processed_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open(processed_path/'final-train_x.pickle','rb') as f: train_x = pickle.load(f)\n",
    "with open(processed_path/'final-train_target_3_day.pickle','rb') as f: train_target_3_day = pickle.load(f)\n",
    "with open(processed_path/'final-train_target_7_day.pickle','rb') as f: train_target_7_day = pickle.load(f)\n",
    "with open(processed_path/'final-train_idens.pickle','rb') as f: train_idens = pickle.load(f)\n",
    "\n",
    "with open(processed_path/'final-valid_x.pickle','rb') as f: valid_x = pickle.load(f)\n",
    "with open(processed_path/'final-valid_target_3_day.pickle','rb') as f: valid_target_3_day = pickle.load(f)\n",
    "with open(processed_path/'final-valid_target_7_day.pickle','rb') as f: valid_target_7_day = pickle.load(f)\n",
    "with open(processed_path/'final-valid_idens.pickle','rb') as f: valid_idens = pickle.load(f)\n",
    "\n",
    "with open(processed_path/'final-test_x.pickle','rb') as f: test_x = pickle.load(f)\n",
    "with open(processed_path/'final-test_target_3_day.pickle','rb') as f: test_target_3_day = pickle.load(f)\n",
    "with open(processed_path/'final-test_target_7_day.pickle','rb') as f: test_target_7_day = pickle.load(f)\n",
    "with open(processed_path/'final-test_idens.pickle','rb') as f: test_idens = pickle.load(f)\n",
    "\n",
    "with open(processed_path/'final-na_filler.pickle', 'rb') as f: na_filler = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_x.shape)\n",
    "print(train_target_3_day.shape)\n",
    "print(valid_x.shape)\n",
    "print(valid_target_3_day.shape)\n",
    "print(test_x.shape)\n",
    "print(test_target_3_day.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('na_fillers column count: ', len(na_filler.keys()))\n",
    "print('feature column count: ', len(train_x.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_data = lgb.Dataset(train_x, label=train_target_3_day)\n",
    "valid_data = lgb.Dataset(valid_x, label=valid_target_3_day)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BaseModel:\n",
    "    model_name: str\n",
    "    model_type: str\n",
    "    model: Any\n",
    "\n",
    "\n",
    "def get_facilities_from_train_data(df):\n",
    "    return list(df.facilityid.unique())\n",
    "\n",
    "def get_date_diff(start_date, end_date):\n",
    "    diff = (pd.to_datetime(end_date) - pd.to_datetime(start_date)).days\n",
    "    return f'{diff}'\n",
    "\n",
    "def get_valid_data():\n",
    "    if HYPER_PARAMETER_TUNING:\n",
    "        return [valid_data]\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def get_pos_neg(np_series):\n",
    "    total = len(np_series)\n",
    "    neg = np.count_nonzero(np_series==0)\n",
    "    pos = np.count_nonzero(np_series==1)\n",
    "#     neg_p = round((neg * 100) / total, 3)\n",
    "#     pos_p = round((pos * 100) / total, 3)\n",
    "    return pos, neg, round(neg/pos, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" Calculate how many transfers were caught up to a particular rank.\n",
    "hospital_cumsum - how many transfers caught upto a certain rank. Eg: Caught transfers till 10 th rank\n",
    "Relavant - total transfers per day per facility\n",
    "\"\"\"\n",
    "\n",
    "def precision_recall_at_k(group):\n",
    "    group.loc[:, \"hospitalized_cumsum\"] = group.hospitalized_within_pred_range.cumsum()\n",
    "    group.loc[:, \"total_relevant\"] = group.hospitalized_within_pred_range.sum()\n",
    "    group.loc[:, \"recall_at_k\"] = group.hospitalized_cumsum / group.total_relevant\n",
    "\n",
    "    return group.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def performance_base_processing(idens, preds, target_3_day):\n",
    "    base = idens.copy()\n",
    "    base['predictionvalue'] = preds\n",
    "    base['hospitalized_within_pred_range'] = target_3_day\n",
    "    base['predictionrank'] = base.groupby(['censusdate', 'facilityid']).predictionvalue.rank(ascending=False)\n",
    "    base = base.sort_values('predictionrank', ascending=True)\n",
    "\n",
    "    performance_base = (\n",
    "        base.groupby([\"facilityid\", \"censusdate\"])\n",
    "        .apply(precision_recall_at_k)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # Get max rank per facility per census day & then get median among max rank across facility\n",
    "    # This is calcludated inorder to find the top 10%, 15% value for any given facility\n",
    "    facility_pats = performance_base.groupby(\n",
    "        ['censusdate','facilityid']\n",
    "    ).predictionrank.max().reset_index().groupby(\n",
    "        'facilityid'\n",
    "    ).predictionrank.median().reset_index()\n",
    "\n",
    "    return performance_base, facility_pats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_auc(target_3_day, preds):\n",
    "    total_aucroc = roc_auc_score(target_3_day, preds)\n",
    "    total_aucroc_25_fpr = roc_auc_score(target_3_day, preds, max_fpr=0.25)\n",
    "    total_ap = average_precision_score(target_3_day, preds)\n",
    "\n",
    "    return total_aucroc, total_aucroc_25_fpr, total_ap\n",
    "\n",
    "def get_pline_recall(performance_base):\n",
    "    at_rank_15 = performance_base.loc[performance_base.predictionrank == 15]\n",
    "    pline_recall_at_rank_15 = at_rank_15.hospitalized_cumsum.sum() / at_rank_15.total_relevant.sum()\n",
    "    \n",
    "    # Each day we predict 15 patients, total prediciton = 15 * number of census days in the dataset\n",
    "    pline_precision_at_rank_15 = at_rank_15.hospitalized_cumsum.sum() / (at_rank_15.shape[0] * 15)\n",
    "    pline_bscore = f_beta_score(pline_precision_at_rank_15, pline_recall_at_rank_15)\n",
    "\n",
    "    return pline_recall_at_rank_15, pline_precision_at_rank_15, pline_bscore\n",
    "\n",
    "\n",
    "def get_rth_recall(performance_base):\n",
    "    rth_df = performance_base.query('rth == 1')[['censusdate', 'facilityid', 'masterpatientid']]\n",
    "    print('Total RTHs = ',rth_df.shape[0])\n",
    "    rth_df.rename(columns={'censusdate': 'rth_censusdate'},\n",
    "          inplace=True, errors='raise')\n",
    "    \n",
    "    df = performance_base.merge(\n",
    "        rth_df, on=['facilityid', 'masterpatientid']\n",
    "    )\n",
    "    df = df[df.censusdate <= df.rth_censusdate]\n",
    "    df['date_diff'] = (df['rth_censusdate'] - df['censusdate']).dt.days\n",
    "    df = df[df.date_diff <= 3]\n",
    "    df['min_predictionrank'] = df.groupby(['rth_censusdate', 'facilityid', 'masterpatientid'])['predictionrank'].transform('min')\n",
    "    df = df.query('rth == 1')\n",
    "    print('Total RTHs after processing = ',df.shape[0])\n",
    "    recall = df.query('min_predictionrank <= 15').shape[0] / df.shape[0]\n",
    "    print('Total Recall = ',recall)\n",
    "    \n",
    "    _df = df.query('LFS <= 30')\n",
    "    recall_LE30 = _df.query('min_predictionrank <= 15').shape[0] / _df.shape[0]\n",
    "    \n",
    "    _df = df.query('LFS > 30')\n",
    "    recall_G30 = _df.query('min_predictionrank <= 15').shape[0] / _df.shape[0]\n",
    "    \n",
    "    return recall, recall_LE30, recall_G30\n",
    "\n",
    "\n",
    "def f_beta_score(precision, recall, beta=2):\n",
    "    return ((1+beta**2)*(precision*recall)) / ((beta**2)*precision + recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test_set(model, selected_modelid, run_id, test_start_date, test_end_date, x_df, target_3_day, idens, dataset = 'TEST',recall_plot=False):\n",
    "    \n",
    "    pos, neg, n2p_ratio = get_pos_neg(target_3_day)\n",
    "    \n",
    "    log_param(f'11_{dataset}_START_DATE', test_start_date)\n",
    "    log_param(f'12_{dataset}_END_DATE', test_end_date)\n",
    "    log_param(f'13_{dataset}_DURATION_days', get_date_diff(test_start_date, test_end_date))\n",
    "    log_param(f'14_{dataset}_POS_COUNT', pos)\n",
    "    log_param(f'15_{dataset}_NEG_COUNT', neg)\n",
    "    log_param(f'16_{dataset}_N2P_RATIO', n2p_ratio)\n",
    "    log_param(f'17_MODEL_DESCRIPTION', SELECTED_MODEL_VERSION)\n",
    "    log_param(f'18_MODEL_ID', selected_modelid)\n",
    "    \n",
    "    # ===========================Predict on test dataset=======================\n",
    "    preds = model.predict(x_df)\n",
    "    print(\"=============================Prediction completed...=============================\")\n",
    "    # =========================== TOTAL AUCROC on TEST SET ===========================\n",
    "    total_aucroc,total_aucroc_25_fpr, total_ap = get_auc(target_3_day, preds)\n",
    "\n",
    "    performance_test_base, facility_test_pats = performance_base_processing(idens, preds, target_3_day)\n",
    "\n",
    "    pline_recall_at_rank_15, pline_precision_at_rank_15, pline_bscore = get_pline_recall(\n",
    "        performance_test_base\n",
    "    )\n",
    "    recall, recall_LE30, recall_G30 = get_rth_recall(performance_test_base)\n",
    "    \n",
    "    log_metric(f'01_{dataset}_aucroc', total_aucroc)\n",
    "    log_metric(f'02_{dataset}_RTH_recall_at_rank_15', recall)\n",
    "    log_metric(f'03_{dataset}_RTH_recall_LOS_LE30_at_rank_15', recall_LE30)\n",
    "    log_metric(f'04_{dataset}_RTH_recall_LOS_G30_at_rank_15', recall_G30)\n",
    "    log_metric(f'05_{dataset}_Pline_recall_at_rank_15', pline_recall_at_rank_15)\n",
    "    log_metric(f'06_{dataset}_Pline_precision_at_rank_15', pline_precision_at_rank_15)\n",
    "    log_metric(f'07_{dataset}_Pline_bscore_at_rank_15',pline_bscore)\n",
    "    log_metric(f'08_{dataset}_ap', total_aucroc_25_fpr)\n",
    "    log_metric(f'09_{dataset}_aucroc_at_.25_fpr', total_ap)\n",
    "    \n",
    "    generate_auc_curve(target_3_day, preds, total_aucroc, run_id)\n",
    "    if recall_plot:\n",
    "        generate_recall_curve(performance_test_base, recall, test_start_date, test_end_date)\n",
    "    \n",
    "    return total_aucroc, pline_recall_at_rank_15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stay_length(staylength):\n",
    "    if staylength > 120:\n",
    "        return 120\n",
    "    else:\n",
    "        return staylength\n",
    "    \n",
    "    \n",
    "def get_line_graph(df, selectedField, label_name, color):\n",
    "    fig = px.line(\n",
    "        df, \n",
    "        y=[selectedField], \n",
    "        x=list(df.index), \n",
    "        labels={\n",
    "            'x':'Length Of Stay',\n",
    "            'y': 'Recall'\n",
    "        }\n",
    "    )\n",
    "    fig['data'][0]['line']['color']=color\n",
    "    newnames = {selectedField:label_name}\n",
    "    \n",
    "    fig.for_each_trace(lambda t: t.update(name = newnames[t.name]))\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def draw_graph(df, recall, sdate, edate):\n",
    "    rth = [0 for i in range(0,121)]  \n",
    "    caught_rth = [0 for i in range(0,121)]\n",
    "    missed_rth = [0 for i in range(0,121)]\n",
    "    for index, row in df.iterrows():\n",
    "        j = int(get_stay_length(row['LFS']))\n",
    "        rth[j] += 1\n",
    "        if row['min_predictionrank'] <= 15:\n",
    "            caught_rth[j] += 1\n",
    "        if row['min_predictionrank'] > 15:\n",
    "            missed_rth[j] += 1\n",
    "    \n",
    "    # create a dataframe from the above 4 lists\n",
    "    metric_df = pd.DataFrame ({\"rth\": rth, \"caught_rth\": caught_rth, \"missed_rth\": missed_rth})\n",
    "    metric_df['recall'] = recall\n",
    "    cumsum_df = metric_df[['caught_rth','rth']].cumsum()\n",
    "    cumsum_df['fraction_rth'] = cumsum_df['caught_rth'] / cumsum_df['rth'] ## harsh def for recall\n",
    "    reverse_cumsum_df = pd.DataFrame()\n",
    "    reverse_cumsum_df['caught_rth'] = metric_df.loc[::-1, 'caught_rth'].cumsum()[::-1]\n",
    "    reverse_cumsum_df['rth'] = metric_df.loc[::-1, 'rth'].cumsum()[::-1]\n",
    "    reverse_cumsum_df['reverse_fraction_rth'] = reverse_cumsum_df['caught_rth'] / reverse_cumsum_df['rth']  ## harsh def\n",
    "\n",
    "    metric_df['reverse_fraction_rth'] = reverse_cumsum_df['reverse_fraction_rth']\n",
    "    metric_df['fraction_rth'] = cumsum_df['fraction_rth']\n",
    "    \n",
    "    print(reverse_cumsum_df['reverse_fraction_rth'].shape)\n",
    "    print(cumsum_df['fraction_rth'].shape)    \n",
    "    \n",
    "    fig = make_subplots()\n",
    "\n",
    "    plot1 = get_line_graph(metric_df,'fraction_rth', 'Recall at LOS <= x axis', 'blue')\n",
    "    plot2 = get_line_graph(metric_df,'reverse_fraction_rth', 'Recall at LOS > x axis', 'red') \n",
    "    \n",
    "    \n",
    "    fig.add_trace(\n",
    "        plot1[\"data\"][0]\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        plot2[\"data\"][0]\n",
    "    )\n",
    "    \n",
    "    fig.add_vline(x=30, \n",
    "                  line_width=3, \n",
    "                  line_dash=\"dot\", \n",
    "                  line_color=\"black\",\n",
    "                  annotation_text='LOS=30'\n",
    "                 )# 30 LOS\n",
    "    \n",
    "    recall = round(recall, 2)\n",
    "    fig.add_hline(y=recall,\n",
    "                  line_width=3, \n",
    "                  line_dash=\"dot\", \n",
    "                  line_color=\"green\",\n",
    "                  annotation_text=f'Total Recall={recall}'\n",
    "                 ) # Total Recall Indicator\n",
    "    \n",
    "    \n",
    "    facilities = get_facilities_from_train_data(df)\n",
    "    \n",
    "    fig.update_layout(hovermode=\"x\")\n",
    "    \n",
    "    days = (pd.to_datetime(edate) - pd.to_datetime(sdate)).days\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title_text=f\"{CLIENT}, Duration: {sdate} to {edate}, {days} days, Facility count={len(facilities)}\",\n",
    "        font_size=9,\n",
    "        yaxis=dict(\n",
    "            title=\"Recall\",\n",
    "            tickformat=',.0%',\n",
    "            range = [0,1]\n",
    "        ),\n",
    "        xaxis=dict(\n",
    "            title='Length of Stay'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    filename = 'recall_plot.png'\n",
    "    pio.write_image(fig, filename)\n",
    "    log_artifact(filename)  # Export to MlFlow\n",
    "    \n",
    "\n",
    "def generate_recall_curve(performance_base, recall, sdate, edate):\n",
    "    rth_df = performance_base.query('rth == 1')[['censusdate', 'facilityid', 'masterpatientid']]\n",
    "    print('Total RTHs = ',rth_df.shape[0])\n",
    "    rth_df.rename(columns={'censusdate': 'rth_censusdate'},\n",
    "          inplace=True, errors='raise')\n",
    "    \n",
    "    df = performance_base.merge(\n",
    "        rth_df, on=['facilityid', 'masterpatientid']\n",
    "    )\n",
    "    df = df[df.censusdate <= df.rth_censusdate]\n",
    "    df['date_diff'] = (df['rth_censusdate'] - df['censusdate']).dt.days\n",
    "    df = df[df.date_diff <= 3]\n",
    "    df['min_predictionrank'] = df.groupby(['rth_censusdate', 'facilityid', 'masterpatientid'])['predictionrank'].transform('min')\n",
    "    df = df.query('rth == 1')\n",
    "    draw_graph(df, recall, sdate, edate)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_auc_curve(actual_y, preds_y, aucroc, run_id):\n",
    "    plt.clf()\n",
    "    filename = f'auc_curve_{run_id}.png'\n",
    "    fpr, tpr, thresh = metrics.roc_curve(actual_y, preds_y)\n",
    "    plt.plot(fpr,tpr)\n",
    "    plt.xlabel(\"AUC=\"+str(aucroc))\n",
    "    plt.savefig(filename)\n",
    "    log_artifact(filename)  # Export to MlFlow\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## =================== Model Training ==================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ITERATION_NUMBER = 1\n",
    "\n",
    "def objective(trial):\n",
    "    global ITERATION_NUMBER\n",
    "    \n",
    "    params = get_hyper_parameters(trial)\n",
    "    \n",
    "    print(f'Training LGB models with parameter: {params}')\n",
    "    with mlflow.start_run():\n",
    "        start_time = timeit.default_timer()\n",
    "        run_uuid = mlflow.active_run().info.run_uuid\n",
    "        \n",
    "        pos, neg, n2p_ratio = get_pos_neg(train_target_3_day)\n",
    "        facilities = get_facilities_from_train_data(train_idens)\n",
    "        \n",
    "        log_param('01_TRAINING_DATA',TRAINING_DATA)\n",
    "        log_param('02_FACILITIES', (', '.join(facilities)))\n",
    "        log_param('03_FACILITIES_COUNT', len(facilities))\n",
    "        log_param('04_ALL_FEATURE_COUNT', train_x.shape[1])\n",
    "        log_param('05_TRAIN_START_DATE', EXPERIMENT_DATES['train_start_date'])\n",
    "        log_param('06_TRAIN_END_DATE', EXPERIMENT_DATES['train_end_date'])\n",
    "        log_param('07_TRAIN_DURATION_days', get_date_diff(EXPERIMENT_DATES['train_start_date'], EXPERIMENT_DATES['train_end_date']))\n",
    "        log_param('08_TRAIN_POS_COUNT', pos)\n",
    "        log_param('09_TRAIN_NEG_COUNT', neg)\n",
    "        log_param('10_TRAIN_N2P_RATIO', n2p_ratio)\n",
    "        \n",
    "        # Log the train, validation & test date ranges\n",
    "        log_param('000_DESCRIPTION', f'{DESCRIPTION_STRING} {ITERATION_NUMBER}')\n",
    "        ITERATION_NUMBER += 1\n",
    "        log_param('001_OPTUNA_VERSION',optuna.__version__)\n",
    "        log_param('002_LGBM_VERSION',lgb.__version__)\n",
    "\n",
    "        for param in params:\n",
    "            log_param(f'hp__{param}', params[param])\n",
    "\n",
    "        set_tag('model', 'lgb')\n",
    "        print(\"=============================Training started...=============================\")\n",
    "        model = lgb.train(params, train_set=train_data, valid_sets=get_valid_data())\n",
    "        print(\"=============================Training completed...=============================\")\n",
    "        gc.collect()\n",
    "\n",
    "        if HYPER_PARAMETER_TUNING:\n",
    "            # Log validation metric only during tuning\n",
    "            aucroc, pline_recall_at_rank_15 = run_test_set(\n",
    "                model,\n",
    "                run_uuid,\n",
    "                run_uuid,\n",
    "                EXPERIMENT_DATES['validation_start_date'],\n",
    "                EXPERIMENT_DATES['validation_end_date'],\n",
    "                valid_x,\n",
    "                valid_target_3_day,\n",
    "                valid_idens,\n",
    "                dataset='VALID'\n",
    "            )\n",
    "            log_param('p__best_score', model.best_score['valid_0']['auc'])\n",
    "            log_param('p__best_iteration', model.best_iteration)\n",
    "        else:\n",
    "            # Run testset only on test+valid trained model\n",
    "            aucroc, pline_recall_at_rank_15 = run_test_set(\n",
    "                model,\n",
    "                run_uuid,\n",
    "                run_uuid,\n",
    "                EXPERIMENT_DATES['test_start_date'],\n",
    "                EXPERIMENT_DATES['test_end_date'],\n",
    "                test_x,\n",
    "                test_target_3_day,\n",
    "                test_idens,\n",
    "                dataset='TEST',\n",
    "                recall_plot=True\n",
    "            )\n",
    "            log_param('p__best_iteration', f'{BEST_ITERATION}')\n",
    "            \n",
    "        ## running metrics against training\n",
    "        aucroc, pline_recall_at_rank_15 = run_test_set(\n",
    "                model,\n",
    "                run_uuid,\n",
    "                run_uuid,\n",
    "                EXPERIMENT_DATES['train_start_date'],\n",
    "                EXPERIMENT_DATES['train_end_date'],\n",
    "                train_x,\n",
    "                train_target_3_day,\n",
    "                train_idens,\n",
    "                dataset='TRAIN'\n",
    "            )\n",
    "        \n",
    "        model_config = {\n",
    "            'modelid':run_uuid,\n",
    "            'dayspredictionvalid':3,\n",
    "            'model_algo': 'lgbm',\n",
    "            'predictiontask': 'hospitalization',\n",
    "            'modeldescription' : MODEL_DESCRIPTION,\n",
    "            'client_data_trained_on' : TRAINING_DATA,\n",
    "            'vector_model' : VECTOR_MODELS[CLIENT],\n",
    "            'model_s3_folder': EXPERIMENT_ID,\n",
    "            'prospectivedatestart':f'{datetime.now().date()}',\n",
    "            'training_start_date':EXPERIMENT_DATES['train_start_date'],\n",
    "            'training_end_date':EXPERIMENT_DATES['train_end_date'],\n",
    "            'validation_start_date': EXPERIMENT_DATES['validation_start_date'],\n",
    "            'validation_end_date': EXPERIMENT_DATES['validation_end_date'],\n",
    "            'test_start_date': EXPERIMENT_DATES['test_start_date'],\n",
    "            'test_end_date': EXPERIMENT_DATES['test_end_date'],\n",
    "            'test_auc': str(aucroc),\n",
    "            'test_recall_at_rank_15': str(pline_recall_at_rank_15),\n",
    "#             'facility_wise_auc': facility_auc_dict,\n",
    "        }\n",
    "\n",
    "        base_models.append(model_config)\n",
    "        \n",
    "        if not HYPER_PARAMETER_TUNING:\n",
    "            # ================= Save model related artifacts =========================\n",
    "            log_artifact(f'distribution_{CLIENT}.png') # log the distribution graph generated in 06a notebook\n",
    "            \n",
    "            # ================= Save feature drop related artifacts =========================\n",
    "            log_artifact(f'./feature_group_drop_stats.txt') # txt files generated in 05 notebook\n",
    "            log_artifact(f'./feature_cumsums_drop_stats.txt')\n",
    "            \n",
    "            with open('./model_config.json', 'w') as outfile: json.dump(model_config, outfile)\n",
    "            log_artifact(f'./model_config.json')\n",
    "\n",
    "            base_model = BaseModel(model_name=run_uuid, model_type='lgb', model=model)\n",
    "            with open(f'./{run_uuid}.pickle', 'wb') as f: pickle.dump(base_model, f)\n",
    "            log_artifact(f'./{run_uuid}.pickle')\n",
    "\n",
    "            input_features = pd.DataFrame(train_x.columns, columns=['feature'])\n",
    "            input_features.to_csv(f'./input_features.csv', index=False)\n",
    "            log_artifact(f'./input_features.csv')\n",
    "\n",
    "            with open('./na_filler.pickle','wb') as f: pickle.dump(na_filler, f, protocol=4)\n",
    "            log_artifact('./na_filler.pickle')\n",
    "\n",
    "            # =============== Save the code used to training in S3 ======================\n",
    "            for notebook in list(Path('/src/notebooks').glob('0*.ipynb')):\n",
    "                log_artifact(str(notebook))\n",
    "\n",
    "            for shared_code in list(Path('/src/shared').glob('*.py')):\n",
    "                log_artifact(str(shared_code))\n",
    "\n",
    "            for client_code in list(Path('/src/clients').glob('*.py')):\n",
    "                log_artifact(str(client_code))\n",
    "        \n",
    "        t_sec = round(timeit.default_timer() - start_time)\n",
    "        log_param('01_RUN_TIME_sec', f'{t_sec}')\n",
    "        \n",
    "        return 1 - aucroc\n",
    "\n",
    "\n",
    "study = optuna.create_study(sampler=TPESampler())  # Create a new study.\n",
    "study.optimize(                # Invoke optimization of the objective function.\n",
    "    objective, \n",
    "    n_trials=get_training_runs()\n",
    ")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open('./model_config.json', 'w') as outfile: json.dump(base_models, outfile)\n",
    "\n",
    "# base_models    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run():\n",
    "    log_param('000_DESCRIPTION', 'UPLOAD LOGS')\n",
    "    log_artifact('./lgbm_training.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## =============== Load Model from local folder ==================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECTED_MODELID = '3bc0d1320c074b5ea4db576003b1e5a9' \n",
    "\n",
    "# model_path = f'./{modelid}/artifacts/{SELECTED_MODELID}.pickle'\n",
    "model_path = f'{SELECTED_MODELID}.pickle'\n",
    "\n",
    "with open(model_path, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "        model = model.model\n",
    "        \n",
    "if not SELECTED_MODELID:\n",
    "    raise Exception('Configure SELECTED_MODELID')\n",
    "    exit() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ================== Download Models from S3 ==================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import subprocess\n",
    "\n",
    "# modelid = '3bc0d1320c074b5ea4db576003b1e5a9'\n",
    "\n",
    "# subprocess.run(\n",
    "#                 f'aws s3 sync s3://saiva-models/223/{modelid} {modelid}',\n",
    "#                 shell=True,\n",
    "#                 stderr=subprocess.DEVNULL,\n",
    "#                 stdout=subprocess.DEVNULL,\n",
    "#             )\n",
    "\n",
    "# print(test_x.shape)\n",
    "# all_feats = pd.read_csv(\n",
    "#             f'./{modelid}/artifacts/input_features.csv'\n",
    "#         )\n",
    "\n",
    "# test_x = test_x.reindex(columns=all_feats.feature, fill_value=0)\n",
    "# print(test_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ============== Run TestSet on any loaded Model above ============"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "# with mlflow.start_run():\n",
    "#     run_uuid = mlflow.active_run().info.run_uuid\n",
    "#     log_param('00_DESCRIPTION', 'TEST RUN')\n",
    "#     log_param('01_CLIENT', TRAINING_DATA)\n",
    "#     log_param('02_FACILITIES', get_facilities_from_train_data(test_idens))\n",
    "#     test_aucroc, test_recall_at_rank_15 = run_test_set(model, SELECTED_MODELID, run_uuid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ================= List Feature Importance of the model ================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imp = (\n",
    "    pd.DataFrame({\n",
    "        'feature': model.feature_name(),\n",
    "        'importance': model.feature_importance(importance_type='gain'),  # split\n",
    "    })\n",
    "    .sort_values('importance', ascending=False)\n",
    ")\n",
    "feature_imp.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb.plot_importance(model, max_num_features=50, figsize=(15,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ================ Run Shap Explanations for Test Set ================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = shap.TreeExplainer(model).shap_values(test_x)\n",
    "\n",
    "shap.summary_plot(shap_values, test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shap takes lot of time to run across all test dataset. Since certain index and run shap for faster results \n",
    "\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(test_x.iloc[0:50])\n",
    "\n",
    "shap_results = []\n",
    "\n",
    "for idx, row in test_x.iloc[0:50].iterrows():\n",
    "    shaps = pd.DataFrame(\n",
    "        {\n",
    "            \"feature\": test_x.columns,\n",
    "            \"attribution_score\": shap_values[1][idx],\n",
    "            \"feature_value\": test_x.iloc[idx],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    shaps[\"masterpatientid\"] = test_idens.iloc[idx].masterpatientid\n",
    "    shaps[\"facilityid\"] = test_idens.iloc[idx].facilityid\n",
    "    shaps[\"censusdate\"] = test_idens.iloc[idx].censusdate\n",
    "\n",
    "    shap_results.append(shaps)\n",
    "\n",
    "results = pd.concat(shap_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.query('attribution_score >= 0.1').sort_values(by=['attribution_score'], ascending=False)['feature'].value_counts().head(25)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}