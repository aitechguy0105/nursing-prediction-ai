{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "import boto3\n",
    "from eliot import log_message\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.engine.url import URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIENT = 'marquis'\n",
    "START_DATE = '2021-05-01'\n",
    "END_DATE = '2021-06-30'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DbEngine(object):\n",
    "    \"\"\"\n",
    "    Fetch the credentials from AWS Secrets Manager.\n",
    "    :return: DB connection to the respective database\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, region_name='us-east-1'):\n",
    "        self.session = boto3.session.Session()\n",
    "        self.secrets_client = self.session.client(\n",
    "            service_name='secretsmanager',\n",
    "            region_name=region_name\n",
    "        )\n",
    "\n",
    "    def get_secrets(self, secret_name):\n",
    "        \"\"\"\n",
    "        :return: Based on the environment get secrets for\n",
    "        Client SQL db & Postgres Saivadb\n",
    "        \"\"\"\n",
    "        log_message(message_type='info', action_type='get_secrets', secret_name=secret_name)\n",
    "        db_info = json.loads(\n",
    "            self.secrets_client.get_secret_value(SecretId=secret_name)[\n",
    "                'SecretString'\n",
    "            ]\n",
    "        )\n",
    "        return db_info\n",
    "\n",
    "    def get_postgresdb_engine(self):\n",
    "        \"\"\"\n",
    "        Based on the environment connects to the respective database\n",
    "        :param client: client name\n",
    "        :return: Saivadb Postgres engine\n",
    "        \"\"\"\n",
    "        log_message(message_type='info', action_type='connect_to_postgresdb', client='SaivaDB')\n",
    "        # Fetch credentials from AWS Secrets Manager\n",
    "        postgresdb_info = self.get_secrets(secret_name=f'prod-saivadb')\n",
    "        # Create DB URL\n",
    "        saivadb_url = URL(\n",
    "            drivername='postgresql',\n",
    "            username=postgresdb_info['username'],\n",
    "            password=postgresdb_info['password'],\n",
    "            host=postgresdb_info['host'],\n",
    "            port=postgresdb_info['port'],\n",
    "            database=postgresdb_info['dbname'],\n",
    "        )\n",
    "        # Return Postgres Engine\n",
    "        return create_engine(saivadb_url, echo=False)\n",
    "    \n",
    "    def get_sqldb_engine(self, clientdb_name):\n",
    "        \"\"\"\n",
    "        Based on the environment connects to the respective database.\n",
    "        Avante db is in client VPN hence we use different credentials.\n",
    "        :param client: client name\n",
    "        :return: Client SQL engine\n",
    "        \"\"\"\n",
    "        log_message(message_type='info', action_type='connect_to_sqldb', client=clientdb_name)\n",
    "        # Fetch credentials from AWS Secrets Manager\n",
    "        if clientdb_name == 'avante':\n",
    "            sqldb_info = self.get_secrets(secret_name=f'avantedb')\n",
    "        else:\n",
    "            sqldb_info = self.get_secrets(secret_name=f'prod-sqlserver')\n",
    "            sqldb_info['dbname'] = clientdb_name\n",
    "\n",
    "        # Create DB URL\n",
    "        client_sqldb_url = URL(\n",
    "            drivername='mssql+pyodbc',\n",
    "            username=sqldb_info['username'],\n",
    "            password=sqldb_info['password'],\n",
    "            host=sqldb_info['host'],\n",
    "            port=sqldb_info['port'],\n",
    "            database=sqldb_info['dbname'],\n",
    "            query={'driver': 'ODBC Driver 17 for SQL Server'},\n",
    "        )\n",
    "        # Return Sql Engine\n",
    "        return create_engine(client_sqldb_url, echo=False)\n",
    "    \n",
    "    def verify_connectivity(self, engine):\n",
    "        assert engine.execute('select 1').fetchall() is not None  # verify connectivity\n",
    "\n",
    "engine = DbEngine()\n",
    "client_engine =  engine.get_postgresdb_engine()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "with rh as (\n",
    "    select ht.*,\n",
    "    fa.facilityname,\n",
    "    fp.masterpatientid,\n",
    "    dp.modelid,\n",
    "    dp.predictionrank,\n",
    "    dp.show_in_report,\n",
    "    fp.patientmrn,\n",
    "    fp.firstname,\n",
    "    fp.lastname\n",
    "    from public.hospital_transfers ht\n",
    "        left join public.facility_patient fp\n",
    "        on ht.client = fp.client\n",
    "        and ht.facilityid = fp.facilityid\n",
    "        and ht.patientid = fp.patientid\n",
    "            left join daily_predictions dp\n",
    "            on ht.client = dp.client\n",
    "            and ht.facilityid = dp.facilityid\n",
    "            and (date(ht.dateoftransfer) - date(dp.censusdate)) <= 3\n",
    "            and (date(ht.dateoftransfer) - date(dp.censusdate)) != 0\n",
    "            and date(dp.censusdate) <= date(ht.dateoftransfer)\n",
    "            and fp.masterpatientid = dp.masterpatientid\n",
    "            left join facility fa\n",
    "            on fa.facilityid = ht.facilityid\n",
    "            and fa.client = ht.client\n",
    "    where (dp.published = True or dp.published is null)\n",
    "      and ht.dateoftransfer >= '2020-01-01 00:00:00'\n",
    "      and (dp.experiment_group = True or dp.experiment_group is null)\n",
    "      and fa.is_active=true\n",
    "      and ht.client='{CLIENT}'\n",
    "      and dp.censusdate between '{START_DATE}' and '{END_DATE}'\n",
    ")\n",
    "    SELECT rh.client,\n",
    "           rh.facilityid,\n",
    "           rh.facilityname,\n",
    "           rh.modelid,\n",
    "           rh.patientid,\n",
    "           rh.masterpatientid,\n",
    "           rh.patientmrn,\n",
    "           rh.lastname,\n",
    "           rh.firstname,\n",
    "           rh.dateoftransfer,\n",
    "           rh.planned,\n",
    "           rh.transferreason,\n",
    "           rh.otherreasonfortransfer,\n",
    "           rh.outcome,\n",
    "           rh.transferredto,\n",
    "           rh.lengthofstay,\n",
    "           rh.payertype,\n",
    "           rh.payerdescription,\n",
    "           min(predictionrank) as rank_cutoff,\n",
    "           bool_or(rh.show_in_report) as show_in_report,\n",
    "           (CASE\n",
    "                WHEN bool_or(rh.show_in_report) IS NULL\n",
    "                    THEN 0\n",
    "                ELSE count(*)\n",
    "               END\n",
    "               ) as num_predictions\n",
    "    FROM rh\n",
    "    GROUP BY rh.client, rh.facilityid, rh.facilityname, rh.modelid,\n",
    "             rh.patientid, rh.masterpatientid, rh.patientmrn, rh.lastname, rh.firstname, rh.dateoftransfer,\n",
    "             rh.planned, rh.transferreason, rh.otherreasonfortransfer, rh.outcome,\n",
    "             rh.transferredto, rh.lengthofstay, rh.payertype, rh.payerdescription\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dataframe from the sql query\n",
    "df = pd.read_sql(query, con = client_engine)\n",
    "df = df[['client','facilityid','dateoftransfer','masterpatientid','rank_cutoff']]\n",
    "df = df[(df['dateoftransfer']>='2021-05-01')&(df['dateoftransfer']<='2021-06-30')]\n",
    "# dataframe contains all the transferred residents.\n",
    "df['resident_transferred'] = 1\n",
    "\n",
    "# sorting the dataframe w.r.t dateoftransfer, facilityid and rank_cutoff\n",
    "df = df.sort_values(by=['dateoftransfer','facilityid','rank_cutoff'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "following code snippet perform the below operation for each date and each facilityid - \n",
    "1) It merges(right join) the dataframe slice(each date, each facilityid) \n",
    "with another dataframe 'one_to_fifty' which contains single column of 1 to 50.\n",
    "2) 'client','facilityid','dateoftransfer' is filled by ffill.\n",
    "3) 'rank' is copied on 'rank_cutoff' and all the null values are filled by 0.\n",
    "All the dataframe slices are appended together to form a single dataframe.\n",
    "\"\"\"\n",
    "combined_list =[]\n",
    "one_to_fifty = pd.Series(range(1, 300+1))\n",
    "one_to_fifty = pd.DataFrame({'rank':one_to_fifty})\n",
    "facilityids = df.facilityid.unique().tolist()\n",
    "for date in pd.date_range(start='2021-05-01', end='2021-06-30'):\n",
    "    for facility in facilityids:\n",
    "        specific_date_facility_df = df[(df['dateoftransfer']==date) & (df['facilityid']==facility)]\n",
    "        \n",
    "        if len(specific_date_facility_df):\n",
    "            specific_date_facility_df = specific_date_facility_df.merge(one_to_fifty,left_on='rank_cutoff', right_on='rank', how='right')\n",
    "            specific_date_facility_df.loc[:,['client','facilityid','dateoftransfer']] = specific_date_facility_df.loc[:,['client','facilityid','dateoftransfer']].ffill()\n",
    "            specific_date_facility_df['resident_transferred'].fillna(0,inplace=True)\n",
    "            specific_date_facility_df['rank_cutoff'] = specific_date_facility_df['rank']\n",
    "            specific_date_facility_df = specific_date_facility_df.drop('rank',axis=1)\n",
    "            specific_date_facility_df = specific_date_facility_df.sort_values(by=['dateoftransfer','facilityid','rank_cutoff'])\n",
    "            combined_list.append(specific_date_facility_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat(combined_list)\n",
    "combined_df = combined_df.sort_values(by=['dateoftransfer','facilityid','rank_cutoff'])\n",
    "combined_df = combined_df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "'hospitalized_cumsum' signifies the cumulative summation of the patients transferred per facility per date\n",
    "'total_relevant' signifies the total residents transferred to the hospital per facility per date\n",
    "\"\"\"\n",
    "def precision_recall_at_k(group):\n",
    "    group.loc[:, \"hospitalized_cumsum\"] = group.resident_transferred.cumsum()\n",
    "    group.loc[:, \"total_relevant\"] = group.resident_transferred.sum()\n",
    "    return group.reset_index(drop=True)\n",
    "\n",
    "performance_base = (\n",
    "            combined_df.groupby([\"client\", \"facilityid\", \"dateoftransfer\"])\n",
    "            .apply(precision_recall_at_k)\n",
    "            .reset_index(drop=True)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summation of the 'hospitalized_cumsum' grouped by client, facilityid and rank_cutoff.\n",
    "predicted_df = performance_base.groupby(['client','facilityid','rank_cutoff']).hospitalized_cumsum.sum().reset_index(name='total_residents_predicted')\n",
    "predicted_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summation of the 'total_relevant' grouped by client, facilityid and rank_cutoff.\n",
    "transferred_df = performance_base.groupby(['client','facilityid','rank_cutoff']).total_relevant.sum().reset_index(name='total_residents_transferred')\n",
    "transferred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining the above to dataframes and dividing total_residents_predicted by total_residents_transferred to get the recall.\n",
    "recall_df = predicted_df.merge(transferred_df,how='inner',on=['client','facilityid','rank_cutoff'])\n",
    "recall_df['recall_at_k'] = recall_df['total_residents_predicted']/recall_df['total_residents_transferred']\n",
    "recall_df = recall_df[recall_df['rank_cutoff']<=50]\n",
    "recall_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlengine = engine.get_sqldb_engine('marquis')\n",
    "\n",
    "census_query = f\"\"\"\n",
    "    select facilityid, count(*)/30 as census_count\n",
    "    from view_ods_daily_census_v2\n",
    "    where censusdate>= '2021-06-01' and censusdate<= '2021-06-30'\n",
    "    group by facilityid\n",
    "\"\"\"\n",
    "\n",
    "census_df = pd.read_sql(census_query, con = sqlengine)\n",
    "census_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "recall_census_df = recall_df.merge(census_df,how='inner',on='facilityid')\n",
    "recall_census_df['fac_id__census__res_transferred'] =  recall_census_df['facilityid'].astype(int).astype(str)+'__'+\\\n",
    "recall_census_df['census_count'].astype(str)+'__'+recall_census_df['total_residents_transferred'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_census_df[(recall_census_df['facilityid']==33) & (recall_census_df['rank_cutoff']==45)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_census_df[(recall_census_df['facilityid']==1) & (recall_census_df['rank_cutoff']==15)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_census_df[(recall_census_df['facilityid']==19) & (recall_census_df['rank_cutoff']==30)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_census_df[(recall_census_df['facilityid']==38) & (recall_census_df['rank_cutoff']==15)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_census_df.to_csv('recall_at_every_rank.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
