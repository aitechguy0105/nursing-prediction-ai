{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: before running the code make sure you turn on avante instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## select the below values as per you experiment.\n",
    "client = 'avante'\n",
    "start_date = '2021-01-01'\n",
    "end_date= '2021-11-23'\n",
    "\n",
    "progressnote_types = ['* Skilled Nursing Note','X Social Service Interview',\n",
    "'* Incident/Accident Note','Avante Daily Skilled Note',\n",
    "'* AVANTE Admission Plan of Care','Alert Note','* Behavior','MDS Note'\n",
    "'eMar - Shift Level Administration Note','* Social Services Admission / Readmission',\n",
    "'* Family/MPOA/Responsible Party Contact','* Activity Note',\n",
    "'Speech Therapy Screen','* Education(Family/Resident)','* Dietary RD/DTR Progress Note',\n",
    "'* Dietary CDM Progress Note','* Social Services Note']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns',None)\n",
    "import json\n",
    "import os\n",
    "import boto3\n",
    "from eliot import log_message\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.engine.url import URL\n",
    "from sqlalchemy import text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  add any preceding or following negation word here.\n",
    "##### example to remove history of yelling, add 'history in preceding negations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from negspacy.negation import Negex\n",
    "from negspacy.termsets import en_clinical\n",
    "import re\n",
    "import string\n",
    "preceding_negations = en_clinical['preceding_negations']\n",
    "following_negations = en_clinical['following_negations']\n",
    "custom_preceding_negations = ['0','assessment','consent','decrease','decreased','decreasing','deny','family',\n",
    "                              'mild','mildly','minimal','n/c','neg','negative','neither','no s/s','no sign',\n",
    "                              'no signs','nor','perform','precaution','precautions','prevent','prevention','history',\n",
    "                              'quarantine','recovered','regulations','restriction','restrictions','retesting',\n",
    "                              'screen','slight','swabbed','vac','w/o sign','w/o signs','w/o','worsening','zero','free from sx/sx']\n",
    "\n",
    "custom_following_negations = ['assessment','care','cares','consent','crisis','decrease','decreased','decreasing','family',\n",
    "                              'guidelines','mild','mildly','minimal','neg','negative','none','note','outbreak',\n",
    "                              'pandemic','perform','precaution','precautions','prevent','prevention','protocol',\n",
    "                              'quarantine','recovered','regulations','restriction','restrictions','retesting',\n",
    "                              'screen','swabbed','test','testing','vac'\n",
    "                             ]\n",
    "preceding_negations += custom_preceding_negations\n",
    "following_negations += custom_following_negations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### connecting with database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DbEngine(object):\n",
    "    \"\"\"\n",
    "    Fetch the credentials from AWS Secrets Manager.\n",
    "    :return: DB connection to the respective database\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, region_name='us-east-1'):\n",
    "        self.session = boto3.session.Session()\n",
    "        self.secrets_client = self.session.client(\n",
    "            service_name='secretsmanager',\n",
    "            region_name=region_name\n",
    "        )\n",
    "\n",
    "    def get_secrets(self, secret_name):\n",
    "        \"\"\"\n",
    "        :return: Based on the environment get secrets for\n",
    "        Client SQL db & Postgres Saivadb\n",
    "        \"\"\"\n",
    "        log_message(message_type='info', action_type='get_secrets', secret_name=secret_name)\n",
    "        db_info = json.loads(\n",
    "            self.secrets_client.get_secret_value(SecretId=secret_name)[\n",
    "                'SecretString'\n",
    "            ]\n",
    "        )\n",
    "        return db_info\n",
    "\n",
    "    def get_postgresdb_engine(self):\n",
    "        \"\"\"\n",
    "        Based on the environment connects to the respective database\n",
    "        :param client: client name\n",
    "        :return: Saivadb Postgres engine\n",
    "        \"\"\"\n",
    "        log_message(message_type='info', action_type='connect_to_postgresdb', client='SaivaDB')\n",
    "        # Fetch credentials from AWS Secrets Manager\n",
    "        postgresdb_info = self.get_secrets(secret_name=f'prod-saivadb')\n",
    "        # Create DB URL\n",
    "        saivadb_url = URL(\n",
    "            drivername='postgresql',\n",
    "            username=postgresdb_info['username'],\n",
    "            password=postgresdb_info['password'],\n",
    "            host=postgresdb_info['host'],\n",
    "            port=postgresdb_info['port'],\n",
    "            database=postgresdb_info['dbname'],\n",
    "        )\n",
    "        # Return Postgres Engine\n",
    "        return create_engine(saivadb_url, echo=False)\n",
    "    \n",
    "    def get_sqldb_engine(self, clientdb_name):\n",
    "        \"\"\"\n",
    "        Based on the environment connects to the respective database.\n",
    "        Avante db is in client VPN hence we use different credentials.\n",
    "        :param client: client name\n",
    "        :return: Client SQL engine\n",
    "        \"\"\"\n",
    "        log_message(message_type='info', action_type='connect_to_sqldb', client=clientdb_name)\n",
    "        # Fetch credentials from AWS Secrets Manager\n",
    "        if clientdb_name == 'avante':\n",
    "            sqldb_info = self.get_secrets(secret_name=f'avantedb')\n",
    "        else:\n",
    "            sqldb_info = self.get_secrets(secret_name=f'prod-sqlserver')\n",
    "            sqldb_info['dbname'] = clientdb_name\n",
    "\n",
    "        # Create DB URL\n",
    "        client_sqldb_url = URL(\n",
    "            drivername='mssql+pyodbc',\n",
    "            username=sqldb_info['username'],\n",
    "            password=sqldb_info['password'],\n",
    "            host=sqldb_info['host'],\n",
    "            port=sqldb_info['port'],\n",
    "            database=sqldb_info['dbname'],\n",
    "            query={'driver': 'ODBC Driver 17 for SQL Server'},\n",
    "        )\n",
    "        # Return Sql Engine\n",
    "        return create_engine(client_sqldb_url, echo=False)\n",
    "    \n",
    "    def verify_connectivity(self, engine):\n",
    "        assert engine.execute('select 1').fetchall() is not None  # verify connectivity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query = f\"\"\"\n",
    "select * from view_ods_progress_note\n",
    "where createddate between '{start_date}' and '{end_date}'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* reading data and joining the progress notes in correct order.\n",
    "* keeping selected progressnote types only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = DbEngine()\n",
    "client_engine =  engine.get_sqldb_engine('avante')\n",
    "df =pd.read_sql(sql_query, con = client_engine)\n",
    "df.columns = df.columns.str.lower()\n",
    "df = df[~df['notetext'].isna()]\n",
    "df = df[df['progressnotetype'].isin(progressnote_types)]\n",
    "\n",
    "df.sort_values(by=['facilityid', 'patientid', 'createddate', 'progressnoteid', 'progressnotetype',\n",
    "                                     'section', 'sectionsequence', 'notetextorder'], inplace=True)\n",
    "\n",
    "grp_cols = ['facilityid', 'patientid', 'createddate', 'progressnoteid', 'progressnotetype', 'section']\n",
    "grouped_df = (df.groupby(grp_cols).agg({'notetext': lambda note_parts: ''.join(\n",
    "    note_parts)}).reset_index())\n",
    "\n",
    "grouped_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#amount of data\n",
    "grouped_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keeping selected columns\n",
    "grouped_df = grouped_df[['facilityid','patientid','createddate','progressnotetype','notetext']]\n",
    "grouped_df.sort_values(by=['createddate'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making patterns out of keywords.\n",
    "from keywords import KEYWORDS_LIST\n",
    "KEYWORDS_LIST = [keyword.replace('\\n','') for keyword in KEYWORDS_LIST]\n",
    "           \n",
    "pattern_label_list = []\n",
    "for word in KEYWORDS_LIST:\n",
    "    pattern_list = []\n",
    "    pattern_words = re.findall(f\"[\\w']+|[{string.punctuation}]\", word)\n",
    "    for sub_pattern in pattern_words:\n",
    "        pattern_list.append({'LOWER': sub_pattern})\n",
    "    pattern_label_list.append({'label': 'ENTITY', 'pattern':pattern_list})\n",
    "print(pattern_label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.pipeline import EntityRuler\n",
    "nlp = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentencizer = nlp.add_pipe('sentencizer')\n",
    "sentencizer.punct_chars.union({'\\n'})\n",
    "\n",
    "# Add EntityRuler - adding labels and patterns\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "ruler.add_patterns(pattern_label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# though negspacy covers almost every negative word but still adding some negative words.\n",
    "negation_words = [\"Zero\",\"Deny\", \"Denies\",\"Denied\",\"No\", \"Not\", \"None\", \"No one\", \"Nobody\", \"Nothing\", \"Neither\",\n",
    "                  \"Nowhere\", \"Never\", \"Hardly\", \"Scarcely\", \"Barely\", \"Doesn't\", \"Isn't\", \"Wasn't\", \"Shouldn't\",\n",
    "                  \"Wouldn't\", \"Couldn't\", \"Won't\", \"Can't\", \"Don't\", \"0\", \"quarantine\",\"no\",\"history\"]\n",
    "negation_words += custom_preceding_negations + custom_following_negations\n",
    "\n",
    "nlp.add_pipe(\n",
    "            \"negex\", config={\n",
    "              \"chunk_prefix\": negation_words+preceding_negations+following_negations,\n",
    "            }\n",
    "            \n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adds span tags around the keywords.\n",
    "\n",
    "def add_tags(clean_note, pos_index_list):\n",
    "        \"\"\"\n",
    "        :param clean_note:\n",
    "        :param pos_index_list:\n",
    "        :return:\n",
    "        this function adds html tags using the indexes from pos_index_list in the reverse order.\n",
    "        \"\"\"\n",
    "        END_TAG = '</span>'\n",
    "        START_TAG = \"<span class='yellow'>\"\n",
    "        for index in pos_index_list[::-1]:\n",
    "            if index[0] != None:\n",
    "                clean_note = clean_note[:index[1]] + END_TAG + clean_note[index[1]:]\n",
    "                clean_note = clean_note[:index[0]] + START_TAG + clean_note[index[0]:]\n",
    "        return clean_note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cleaning the text.\n",
    "# extracting keywords with positive occurence.\n",
    "# removing words that occur is both positive and negative form.\n",
    "#removing duplicate words.\n",
    "# keeping texts which have span tags in them.\n",
    "def extract_entities(row):\n",
    "    \n",
    "    row['clean_note'] = row['notetext']\n",
    "    row['clean_note'] = row['clean_note'].replace('  ', ' ')\n",
    "    doc = nlp(row['clean_note'])\n",
    "\n",
    "    neg_list = list()\n",
    "    pos_list, pos_index_list = list(), list()\n",
    "    for word in doc.ents:\n",
    "        # segregating positive and negative words.\n",
    "        if not word._.negex or word.text in 'non-compliant':\n",
    "            # populating positive and respective positive index list.\n",
    "            pos_list.append(word.text.lower())\n",
    "            pos_index_list.append((word.start_char, word.end_char))\n",
    "        else:\n",
    "            neg_list.append(word.text)\n",
    "    # neutral words are words present in both negative and positive form.\n",
    "    # patient tested for edema. No edema found. -> edema is neutral word.\n",
    "    # neutral words are removed from positive list and its respective index list\n",
    "    neutral_words = list(set(pos_list).intersection(set(neg_list)))\n",
    "    if neutral_words:\n",
    "        # removing the neutral words from positive list and index in reverse order.\n",
    "        for neutral_word in neutral_words[::-1]:\n",
    "            neutral_word_index = pos_list.index(neutral_word)\n",
    "            del pos_list[neutral_word_index]\n",
    "            del pos_index_list[neutral_word_index]\n",
    "\n",
    "\n",
    "    if len(pos_index_list):\n",
    "\n",
    "        filtered_pos_index_list = []\n",
    "        filtered_pos_list = []\n",
    "        for ind in range(len(pos_list)):\n",
    "            if pos_list[ind] not in filtered_pos_list:\n",
    "                filtered_pos_list.append(pos_list[ind])\n",
    "                filtered_pos_index_list.append(pos_index_list[ind])\n",
    "        row['index_list'] = filtered_pos_index_list\n",
    "        row['word_list'] = filtered_pos_list\n",
    "        row['clean_note'] = add_tags(row['clean_note'], filtered_pos_index_list)\n",
    "        doc = nlp(row['clean_note'])\n",
    "        row['clipped_note'] = ''\n",
    "        for sent in doc.sents:\n",
    "            tagged_sent = add_tags(sent.text, filtered_pos_index_list)\n",
    "            if '<span' in sent.text or 'span>' in sent.text:\n",
    "                row['clipped_note'] +=sent.text\n",
    "        print(row['clipped_note'])\n",
    "        print(filtered_pos_list)\n",
    "        print('====================')\n",
    "    return row\n",
    "\n",
    "grouped_df = grouped_df.apply(extract_entities, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = grouped_df[~grouped_df['index_list'].isna()]\n",
    "del grouped_df['clean_note']\n",
    "grouped_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df.to_csv('highrisk_patients.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
