{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook compares the output of test set on various models of different experiment-id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import timeit\n",
    "import boto3\n",
    "import subprocess\n",
    "s3 = boto3.resource('s3')\n",
    "import gc\n",
    "import pickle\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any\n",
    "\n",
    "@dataclass\n",
    "class BaseModel:\n",
    "    \"\"\"Class for keeping track of base models\"\"\"\n",
    "\n",
    "    model_name: str\n",
    "    model_type: str\n",
    "    model: Any\n",
    "\n",
    "    def predict(self, x) -> float:\n",
    "        if self.model_type == \"rf\":\n",
    "            return self.model.predict_proba(x)[:, 1]\n",
    "        elif self.model_type == \"lgb\":\n",
    "            return self.model.predict(x)\n",
    "        else:\n",
    "            raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the test dataset.\n",
    "\n",
    "processed_path = Path('/data/processed')\n",
    "processed_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "with open(processed_path/'03-test_x.pickle','rb') as f: test_x = pickle.load(f)\n",
    "with open(processed_path/'03-test_target_3_day.pickle','rb') as f: test_target_3_day = pickle.load(f)\n",
    "with open(processed_path/'03-test_target_7_day.pickle','rb') as f: test_target_7_day = pickle.load(f)\n",
    "with open(processed_path/'03-test_idens.pickle','rb') as f: test_idens = pickle.load(f)\n",
    "    \n",
    "with open(processed_path/'03-na_filler.pickle', 'rb') as f: na_filler = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* fill the experiment_id_name_dict with the expeirment and corresponding experimentname\n",
    "* models of the experiment-ids will be tested against the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_id_name_dict ={\n",
    "#     '60':'meridian_less_data'\n",
    "#     '63': 'meridian_snf-and-alf_test',\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_at_k(group):\n",
    "    group.loc[:, \"hospitalized_cumsum\"] = group.hospitalized_within_pred_range.cumsum()\n",
    "    group.loc[:, \"total_relevant\"] = group.hospitalized_within_pred_range.sum()\n",
    "    group.loc[:, \"recall_at_k\"] = group.hospitalized_cumsum / group.total_relevant\n",
    "\n",
    "    return group.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* uncomment the code.\n",
    "* manually change the experiment-id number and run the below command for all the experiment_id_keys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !aws s3 cp 's3://saiva-models/63' /data/model_comparison/63/  --recursive --include=\".pickle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     function fills the na of the testset with the na_filler.pickle of the models.\n",
    "def fill_na( df, path, model_id):\n",
    "    with open(f'{path}/{model_id}/artifacts/na_filler.pickle','rb') as f: \n",
    "        na_filler = pickle.load(f)\n",
    "\n",
    "    return df.fillna(na_filler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function is replica of the 'objective' function of 04 notebook.\n",
    "# here it is run on test dataset.\n",
    "\n",
    "def main_testing_function(experiment_id, df):\n",
    "    print(f'***********Processing for {experiment_id_name_dict[experiment_id]}******************')\n",
    "    processed_path = Path(f'/data/model_comparison/{experiment_id}/')\n",
    "    processed_path.mkdir(parents=True, exist_ok=True)\n",
    "    model_ids = os.listdir(processed_path)\n",
    "    print('model_ids----> ',model_ids)\n",
    "    print('processed_path----> ',processed_path)\n",
    "    dataframe_list = []\n",
    "    for model_id in model_ids:\n",
    "        test_x = df.copy()\n",
    "        print('model_id--->', model_id)\n",
    "        with open(f'{processed_path}/{model_id}/artifacts/{model_id}.pickle','rb') as f: \n",
    "            model = pickle.load(f)\n",
    "        print('total_null_in_test---->', test_x.mean().mean())\n",
    "        test_x = fill_na(test_x, processed_path, model_id)\n",
    "        print('total_null_in_test_after fill na---->', test_x.mean().mean())\n",
    "        test_preds = model.predict(test_x)\n",
    "        total_test_aucroc = roc_auc_score(test_target_3_day, test_preds)\n",
    "        test_base = test_idens.copy()\n",
    "        test_base['predictionvalue'] = test_preds\n",
    "        test_base['hospitalized_within_pred_range'] = test_target_3_day\n",
    "        test_base['predictionrank'] = test_base.groupby(['censusdate', 'facilityid']).predictionvalue.rank(ascending=False)\n",
    "        test_base = test_base.sort_values('predictionrank', ascending=True)\n",
    "\n",
    "        performance_base = (\n",
    "            test_base.groupby([\"facilityid\", \"censusdate\"])\n",
    "            .apply(precision_recall_at_k)\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "        facility_pats = performance_base.groupby(['censusdate','facilityid']\n",
    "                                                ).predictionrank.max().reset_index().groupby('facilityid').predictionrank.median().reset_index()\n",
    "\n",
    "        for facilityid in sorted(test_idens.facilityid.unique()):\n",
    "            mask = (test_idens.facilityid == facilityid).values\n",
    "            k_at_15_percent = round(facility_pats.loc[facility_pats.facilityid == facilityid].predictionrank * .15).values[0]\n",
    "\n",
    "            rank_subset = performance_base.loc[(performance_base.facilityid==facilityid)]\n",
    "\n",
    "            try:\n",
    "\n",
    "                agg_recall_at_15_percent = (\n",
    "                    rank_subset.loc[rank_subset.predictionrank == k_at_15_percent].hospitalized_cumsum.sum() / rank_subset.loc[rank_subset.predictionrank == k_at_15_percent].total_relevant.sum()\n",
    "                )\n",
    "                auc_roc_score = roc_auc_score(test_target_3_day[mask], test_preds[mask])\n",
    "\n",
    "                dataframe_list.extend([[model_id, facilityid, auc_roc_score, agg_recall_at_15_percent]])\n",
    "            except Exception as e:\n",
    "                # workaround for infinity-benchmark because you cannot calculate facility level\n",
    "                # metric for one facility.  This workaround will just skip calculating that\n",
    "                # facility level metric - it will print the exception, but continue\n",
    "                print(e)\n",
    "                continue\n",
    "    return dataframe_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above function is being ran for each model of each experiment-id.\n",
    "for experiment_id in experiment_id_name_dict.keys():\n",
    "    dataframe_list = main_testing_function(experiment_id, test_x)\n",
    "    df = pd.DataFrame(dataframe_list,columns=['modelid', 'facilityid', f'{experiment_id_name_dict[experiment_id]}_auc_roc_score',f'{experiment_id_name_dict[experiment_id]}_recall_at_15'])\n",
    "    df.sort_values(by=['facilityid'],inplace=True)\n",
    "    df.to_csv(f'{experiment_id_name_dict[experiment_id]}.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
