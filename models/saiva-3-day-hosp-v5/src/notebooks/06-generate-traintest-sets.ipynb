{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import sys\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import timedelta, datetime\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.insert(0, '/src')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shared.constants import LOCAL_TRAINING_CONFIG_PATH\n",
    "from shared.utils import load_config\n",
    "\n",
    "config = load_config(LOCAL_TRAINING_CONFIG_PATH)\n",
    "training_config = config.training_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIENT = \"+\".join([config.organization_id for config in training_config.ml_model_org_configs])\n",
    "\n",
    "EXPERIMENT_DATES = training_config.training_metadata.experiment_dates\n",
    "HYPER_PARAMETER_TUNING = training_config.training_metadata.hyper_parameter_tuning\n",
    "\n",
    "IDEN_COLS = ['censusdate', 'facilityid', 'masterpatientid', 'bedid',\n",
    "            'censusactioncode', 'payername', 'payercode','client','rth', 'LFS']\n",
    "\n",
    "CAT_COLS = ['facility']\n",
    "\n",
    "if not HYPER_PARAMETER_TUNING:\n",
    "    EXPERIMENT_DATES['train_end_date'] = (datetime.strptime(EXPERIMENT_DATES['validation_end_date'], '%Y-%m-%d') - timedelta(days=2)).strftime('%Y-%m-%d')\n",
    "    EXPERIMENT_DATES['validation_start_date'] = (datetime.strptime(EXPERIMENT_DATES['validation_end_date'], '%Y-%m-%d') - timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "    \n",
    "print(CLIENT)\n",
    "print(HYPER_PARAMETER_TUNING)\n",
    "print(EXPERIMENT_DATES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "processed_path = Path('/data/processed')\n",
    "processed_path.mkdir(parents=True, exist_ok=True)\n",
    "filename = 'final_cleaned_df.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# final = pd.read_parquet(processed_path/'02-result.parquet')\n",
    "final = pd.read_parquet(processed_path/f'{filename}')\n",
    "# Remove columns with names containing '\\t' or '\\n'\n",
    "cols_to_drop = [col for col in final.columns if '\\t' in col or '\\n' in col]\n",
    "final.drop(columns=cols_to_drop, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for certain facilities for whom we send report\n",
    "\n",
    "# print(final.shape)\n",
    "# final = final.query('facilityid in [5, 7, 10, 21, 9, 1, 6, 8, 3, 13, 4]')\n",
    "# print(final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total Admissions', final.query('~dateofadmission.isna()').shape)\n",
    "print('Total RTHs', final.query('~date_of_transfer.isna()').shape)\n",
    "print('Total Patient Lines', final.query('hosp_target_3_day_hosp == 1').shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all rows which have same day admission and RTH\n",
    "\n",
    "index_names = final.query('dateofadmission == date_of_transfer').index\n",
    "final.drop(index_names, inplace = True) \n",
    "final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all rows which have only 1 census row\n",
    "\n",
    "final = final.set_index(['masterpatientid'])\n",
    "index_names = final.groupby('masterpatientid').filter(\n",
    "    lambda g: len(g) == 1\n",
    ").index\n",
    "\n",
    "final.drop(index_names, inplace = True) \n",
    "final = final.reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find census day difference for every patient record (current census - previous census)\n",
    "\n",
    "final = final.sort_values(by=['masterpatientid','censusdate'])\n",
    "final['census_diff'] = final.groupby(final.masterpatientid,\n",
    "              as_index=False)['censusdate'].diff()\n",
    "final['census_diff'] = final['census_diff'].dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all rows which have RTH and census_diff greater than 1\n",
    "\n",
    "final = final.set_index(['masterpatientid','censusdate'])\n",
    "index_names = final.query('(hosp_target_3_day_hosp == 1) & (census_diff > 1)').index\n",
    "final.drop(index_names, inplace = True) \n",
    "final = final.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" We increment the census date by 1, since the prediction day always includes data upto last night.\n",
    "This means for every census date the data is upto previous night. \n",
    "\"\"\"\n",
    "print(final.shape)\n",
    "\n",
    "# Increment censusdate by 1\n",
    "final['censusdate'] = (pd.to_datetime(final['censusdate']) + timedelta(days=1))\n",
    "\n",
    "# Retain RTH days in a separate dataframe\n",
    "rth_df = final.query('~date_of_transfer.isna()')[['masterpatientid','date_of_transfer']]\n",
    "rth_df['rth'] = 1\n",
    "\n",
    "# drop all RTH days, so that we mark a day previous as RTH\n",
    "index = final.query('~date_of_transfer.isna()').index\n",
    "final.drop(index, inplace=True)\n",
    "final.drop(['date_of_transfer'], axis = 1, inplace = True) \n",
    "\n",
    "# Add RTH day as an extra indicator column\n",
    "final = final.merge(\n",
    "            rth_df,\n",
    "            how='left',\n",
    "            left_on=['masterpatientid', 'censusdate'],\n",
    "            right_on=['masterpatientid', 'date_of_transfer']\n",
    "        )\n",
    "\n",
    "final['rth'] = final['rth'].fillna(0)\n",
    "\n",
    "print(final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final['LFS'] = final['admissions_days_since_last_admission']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop extra columns\n",
    "\n",
    "final.drop(\n",
    "        ['date_of_transfer','dateofadmission','census_diff'],\n",
    "        axis=1,\n",
    "        inplace=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# column processing\n",
    "\n",
    "final['client'] = final['masterpatientid'].apply(lambda z: z.split('_')[0])\n",
    "final[\"facilityid\"] = final[\"client\"] + \"_\" + final[\"facilityid\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# manually fill in target columns with 0 so we don't also get na indicators for them\n",
    "final['hosp_target_3_day_hosp'] = final.hosp_target_3_day_hosp.fillna(False)\n",
    "final['hosp_target_7_day_hosp'] = final.hosp_target_7_day_hosp.fillna(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# manual check to make sure we're not including any columns that could leak data\n",
    "with open('/data/processed/columns.txt','w') as f:\n",
    "    for col in final.columns:\n",
    "        f.write(col + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train = final.loc[final.censusdate < EXPERIMENT_DATES['validation_start_date']]\n",
    "valid = final.loc[(final.censusdate >= EXPERIMENT_DATES['validation_start_date']) & (final.censusdate <= EXPERIMENT_DATES['validation_end_date'])]\n",
    "test = final.loc[final.censusdate >= EXPERIMENT_DATES['test_start_date']]\n",
    "\n",
    "print(final.shape)\n",
    "print(train.shape)\n",
    "print(valid.shape)\n",
    "print(test.shape)\n",
    "# assert that the sum of rows of the 3 different dataframes is the same as the original\n",
    "# assert final.shape[0] == (train.shape[0] + valid.shape[0] + test.shape[0])\n",
    "\n",
    "del final\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retain d+1 RTH day\n",
    "\n",
    "# def remove_dplusone(df):\n",
    "#     # drop all RTH days, so that we mark a day previous as RTH\n",
    "#     index = df.query('~rth_dayplusone.isna()').index\n",
    "#     df.drop(index, inplace=True)\n",
    "#     df.drop(['rth_dayplusone'], axis = 1, inplace = True)\n",
    "#     return df\n",
    "\n",
    "# def retain_dplusone(df):\n",
    "#     df.drop(['rth_dayplusone'], axis = 1, inplace = True)\n",
    "#     return df\n",
    "\n",
    "\n",
    "# train = retain_dplusone(train)\n",
    "# valid = retain_dplusone(valid)\n",
    "# test = remove_dplusone(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# start of basic tests - assert we have disjoint sets over time\n",
    "assert train.censusdate.max() < valid.censusdate.min()\n",
    "assert valid.censusdate.max() < test.censusdate.min()\n",
    "assert train.hosp_target_3_day_hosp.mean() < train.hosp_target_7_day_hosp.mean()\n",
    "assert valid.hosp_target_3_day_hosp.mean() < valid.hosp_target_7_day_hosp.mean()\n",
    "assert test.hosp_target_3_day_hosp.mean() < test.hosp_target_7_day_hosp.mean()\n",
    "print('Success...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f'Train set covers {train.censusdate.min()} to {train.censusdate.max()} with 3_day_hosp percentage {train.hosp_target_3_day_hosp.mean()} and 7_day_hosp percentage {train.hosp_target_7_day_hosp.mean()}')\n",
    "print(f'Valid set covers {valid.censusdate.min()} to {valid.censusdate.max()} with 3_day_hosp percentage {valid.hosp_target_3_day_hosp.mean()} and 7_day_hosp percentage {valid.hosp_target_7_day_hosp.mean()}')\n",
    "print(f'Test set covers {test.censusdate.min()} to {test.censusdate.max()} with 3_day_hosp percentage {test.hosp_target_3_day_hosp.mean()} and 7_day_hosp percentage {test.hosp_target_7_day_hosp.mean()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "import os\n",
    "\n",
    "def get_median(job_data):\n",
    "    col = job_data[0]\n",
    "    ser = job_data[1]\n",
    "    return ser.median()\n",
    "\n",
    "def fill_na_train(df):\n",
    "    \"\"\" Get Median value for all columns that contain NaN and all vital columns.\n",
    "    Store these Median values in a file in S3 & use them when ever a column has NaN during prediction\n",
    "    \"\"\"\n",
    "    nan_cols = []\n",
    "    for col in df.columns:\n",
    "        if col in IDEN_COLS:\n",
    "            continue\n",
    "        if (df[col].isnull().any()) or (col.startswith((\"vtl\", \"hosp\"))):\n",
    "            nan_cols.append(col)\n",
    "\n",
    "    job_data = []\n",
    "    for col in nan_cols:\n",
    "        job_data.append((col,df[col]))\n",
    "\n",
    "    with Pool(min(os.cpu_count() - 4, 24)) as pool:\n",
    "        d = pool.map(get_median, job_data)\n",
    "\n",
    "    d = pd.Series(data=d, index=nan_cols)\n",
    "    df = df.fillna(d)\n",
    "\n",
    "    return df, d\n",
    "\n",
    "def fill_na_valid_or_test(df, na_filler):\n",
    "    return df.fillna(na_filler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# fill in any remaining na's - now that we're not forwardfilling past info it's not correct to use a global imputation\n",
    "# hence we impute on the train and apply to the valid and test\n",
    "# We save these na filler values to use them during predictions\n",
    "\n",
    "train, na_filler = fill_na_train(train)\n",
    "valid = fill_na_valid_or_test(valid, na_filler)\n",
    "test = fill_na_valid_or_test(test, na_filler)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Remove the Target values & identification columns\n",
    "# Keep facilityid in idens and add a duplicate field as facility for featurisation\n",
    "def prep(df):\n",
    "    drop_cols = IDEN_COLS + [col for col in df.columns if 'target' in col]\n",
    "\n",
    "    target_3_day = df.hosp_target_3_day_hosp.astype('float32').values\n",
    "    target_7_day = df.hosp_target_7_day_hosp.astype('float32').values\n",
    "    df['facility'] = df['facilityid']    \n",
    "    x = df.drop(columns=drop_cols).reset_index(drop=True)\n",
    "\n",
    "    # Convert all columns to float32 & make facility as categorical data\n",
    "    facility_col = x['facility'].astype('category')\n",
    "    x = x[x.columns.difference(CAT_COLS)].astype('float32')\n",
    "    x['facility'] = facility_col\n",
    " \n",
    "    idens = df.loc[:,IDEN_COLS]\n",
    "\n",
    "    return x, target_3_day, target_7_day, idens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Seperate target, x-frame and identification columns\n",
    "train_x, train_target_3_day, train_target_7_day, train_idens = prep(train)\n",
    "del train\n",
    "valid_x, valid_target_3_day, valid_target_7_day, valid_idens = prep(valid)\n",
    "del valid\n",
    "test_x, test_target_3_day, test_target_7_day, test_idens = prep(test)\n",
    "del test\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# make sure for that x's, targets, an idens all have the same # of rows\n",
    "assert train_x.shape[0] == train_target_3_day.shape[0] == train_target_7_day.shape[0] == train_idens.shape[0]\n",
    "assert valid_x.shape[0] == valid_target_3_day.shape[0] == valid_target_7_day.shape[0] == valid_idens.shape[0]\n",
    "assert test_x.shape[0] == test_target_3_day.shape[0] == test_target_7_day.shape[0] == test_idens.shape[0]\n",
    "\n",
    "# make sure that train, valid, and test have the same # of columns\n",
    "assert train_x.shape[1] == valid_x.shape[1] == test_x.shape[1]\n",
    "\n",
    "# make sure that the idens all have the same # of columns\n",
    "assert train_idens.shape[1] == valid_idens.shape[1] == test_idens.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Save train, test and validation datasets in local folder\n",
    "\n",
    "import pickle;\n",
    "with open(processed_path/'final-train_x.pickle','wb') as f: pickle.dump(train_x, f, protocol=4)\n",
    "with open(processed_path/'final-train_target_3_day.pickle','wb') as f: pickle.dump(train_target_3_day, f, protocol=4)\n",
    "with open(processed_path/'final-train_target_7_day.pickle','wb') as f: pickle.dump(train_target_7_day, f, protocol=4)\n",
    "with open(processed_path/'final-train_idens.pickle','wb') as f: pickle.dump(train_idens, f, protocol=4)\n",
    "\n",
    "with open(processed_path/'final-valid_x.pickle','wb') as f: pickle.dump(valid_x, f, protocol=4)\n",
    "with open(processed_path/'final-valid_target_3_day.pickle','wb') as f: pickle.dump(valid_target_3_day, f, protocol=4)\n",
    "with open(processed_path/'final-valid_target_7_day.pickle','wb') as f: pickle.dump(valid_target_7_day, f, protocol=4)\n",
    "with open(processed_path/'final-valid_idens.pickle','wb') as f: pickle.dump(valid_idens, f, protocol=4)\n",
    "\n",
    "with open(processed_path/'final-test_x.pickle','wb') as f: pickle.dump(test_x, f, protocol=4)\n",
    "with open(processed_path/'final-test_target_3_day.pickle','wb') as f: pickle.dump(test_target_3_day, f, protocol=4)\n",
    "with open(processed_path/'final-test_target_7_day.pickle','wb') as f: pickle.dump(test_target_7_day, f, protocol=4)\n",
    "with open(processed_path/'final-test_idens.pickle','wb') as f: pickle.dump(test_idens, f, protocol=4)\n",
    "\n",
    "with open(processed_path/'final-na_filler.pickle', 'wb') as f: pickle.dump(na_filler, f, protocol=4)\n",
    "\n",
    "print(\"--------------Completed--------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
