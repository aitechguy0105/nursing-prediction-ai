{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import json\n",
    "import pickle\n",
    "import sys\n",
    "import timeit\n",
    "from datetime import timedelta, datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import hyperopt\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from hyperopt import tpe, fmin\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "import mlflow\n",
    "from mlflow import log_metric, log_param, log_artifact, set_tag\n",
    "\n",
    "sys.path.insert(0, '/src')\n",
    "from shared.utils import get_client_class\n",
    "from dataclasses import dataclass\n",
    "from typing import Any\n",
    "import matplotlib.pyplot as plt \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shared.constants import LOCAL_TRAINING_CONFIG_PATH\n",
    "from shared.utils import load_config\n",
    "\n",
    "config = load_config(LOCAL_TRAINING_CONFIG_PATH)\n",
    "training_config = config.training_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JUPYTER NOTEBOOK Magic function\n",
    "# Write the cell content to a file & execute the cell\n",
    "\n",
    "from IPython.core.magic import register_cell_magic\n",
    "\n",
    "\n",
    "@register_cell_magic\n",
    "def write_and_run(line, cell):\n",
    "    argz = line.split()\n",
    "    file = argz[-1]\n",
    "    mode = 'w'\n",
    "    if len(argz) == 2 and argz[0] == '-a':\n",
    "        mode = 'a'\n",
    "    with open(file, mode) as f:\n",
    "        f.write(cell)\n",
    "    get_ipython().run_cell(cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_DATES = training_config.training_metadata.experiment_dates\n",
    "CLIENT = \"+\".join([config.organization_id for config in training_config.ml_model_org_configs])\n",
    "vector_model = training_config.training_metadata.vector_model\n",
    "HYPER_PARAMETER_TUNING = training_config.training_metadata.hyper_parameter_tuning\n",
    "\n",
    "TRAINING_DATA=CLIENT   # trained on which data? e.g. avante + champion\n",
    "SELECTED_MODEL_VERSION = 'saiva-3-day-hosp-v5'    # e.g. v3, v4 or v6 model\n",
    "\n",
    "# Name used to filter models in AWS quicksight & also used as ML Flow experiment name\n",
    "MODEL_DESCRIPTION = f'{CLIENT}-3-day-hosp-v5'   \n",
    "\n",
    "base_models = []\n",
    "model_config = {}\n",
    "\n",
    "\n",
    "# Change train_end_date, validation_start_date for Tuning run\n",
    "if not HYPER_PARAMETER_TUNING:\n",
    "    DESCRIPTION_STRING = 'TRAIN+VALID RUN'\n",
    "    EXPERIMENT_DATES['train_end_date'] = (datetime.strptime(EXPERIMENT_DATES['validation_end_date'], '%Y-%m-%d') - timedelta(days=2)).strftime('%Y-%m-%d')\n",
    "    EXPERIMENT_DATES['validation_start_date'] = (datetime.strptime(EXPERIMENT_DATES['validation_end_date'], '%Y-%m-%d') - timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "else:\n",
    "    DESCRIPTION_STRING = 'TUNING RUN'\n",
    "\n",
    "    \n",
    "def get_training_runs():\n",
    "    if HYPER_PARAMETER_TUNING:\n",
    "        return 9\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def get_valid_data():\n",
    "    if HYPER_PARAMETER_TUNING:\n",
    "        return [valid_data]\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "print(HYPER_PARAMETER_TUNING)    \n",
    "EXPERIMENT_DATES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%write_and_run parameterTunningConfig.py\n",
    "# This cell executes & creates a python file containing the contents of this cell\n",
    "\n",
    "from hyperopt import hp\n",
    "global HYPER_PARAMETER_TUNING\n",
    "\n",
    "lgb_param_space = {\n",
    "     'application': 'binary',\n",
    "     'objective': 'binary',\n",
    "     'metric': 'auc',\n",
    "     #'boosting_type': hp.choice('boosting_type', ['gbdt']),\n",
    "     #'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1),\n",
    "     #'max_depth': hp.quniform('max_depth', -1, 10, 1),\n",
    "     #'min_child_samples': 20,\n",
    "     #'min_child_weight': 0.001,\n",
    "     #'min_split_gain': 0.0,\n",
    "     'n_jobs': -1,\n",
    "    #  'num_leaves': hp.quniform('num_leaves', 30, 300, 1),\n",
    "     #'subsample': hp.uniform('subsample', 0, 1),\n",
    "     #'subsample_for_bin': hp.quniform('subsample_for_bin', 200000, 500000, 1000),\n",
    "     'verbose': 1,\n",
    "     'is_unbalance': True,\n",
    "     #'max_bin': hp.quniform('max_bin', 100,1000, 100),\n",
    "}\n",
    "\n",
    "# Parameter tunning\n",
    "if HYPER_PARAMETER_TUNING:\n",
    "    lgb_param_space['learning_rate'] = hp.uniform('learning_rate', 0.01, 0.05)\n",
    "    lgb_param_space['n_estimators'] = hp.quniform('n_estimators',108,405,10)\n",
    "    lgb_param_space['early_stopping_round'] = 10\n",
    "else:\n",
    "    lgb_param_space['learning_rate'] = 0.017817765000204273\n",
    "    lgb_param_space['n_estimators'] = 151\n",
    "    lgb_param_space['early_stopping_round'] = None\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ======================= CONFIG ENDS =============================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an ML-flow experiment\n",
    "mlflow.set_tracking_uri('http://mlflow.saiva-dev')\n",
    "\n",
    "# Experiment name which appears in ML flow\n",
    "mlflow.set_experiment(MODEL_DESCRIPTION)\n",
    "\n",
    "EXPERIMENT = mlflow.get_experiment_by_name(MODEL_DESCRIPTION)\n",
    "MLFLOW_EXPERIMENT_ID = EXPERIMENT.experiment_id\n",
    "\n",
    "print(f'Experiment ID: {MLFLOW_EXPERIMENT_ID}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "processed_path = Path('/data/processed')\n",
    "processed_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open(processed_path/'final-train_x.pickle','rb') as f: train_x = pickle.load(f)\n",
    "with open(processed_path/'final-train_target_3_day.pickle','rb') as f: train_target_3_day = pickle.load(f)\n",
    "with open(processed_path/'final-train_target_7_day.pickle','rb') as f: train_target_7_day = pickle.load(f)\n",
    "with open(processed_path/'final-train_idens.pickle','rb') as f: train_idens = pickle.load(f)\n",
    "\n",
    "with open(processed_path/'final-valid_x.pickle','rb') as f: valid_x = pickle.load(f)\n",
    "with open(processed_path/'final-valid_target_3_day.pickle','rb') as f: valid_target_3_day = pickle.load(f)\n",
    "with open(processed_path/'final-valid_target_7_day.pickle','rb') as f: valid_target_7_day = pickle.load(f)\n",
    "with open(processed_path/'final-valid_idens.pickle','rb') as f: valid_idens = pickle.load(f)\n",
    "\n",
    "with open(processed_path/'final-test_x.pickle','rb') as f: test_x = pickle.load(f)\n",
    "with open(processed_path/'final-test_target_3_day.pickle','rb') as f: test_target_3_day = pickle.load(f)\n",
    "with open(processed_path/'final-test_target_7_day.pickle','rb') as f: test_target_7_day = pickle.load(f)\n",
    "with open(processed_path/'final-test_idens.pickle','rb') as f: test_idens = pickle.load(f)\n",
    "\n",
    "with open(processed_path/'final-na_filler.pickle', 'rb') as f: na_filler = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_x.shape)\n",
    "print(train_target_3_day.shape)\n",
    "print(valid_x.shape)\n",
    "print(valid_target_3_day.shape)\n",
    "print(test_x.shape)\n",
    "print(test_target_3_day.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('na_fillers column count: ', len(na_filler.keys()))\n",
    "print('feature column count: ', len(train_x.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_data = lgb.Dataset(train_x, label=train_target_3_day)\n",
    "valid_data = lgb.Dataset(valid_x, label=valid_target_3_day)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BaseModel:\n",
    "    model_name: str\n",
    "    model_type: str\n",
    "    model: Any\n",
    "\n",
    "\n",
    "def get_facilities_from_train_data(df):\n",
    "    return list(df.facilityid.unique())\n",
    "\n",
    "def get_date_diff(start_date, end_date):\n",
    "    diff = (pd.to_datetime(end_date) - pd.to_datetime(start_date)).days\n",
    "    return f'{diff} days'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" Calculate how many transfers were caught up to a particular rank.\n",
    "hospital_cumsum - how many transfers caught upto a certain rank. Eg: Caught transfers till 10 th rank\n",
    "Relavant - total transfers per day per facility\n",
    "\"\"\"\n",
    "\n",
    "def precision_recall_at_k(group):\n",
    "    group.loc[:, \"hospitalized_cumsum\"] = group.hospitalized_within_pred_range.cumsum()\n",
    "    group.loc[:, \"total_relevant\"] = group.hospitalized_within_pred_range.sum()\n",
    "    group.loc[:, \"recall_at_k\"] = group.hospitalized_cumsum / group.total_relevant\n",
    "\n",
    "    return group.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def performance_base_processing(idens, preds, target_3_day):\n",
    "    base = idens.copy()\n",
    "    base['predictionvalue'] = preds\n",
    "    base['hospitalized_within_pred_range'] = target_3_day\n",
    "    base['predictionrank'] = base.groupby(['censusdate', 'facilityid']).predictionvalue.rank(ascending=False)\n",
    "    base = base.sort_values('predictionrank', ascending=True)\n",
    "\n",
    "    performance_base = (\n",
    "        base.groupby([\"facilityid\", \"censusdate\"])\n",
    "        .apply(precision_recall_at_k)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # Get max rank per facility per census day & then get median among max rank across facility\n",
    "    # This is calcludated inorder to find the top 10%, 15% value for any given facility\n",
    "    facility_pats = performance_base.groupby(\n",
    "        ['censusdate','facilityid']\n",
    "    ).predictionrank.max().reset_index().groupby(\n",
    "        'facilityid'\n",
    "    ).predictionrank.median().reset_index()\n",
    "\n",
    "    return performance_base, facility_pats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_auc(target_3_day, preds):\n",
    "    total_aucroc = roc_auc_score(target_3_day, preds)\n",
    "    total_aucroc_25_fpr = roc_auc_score(target_3_day, preds, max_fpr=0.25)\n",
    "    total_ap = average_precision_score(target_3_day, preds)\n",
    "\n",
    "    return total_aucroc, total_aucroc_25_fpr, total_ap\n",
    "\n",
    "def get_pline_recall(performance_base):\n",
    "    at_rank_15 = performance_base.loc[performance_base.predictionrank == 15]\n",
    "    pline_recall_at_rank_15 = at_rank_15.hospitalized_cumsum.sum() / at_rank_15.total_relevant.sum()\n",
    "    \n",
    "    # Each day we predict 15 patients, total prediciton = 15 * number of census days in the dataset\n",
    "    pline_precision_at_rank_15 = at_rank_15.hospitalized_cumsum.sum() / (at_rank_15.shape[0] * 15)\n",
    "    pline_bscore = f_beta_score(pline_precision_at_rank_15, pline_recall_at_rank_15)\n",
    "\n",
    "    return pline_recall_at_rank_15, pline_precision_at_rank_15, pline_bscore\n",
    "    \n",
    "    \n",
    "def get_rth_recall(performance_base):\n",
    "    \"\"\" If Total RTHs before processing & after processing varies \n",
    "    it means there were 2 RTHs within 3 day span. \n",
    "    ie. A RTH occured on 18th Nov and another happened on 21st Nov and we have predicted it on 18th Nov\n",
    "    As part of recall calulation we get credit for both these RTHs\n",
    "    \"\"\"\n",
    "    \n",
    "    rth_df = performance_base.query('rth == 1')[['censusdate', 'facilityid', 'masterpatientid']]\n",
    "    print('Total RTHs = ',rth_df.shape[0])\n",
    "    rth_df.rename(columns={'censusdate': 'rth_censusdate'},\n",
    "          inplace=True, errors='raise')\n",
    "    \n",
    "    df = performance_base.merge(\n",
    "        rth_df, on=['facilityid', 'masterpatientid']\n",
    "    )\n",
    "    df = df[df.censusdate <= df.rth_censusdate]\n",
    "    df['date_diff'] = (df['rth_censusdate'] - df['censusdate']).dt.days\n",
    "    df = df[df.date_diff <= 3]\n",
    "    df['min_predictionrank'] = df.groupby(['rth_censusdate', 'facilityid', 'masterpatientid'])['predictionrank'].transform('min')\n",
    "    df = df.query('rth == 1')\n",
    "    print('Total RTHs after processing = ',df.shape[0])\n",
    "    recall = df.query('min_predictionrank <= 15').shape[0] / df.shape[0]\n",
    "    print('Total Recall = ',recall)\n",
    "    \n",
    "    _df = df.query('LFS <= 30')\n",
    "    recall_LE30 = _df.query('min_predictionrank <= 15').shape[0] / _df.shape[0]\n",
    "    \n",
    "    _df = df.query('LFS > 30')\n",
    "    recall_G30 = _df.query('min_predictionrank <= 15').shape[0] / _df.shape[0]\n",
    "    \n",
    "    return recall, recall_LE30, recall_G30\n",
    "       \n",
    "    \n",
    "def f_beta_score(precision, recall, beta=2):\n",
    "    return ((1+beta**2)*(precision*recall)) / ((beta**2)*precision + recall)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_auc_curve(actual_y, preds_y, aucroc, run_id):\n",
    "    filename = f'auc_curve_{run_id}.png'\n",
    "    fpr, tpr, thresh = metrics.roc_curve(actual_y, preds_y)\n",
    "    plt.plot(fpr,tpr)\n",
    "    plt.xlabel(\"AUC=\"+str(aucroc))\n",
    "    plt.savefig(filename)\n",
    "    log_artifact(filename)  # Export to MlFlow\n",
    "    \n",
    "\n",
    "def get_pos_neg(np_series):\n",
    "    total = len(np_series)\n",
    "    neg = np.count_nonzero(np_series==0)\n",
    "    pos = np.count_nonzero(np_series==1)\n",
    "#     neg_p = round((neg * 100) / total, 3)\n",
    "#     pos_p = round((pos * 100) / total, 3)\n",
    "    return pos, neg, round(neg/pos, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test_set(model, selected_modelid, run_id, test_start_date, test_end_date, x_df, target_3_day, idens):\n",
    "    \n",
    "    pos, neg, n2p_ratio = get_pos_neg(target_3_day)\n",
    "    \n",
    "    log_param('12_TEST_START_DATE', test_start_date)\n",
    "    log_param('13_TEST_END_DATE', test_end_date)\n",
    "    log_param('14_TEST_DURATION', get_date_diff(test_start_date, test_end_date))\n",
    "    log_param('15_TEST_POS_COUNT', pos)\n",
    "    log_param('16_TEST_NEG_COUNT', neg)\n",
    "    log_param('17_TEST_N2P_RATIO', n2p_ratio)\n",
    "    log_param('18_MODEL_DESCRIPTION', SELECTED_MODEL_VERSION)\n",
    "    log_param('19_MODEL_ID', selected_modelid)\n",
    "    \n",
    "    # ===========================Predict on test dataset=======================\n",
    "    preds = model.predict(x_df)\n",
    "    print(\"=============================Prediction completed...=============================\")\n",
    "    # =========================== TOTAL AUCROC on TEST SET ===========================\n",
    "    total_aucroc,total_aucroc_25_fpr, total_ap = get_auc(target_3_day, preds)\n",
    "\n",
    "    performance_test_base, facility_test_pats = performance_base_processing(idens, preds, target_3_day)\n",
    "\n",
    "    pline_recall_at_rank_15, pline_precision_at_rank_15, pline_bscore = get_pline_recall(\n",
    "        performance_test_base\n",
    "    )\n",
    "    recall, recall_LE30, recall_G30 = get_rth_recall(performance_test_base)\n",
    "    \n",
    "    log_metric(f'01_aucroc', total_aucroc)\n",
    "    log_metric(f'02_RTH_recall_at_rank_15', recall)\n",
    "    log_metric(f'03_RTH_recall_LOS_LE30_at_rank_15', recall_LE30)\n",
    "    log_metric(f'04_RTH_recall_LOS_G30_at_rank_15', recall_G30)\n",
    "    log_metric(f'05_Pline_recall_at_rank_15', pline_recall_at_rank_15)\n",
    "    log_metric(f'06_Pline_precision_at_rank_15', pline_precision_at_rank_15)\n",
    "    log_metric(f'07_Pline_bscore_at_rank_15',pline_bscore)\n",
    "    log_metric(f'08_ap', total_aucroc_25_fpr)\n",
    "    log_metric(f'09_aucroc_at_.25_fpr', total_ap)\n",
    "    \n",
    "    generate_auc_curve(target_3_day, preds, total_aucroc, run_id)\n",
    "    \n",
    "    return total_aucroc, pline_recall_at_rank_15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## =================== Model Training ==================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ITERATION_NUMBER = 1\n",
    "\n",
    "def objective(params):\n",
    "    global ITERATION_NUMBER\n",
    "    print(f'Training LGB models with parameter: {params}')\n",
    "    with mlflow.start_run():\n",
    "        start_time = timeit.default_timer()\n",
    "        run_uuid = mlflow.active_run().info.run_uuid\n",
    "\n",
    "        # Convert these params to Integer since LGBM excepts it as Int\n",
    "        params['n_estimators'] = int(params.get('n_estimators'))\n",
    "        #params['num_leaves'] = int(params.get('num_leaves'))\n",
    "        #params['max_depth'] = int(params.get('max_depth'))\n",
    "        #params['subsample_for_bin'] = int(params.get('subsample_for_bin'))\n",
    "        #params['max_bin'] = int(params.get('max_bin'))\n",
    "        \n",
    "        pos, neg, n2p_ratio = get_pos_neg(train_target_3_day)\n",
    "        facilities = get_facilities_from_train_data(train_idens)\n",
    "        \n",
    "        # Log the train, validation & test date ranges\n",
    "        log_param('00_DESCRIPTION', f'{DESCRIPTION_STRING} {ITERATION_NUMBER}')\n",
    "        ITERATION_NUMBER += 1\n",
    "        log_param('02_TRAINING_DATA',TRAINING_DATA)\n",
    "        log_param('03_FACILITIES', (', '.join(facilities)))\n",
    "        log_param('04_FACILITIES_COUNT', len(facilities))\n",
    "        log_param('05_TRAIN_START_DATE', EXPERIMENT_DATES['train_start_date'])\n",
    "        log_param('06_TRAIN_END_DATE', EXPERIMENT_DATES['train_end_date'])\n",
    "        log_param('07_TRAIN_DURATION', get_date_diff(EXPERIMENT_DATES['train_start_date'], EXPERIMENT_DATES['train_end_date']))\n",
    "        log_param('08_TRAIN_POS_COUNT', pos)\n",
    "        log_param('09_TRAIN_NEG_COUNT', neg)\n",
    "        log_param('10_TRAIN_N2P_RATIO', n2p_ratio)\n",
    "        log_param('11_TRAIN_FEATURE_COUNT', train_x.shape[1])\n",
    "\n",
    "        log_param('001_HYPEROPT_VERSION',hyperopt.__version__)\n",
    "        log_param('002_LGBM_VERSION',lgb.__version__)\n",
    "\n",
    "        for param in params:\n",
    "            log_param(f'hp__{param}', params[param])\n",
    "\n",
    "        set_tag('model', 'lgb')\n",
    "        print(\"=============================Training started...=============================\")\n",
    "        model = lgb.train(params, train_set=train_data, valid_sets=get_valid_data())\n",
    "        print(\"=============================Training completed...=============================\")\n",
    "        gc.collect()\n",
    "\n",
    "        if HYPER_PARAMETER_TUNING:\n",
    "            # Log validation metric only during tuning\n",
    "            aucroc, pline_recall_at_rank_15 = run_test_set(\n",
    "                model,\n",
    "                run_uuid,\n",
    "                run_uuid,\n",
    "                EXPERIMENT_DATES['validation_start_date'],\n",
    "                EXPERIMENT_DATES['validation_end_date'],\n",
    "                valid_x,\n",
    "                valid_target_3_day,\n",
    "                valid_idens\n",
    "            )\n",
    "            log_param('p__best_score', model.best_score['valid_0']['auc'])\n",
    "            log_param('p__best_iteration', model.best_iteration)\n",
    "        else:\n",
    "            # Run testset only on test+valid trained model\n",
    "            aucroc, pline_recall_at_rank_15 = run_test_set(\n",
    "                model,\n",
    "                run_uuid,\n",
    "                run_uuid,\n",
    "                EXPERIMENT_DATES['test_start_date'],\n",
    "                EXPERIMENT_DATES['test_end_date'],\n",
    "                test_x,\n",
    "                test_target_3_day,\n",
    "                test_idens\n",
    "            )\n",
    "        \n",
    "        model_config = {\n",
    "            'modelid':run_uuid,\n",
    "            'dayspredictionvalid':3,\n",
    "            'model_algo': 'lgbm',\n",
    "            'predictiontask': 'hospitalization',\n",
    "            'modeldescription' : MODEL_DESCRIPTION,\n",
    "            'client_data_trained_on' : TRAINING_DATA,\n",
    "            'vector_model' : vector_model,\n",
    "            'model_s3_folder': MLFLOW_EXPERIMENT_ID,\n",
    "            'prospectivedatestart':f'{datetime.now().date()}',\n",
    "            'training_start_date':EXPERIMENT_DATES['train_start_date'],\n",
    "            'training_end_date':EXPERIMENT_DATES['train_end_date'],\n",
    "            'validation_start_date': EXPERIMENT_DATES['validation_start_date'],\n",
    "            'validation_end_date': EXPERIMENT_DATES['validation_end_date'],\n",
    "            'test_start_date': EXPERIMENT_DATES['test_start_date'],\n",
    "            'test_end_date': EXPERIMENT_DATES['test_end_date'],\n",
    "            'test_auc': str(aucroc),\n",
    "            'test_recall_at_rank_15': str(pline_recall_at_rank_15),\n",
    "#             'facility_wise_auc': facility_auc_dict,\n",
    "        }\n",
    "\n",
    "        base_models.append(model_config)\n",
    "        \n",
    "        # ================= Save model related artifacts =========================\n",
    "        with open('./model_config.json', 'w') as outfile: json.dump(model_config, outfile)\n",
    "        log_artifact(f'./model_config.json')\n",
    "        \n",
    "        base_model = BaseModel(model_name=run_uuid, model_type='lgb', model=model)\n",
    "        with open(f'./{run_uuid}.pickle', 'wb') as f: pickle.dump(base_model, f)\n",
    "        log_artifact(f'./{run_uuid}.pickle')\n",
    "        \n",
    "        input_features = pd.DataFrame(train_x.columns, columns=['feature'])\n",
    "        input_features.to_csv(f'./input_features.csv', index=False)\n",
    "        log_artifact(f'./input_features.csv')\n",
    "        \n",
    "        with open('./na_filler.pickle','wb') as f: pickle.dump(na_filler, f, protocol=4)\n",
    "        log_artifact('./na_filler.pickle')\n",
    "\n",
    "        # =============== Save the code used to training in S3 ======================\n",
    "        for notebook in list(Path('/src/notebooks').glob('0*.ipynb')):\n",
    "            log_artifact(str(notebook))\n",
    "\n",
    "        for shared_code in list(Path('/src/shared').glob('*.py')):\n",
    "            log_artifact(str(shared_code))\n",
    "\n",
    "        for client_code in list(Path('/src/clients').glob('*.py')):\n",
    "            log_artifact(str(client_code))\n",
    "\n",
    "        log_artifact('./parameterTunningConfig.py')\n",
    "        \n",
    "        t_sec = round(timeit.default_timer() - start_time)\n",
    "        (t_min, t_sec) = divmod(t_sec,60)\n",
    "        (t_hour,t_min) = divmod(t_min,60) \n",
    "        log_param('01_RUN_TIME', '{}h:{}m:{}s'.format(t_hour,t_min,t_sec))\n",
    "        \n",
    "        return 1 - aucroc\n",
    "\n",
    "\n",
    "best = fmin(fn=objective,\n",
    "        space=lgb_param_space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=get_training_runs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open('./model_config.json', 'w') as outfile: json.dump(base_models, outfile)\n",
    "\n",
    "# base_models    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run():\n",
    "    log_param('00_DESCRIPTION', 'UPLOAD LOGS')\n",
    "    log_artifact('./lgbm_training.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ==================== Load Model from local folder ========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECTED_MODELID = 'ec308e6c819e4ba18f14f27559982070'\n",
    "\n",
    "# model_path = f'./{modelid}/artifacts/{SELECTED_MODELID}.pickle'\n",
    "model_path = f'{SELECTED_MODELID}.pickle'\n",
    "\n",
    "with open(model_path, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "        model = model.model\n",
    "        \n",
    "if not SELECTED_MODELID:\n",
    "    raise Exception('Configure SELECTED_MODELID')\n",
    "    exit() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ======================= Download Models from S3 ======================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import subprocess\n",
    "\n",
    "# modelid = 'fd27a573c801437488e4c2b432205834'\n",
    "\n",
    "# subprocess.run(\n",
    "#                 f'aws s3 sync s3://saiva-models/165/{modelid} {modelid}',\n",
    "#                 shell=True,\n",
    "#                 stderr=subprocess.DEVNULL,\n",
    "#                 stdout=subprocess.DEVNULL,\n",
    "#             )\n",
    "\n",
    "# print(test_x.shape)\n",
    "# all_feats = pd.read_csv(\n",
    "#             f'./{modelid}/artifacts/input_features.csv'\n",
    "#         )\n",
    "\n",
    "# test_x = test_x.reindex(columns=all_feats.feature, fill_value=0)\n",
    "# print(test_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## =================== Run TestSet on any loaded Model above ==============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "# with mlflow.start_run():\n",
    "#     run_uuid = mlflow.active_run().info.run_uuid\n",
    "#     log_param('00_DESCRIPTION', 'TEST RUN')\n",
    "#     log_param('01_CLIENT', TRAINING_DATA)\n",
    "#     log_param('02_FACILITIES', get_facilities_from_train_data(test_idens))\n",
    "#     test_aucroc, test_recall_at_rank_15 = run_test_set(model, SELECTED_MODELID, run_uuid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## =================== List Feature Importance of the model ===================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imp = (\n",
    "    pd.DataFrame({\n",
    "        'feature': model.feature_name(),\n",
    "        'importance': model.feature_importance(importance_type='gain'),  # split\n",
    "    })\n",
    "    .sort_values('importance', ascending=False)\n",
    ")\n",
    "feature_imp.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb.plot_importance(model, max_num_features=50, figsize=(15,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ================ Run Shap Explanations for Test Set ================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = shap.TreeExplainer(model).shap_values(test_x)\n",
    "\n",
    "shap.summary_plot(shap_values, test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shap takes lot of time to run across all test dataset. Since certain index and run shap for faster results \n",
    "\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(test_x.iloc[0:50])\n",
    "\n",
    "shap_results = []\n",
    "\n",
    "for idx, row in test_x.iloc[0:50].iterrows():\n",
    "    shaps = pd.DataFrame(\n",
    "        {\n",
    "            \"feature\": test_x.columns,\n",
    "            \"attribution_score\": shap_values[1][idx],\n",
    "            \"feature_value\": test_x.iloc[idx],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    shaps[\"masterpatientid\"] = test_idens.iloc[idx].masterpatientid\n",
    "    shaps[\"facilityid\"] = test_idens.iloc[idx].facilityid\n",
    "    shaps[\"censusdate\"] = test_idens.iloc[idx].censusdate\n",
    "\n",
    "    shap_results.append(shaps)\n",
    "\n",
    "results = pd.concat(shap_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.query('attribution_score >= 0.1').sort_values(by=['attribution_score'], ascending=False)['feature'].value_counts().head(25)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
