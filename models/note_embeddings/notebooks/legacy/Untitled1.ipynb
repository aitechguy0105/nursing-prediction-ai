{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_x_with_notes.pickle', 'rb') as f:\n",
    "    train_x = pickle.load(f)\n",
    "\n",
    "with open('train_idens_with_notes.pickle', 'rb') as f:\n",
    "    train_idens = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-01-01 00:00:00 2018-04-30 00:00:00\n"
     ]
    }
   ],
   "source": [
    "print(train_idens['censusdate'].min(), train_idens['censusdate'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('valid_x_with_notes.pickle', 'rb') as f:\n",
    "    valid_x = pickle.load(f)\n",
    "\n",
    "with open('valid_idens_with_notes.pickle', 'rb') as f:\n",
    "    valid_idens = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-01 00:00:00 2018-08-19 00:00:00\n"
     ]
    }
   ],
   "source": [
    "print(valid_idens['censusdate'].min(), valid_idens['censusdate'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = pd.read_pickle('data/processed/avante_notes.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-01-01 00:01:52.583000 2019-06-25 23:59:34.697000\n"
     ]
    }
   ],
   "source": [
    "print(notes['CreatedDate'].min(), notes['CreatedDate'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.engine.url import URL\n",
    "\n",
    "from sklearn.ensemble import forest, gradient_boosting\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.feature_selection import SelectFromModel, SelectKBest\n",
    "\n",
    "import mlflow\n",
    "from mlflow import log_metric, log_param, log_artifact\n",
    "from mlflow.sklearn import log_model\n",
    "from sklearn.model_selection import ParameterGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "rehosps = pd.read_parquet('/code/data/raw/patient_rehosps.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "rehosps['censusdate'] = pd.DatetimeIndex(rehosps['dateoftransfer']).normalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = pd.read_parquet('/code/data/processed/combined.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = combined[['censusdate', 'masterpatientid', 'patientid', 'facilityid', 'hosp_target_3_day_hosp']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "target['masterpatientid'] = target['masterpatientid'].apply(lambda x: f'avante_{x}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_df = train_idens.merge(target, on=['censusdate', 'masterpatientid'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_y_df = valid_idens.merge(target, on=['censusdate', 'masterpatientid'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_y_df) == len(valid_idens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>masterpatientid</th>\n",
       "      <th>censusdate</th>\n",
       "      <th>patientid</th>\n",
       "      <th>facilityid</th>\n",
       "      <th>hosp_target_3_day_hosp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>avante_100054</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>268105</td>\n",
       "      <td>9</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>avante_100063</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>267941</td>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>avante_100084</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>268060</td>\n",
       "      <td>8</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>avante_100373</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>268634</td>\n",
       "      <td>15</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>avante_1004</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>1004</td>\n",
       "      <td>10</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  masterpatientid censusdate  patientid  facilityid hosp_target_3_day_hosp\n",
       "0   avante_100054 2017-01-01     268105           9                   None\n",
       "1   avante_100063 2017-01-01     267941          11                  False\n",
       "2   avante_100084 2017-01-01     268060           8                   None\n",
       "3   avante_100373 2017-01-01     268634          15                   None\n",
       "4     avante_1004 2017-01-01       1004          10                   None"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     9\n",
       "1    11\n",
       "2     8\n",
       "3    15\n",
       "4    10\n",
       "5    11\n",
       "6     4\n",
       "7    11\n",
       "8    12\n",
       "9     3\n",
       "Name: facilityid, dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x['facilityid'].astype(int)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: 'target_hosp_3_day_with_note_text_new' does not exist. Creating a new experiment\n"
     ]
    }
   ],
   "source": [
    "mlflow.set_experiment('target_hosp_3_day_with_note_text_new')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators':[48],\n",
    "    'feat_select_threshold': ['64*median'],\n",
    "    'max_features': ['auto'], \n",
    "    'min_samples_leaf': [200], \n",
    "    'class_weight': [None],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = train_y_df['hosp_target_3_day_hosp'].fillna(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_y = valid_y_df['hosp_target_3_day_hosp'].fillna(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying hyperparamters: {'class_weight': None, 'feat_select_threshold': '64*median', 'max_features': 'auto', 'min_samples_leaf': 200, 'n_estimators': 48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 48 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 48\n",
      "building tree 2 of 48\n",
      "building tree 3 of 48\n",
      "building tree 4 of 48\n",
      "building tree 5 of 48\n",
      "building tree 6 of 48\n",
      "building tree 7 of 48\n",
      "building tree 8 of 48\n",
      "building tree 9 of 48\n",
      "building tree 10 of 48\n",
      "building tree 11 of 48\n",
      "building tree 12 of 48\n",
      "building tree 13 of 48\n",
      "building tree 14 of 48\n",
      "building tree 15 of 48\n",
      "building tree 16 of 48\n",
      "building tree 17 of 48building tree 18 of 48\n",
      "building tree 19 of 48\n",
      "\n",
      "building tree 20 of 48\n",
      "building tree 21 of 48\n",
      "building tree 22 of 48building tree 23 of 48\n",
      "building tree 24 of 48\n",
      "\n",
      "building tree 25 of 48building tree 26 of 48\n",
      "\n",
      "building tree 27 of 48\n",
      "building tree 28 of 48\n",
      "building tree 29 of 48\n",
      "building tree 30 of 48\n",
      "building tree 31 of 48\n",
      "building tree 32 of 48\n",
      "building tree 33 of 48\n",
      "building tree 34 of 48\n",
      "building tree 35 of 48\n",
      "building tree 36 of 48\n",
      "building tree 37 of 48\n",
      "building tree 38 of 48building tree 39 of 48\n",
      "building tree 40 of 48\n",
      "building tree 41 of 48\n",
      "building tree 42 of 48\n",
      "building tree 43 of 48\n",
      "building tree 44 of 48\n",
      "\n",
      "building tree 45 of 48\n",
      "building tree 46 of 48\n",
      "building tree 47 of 48\n",
      "building tree 48 of 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   4 out of  48 | elapsed:   34.3s remaining:  6.3min\n",
      "[Parallel(n_jobs=-1)]: Done  21 out of  48 | elapsed:   36.3s remaining:   46.7s\n",
      "[Parallel(n_jobs=-1)]: Done  38 out of  48 | elapsed:   37.3s remaining:    9.8s\n",
      "[Parallel(n_jobs=-1)]: Done  48 out of  48 | elapsed:   39.4s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 48 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 48\n",
      "building tree 2 of 48\n",
      "building tree 3 of 48\n",
      "building tree 4 of 48\n",
      "building tree 5 of 48\n",
      "building tree 6 of 48\n",
      "building tree 7 of 48\n",
      "building tree 8 of 48\n",
      "building tree 9 of 48\n",
      "building tree 10 of 48\n",
      "building tree 11 of 48\n",
      "building tree 12 of 48\n",
      "building tree 13 of 48\n",
      "building tree 14 of 48\n",
      "building tree 15 of 48building tree 16 of 48\n",
      "building tree 17 of 48\n",
      "building tree 18 of 48\n",
      "building tree 19 of 48\n",
      "building tree 20 of 48\n",
      "building tree 21 of 48\n",
      "building tree 22 of 48\n",
      "building tree 23 of 48\n",
      "building tree 24 of 48\n",
      "building tree 25 of 48\n",
      "building tree 26 of 48\n",
      "building tree 27 of 48\n",
      "\n",
      "building tree 28 of 48\n",
      "building tree 29 of 48\n",
      "building tree 30 of 48\n",
      "building tree 31 of 48\n",
      "building tree 32 of 48\n",
      "building tree 33 of 48building tree 34 of 48\n",
      "\n",
      "building tree 35 of 48\n",
      "building tree 36 of 48\n",
      "building tree 37 of 48\n",
      "building tree 38 of 48\n",
      "building tree 39 of 48\n",
      "building tree 40 of 48\n",
      "building tree 41 of 48\n",
      "building tree 42 of 48\n",
      "building tree 43 of 48\n",
      "building tree 44 of 48\n",
      "building tree 45 of 48\n",
      "building tree 46 of 48\n",
      "building tree 47 of 48\n",
      "building tree 48 of 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   4 out of  48 | elapsed:   33.7s remaining:  6.2min\n",
      "[Parallel(n_jobs=-1)]: Done  21 out of  48 | elapsed:   36.5s remaining:   46.9s\n",
      "[Parallel(n_jobs=-1)]: Done  38 out of  48 | elapsed:   37.8s remaining:   10.0s\n",
      "[Parallel(n_jobs=-1)]: Done  48 out of  48 | elapsed:   39.5s finished\n",
      "[Parallel(n_jobs=48)]: Using backend ThreadingBackend with 48 concurrent workers.\n",
      "[Parallel(n_jobs=48)]: Done   4 out of  48 | elapsed:    0.4s remaining:    4.4s\n",
      "[Parallel(n_jobs=48)]: Done  21 out of  48 | elapsed:    0.4s remaining:    0.6s\n",
      "[Parallel(n_jobs=48)]: Done  38 out of  48 | elapsed:    0.5s remaining:    0.1s\n",
      "[Parallel(n_jobs=48)]: Done  48 out of  48 | elapsed:    0.5s finished\n",
      "[Parallel(n_jobs=48)]: Using backend ThreadingBackend with 48 concurrent workers.\n",
      "[Parallel(n_jobs=48)]: Done   4 out of  48 | elapsed:    0.1s remaining:    0.7s\n",
      "[Parallel(n_jobs=48)]: Done  21 out of  48 | elapsed:    0.1s remaining:    0.1s\n",
      "[Parallel(n_jobs=48)]: Done  38 out of  48 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=48)]: Done  48 out of  48 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'na_filler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-0643f4caa6fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mlog_artifact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./input_features.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./na_filler.pickle'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mna_filler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mlog_artifact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./na_filler.pickle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'na_filler' is not defined"
     ]
    }
   ],
   "source": [
    "for config in ParameterGrid(param_grid):\n",
    "    print(f'Trying hyperparamters: {config}')\n",
    "    \n",
    "    with mlflow.start_run():\n",
    "        feat_est = forest.RandomForestClassifier(\n",
    "            n_estimators=config['n_estimators'],\n",
    "            max_features=config['max_features'],\n",
    "            min_samples_leaf=config['min_samples_leaf'],\n",
    "            class_weight=config['class_weight'],\n",
    "            n_jobs=-1,\n",
    "            verbose=3\n",
    "        )\n",
    "\n",
    "        feat_selector = SelectFromModel(feat_est, threshold=config['feat_select_threshold'])\n",
    "        train_x_new = feat_selector.fit_transform(train_x, train_y)\n",
    "\n",
    "        clf = forest.RandomForestClassifier(\n",
    "            n_estimators=config['n_estimators'],\n",
    "            max_features=config['max_features'],\n",
    "            min_samples_leaf=config['min_samples_leaf'],\n",
    "            class_weight=config['class_weight'],\n",
    "            n_jobs=-1,\n",
    "            verbose=3\n",
    "        )\n",
    "\n",
    "        clf.fit(train_x_new, train_y.astype(int))\n",
    "\n",
    "        valid_x_new = feat_selector.transform(valid_x)\n",
    "\n",
    "        train_preds = clf.predict_proba(train_x_new)\n",
    "        valid_preds = clf.predict_proba(valid_x_new)\n",
    "\n",
    "        for param in config:\n",
    "            log_param(param, config[param])\n",
    "\n",
    "        log_metric('train_aucroc', roc_auc_score(train_y, [pred[1] for pred in train_preds]))\n",
    "        log_metric('train_ap', average_precision_score(train_y, [pred[1] for pred in train_preds]))\n",
    "        log_metric('valid_aucroc', roc_auc_score(valid_y, [pred[1] for pred in valid_preds]))\n",
    "        log_metric('valid_ap', average_precision_score(valid_y, [pred[1] for pred in valid_preds]))\n",
    "\n",
    "        log_model(feat_selector, 'feat_selector')\n",
    "        log_model(clf, \"model\")\n",
    "\n",
    "        feature_selected_features = pd.DataFrame(zip(train_x.columns[feat_selector.get_support()], clf.feature_importances_), columns=['feature', 'rf_importance']).sort_values('rf_importance', ascending=False)\n",
    "        feature_selected_features.to_csv('./feature_selected_features.csv', index=False)\n",
    "        log_artifact('./feature_selected_features.csv')\n",
    "\n",
    "        input_features = pd.DataFrame(train_x.columns, columns=['feature'])\n",
    "        input_features.to_csv('./input_features.csv', index=False)\n",
    "        log_artifact('./input_features.csv')\n",
    "        \n",
    "        with open('./na_filler.pickle','wb') as f: pickle.dump(na_filler, f, protocol=4)\n",
    "        log_artifact('./na_filler.pickle')\n",
    "        \n",
    "        print('valid_aucroc:', roc_auc_score(valid_y, [pred[1] for pred in valid_preds]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_aucroc: 0.7873359645676679\n"
     ]
    }
   ],
   "source": [
    "print('valid_aucroc:', roc_auc_score(valid_y, [pred[1] for pred in valid_preds]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(960181,)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(141455, 2808)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(960181, 2808)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
