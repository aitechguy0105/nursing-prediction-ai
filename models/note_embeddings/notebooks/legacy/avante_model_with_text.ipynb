{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.engine.url import URL\n",
    "\n",
    "from sklearn.ensemble import forest, gradient_boosting\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.feature_selection import SelectFromModel, SelectKBest\n",
    "\n",
    "import mlflow\n",
    "from mlflow import log_metric, log_param, log_artifact\n",
    "from mlflow.sklearn import log_model\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "#import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_start_date = '2017-01-01'\n",
    "train_end_date = '2018-05-31'\n",
    "valid_end_date = '2019-10-31'\n",
    "test_end_date = '2019-06-25'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/code/data/processed/full_notes.pickle', 'rb') as f:\n",
    "    notes_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = notes_df['provider'] == 'avante'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notes_df['provider'] = 'avante'\n",
    "avante_note_text_mask = (notes_df['Section'] == 'Note Text') & (notes_df['provider'] == 'avante')\n",
    "avante_notes_df = notes_df.loc[avante_note_text_mask,].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_patient_ids = pd.read_parquet('s3://saiva-restricted-data/raw/avante/master_patient_lookup.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "avante_notes_df = avante_notes_df.merge(master_patient_ids, left_on=['PatientID', 'FacilityID'], right_on=['patientid', 'facilityid'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = pd.read_pickle('/code/data/processed/avante/03-train_target_3_day.pickle')\n",
    "valid_y = pd.read_pickle('/code/data/processed/avante/03-valid_target_3_day.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = pd.read_pickle('/code/data/processed/avante/03-train_x.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_x = pd.read_pickle('/code/data/processed/avante/03-valid_x.pickle')\n",
    "test_x = pd.read_pickle('/code/data/processed/avante/03-test_x.pickle')\n",
    "# train_y = pd.read_pickle('/code/data/processed/train_y.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = pd.read_pickle('/code/data/processed/avante/03-train_target_3_day.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_cols = list(train_x.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1174744"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "avante_notes_df['censusdate'] = avante_notes_df['CreatedDate'].dt.normalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_patient_days = avante_notes_df.groupby(['masterpatientid', 'censusdate'])\n",
    "\n",
    "embedding_cols = [c for c in avante_notes_df.columns if c.startswith('e_')]\n",
    "\n",
    "note_aggs = notes_patient_days[embedding_cols].sum()\n",
    "note_aggs.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note_aggs['masterpatientid'] = note_aggs['masterpatientid'].apply(lambda x: f'avante_{int(x)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = pd.read_pickle('/code/data/processed/avante/03-train_idens.pickle')\n",
    "# valid_ids = pd.read_pickle('/code/data/processed/avante/03-valid_idens.pickle')\n",
    "# test_ids = pd.read_pickle('/code/data/processed/avante/03-test_idens.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x 1174744\n",
      "train_ids 1174744\n"
     ]
    }
   ],
   "source": [
    "print('train_x', len(train_x))\n",
    "print('train_ids', len(train_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = pd.concat([train_ids.reset_index(drop=True), train_x.reset_index(drop=True)], axis=1)\n",
    "# valid_x = pd.concat([valid_ids.reset_index(drop=True), valid_x.reset_index(drop=True)], axis=1)\n",
    "# test_x = pd.concat([test_ids.reset_index(drop=True), test_x.reset_index(drop=True)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x.merge(note_aggs, on=['masterpatientid', 'censusdate'], how='left')\n",
    "# valid_x = valid_x.merge(note_aggs, on=['masterpatientid', 'censusdate'], how='left')\n",
    "# test_x = test_x.merge(note_aggs, on=['masterpatientid', 'censusdate'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x['y'] = train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1174744"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_x = train_x.loc[train_x['censusdate'] > train_end_date].copy()\n",
    "train_x = train_x.loc[train_x['censusdate'] <= valid_end_date].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2017-01-01 00:00:00 2018-10-31 00:00:00\n",
      "valid 2018-06-01 00:00:00 2018-10-31 00:00:00\n"
     ]
    }
   ],
   "source": [
    "print('train', train_x['censusdate'].min(), train_x['censusdate'].max())\n",
    "print('valid', valid_x['censusdate'].min(), valid_x['censusdate'].max())\n",
    "# print('test', test_x['censusdate'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 0.30260465258813835\n",
      "valid 0.2920457106067922\n"
     ]
    }
   ],
   "source": [
    "print('train', train_x['e_29'].isna().mean())\n",
    "print('valid', valid_x['e_29'].isna().mean())\n",
    "# print('test', test_x['e_29'].isna().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_x = test_x.loc[test_x['censusdate']<='2019-06-25', ] # Drop the last day of test data because we don't have notes for that day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = train_x['y'].copy()\n",
    "valid_y = valid_x['y'].copy()\n",
    "train_ids = train_x[train_ids.columns].copy()\n",
    "valid_ids = valid_x[train_ids.columns].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_cols = training_cols + embedding_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x[training_cols]\n",
    "valid_x = valid_x[training_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_na_train(df):\n",
    "    has_na = df.isna().sum() > 0\n",
    "    d = df.loc[:, has_na].median()\n",
    "    df = df.fillna(d)\n",
    "    \n",
    "    return df, d\n",
    "\n",
    "def fill_na_valid(df, na_filler):\n",
    "    return df.fillna(na_filler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in any remaining na's - now that we're not forwardfilling past info it's not correct to use a global imputation\n",
    "# hence we impute on the train and apply to the valid\n",
    "train_x, na_filler = fill_na_train(train_x)\n",
    "valid_x = fill_na_valid(valid_x, na_filler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators':[48],\n",
    "    'feat_select_threshold': ['64*median'],\n",
    "    'max_features': ['auto'], \n",
    "    'min_samples_leaf': [400], \n",
    "    'class_weight': [None],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1019419"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1019419"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "155325"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x.to_pickle('/code/data/processed/avante/03-train_x_with_notes.pickle')\n",
    "valid_x.to_pickle('/code/data/processed/avante/03-valid_x_with_notes.pickle')\n",
    "# train_ids.to_pickle('/code/data/processed/train_idens_with_notes.pickle')\n",
    "# valid_ids.to_pickle('valid_idens_with_notes.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/code/data/processed/train_y_with_notes.pickle', 'wb') as f:\n",
    "    pickle.dump(train_y, f)\n",
    "\n",
    "with open('/code/data/processed/valid_y_with_notes.pickle', 'wb') as f:\n",
    "    pickle.dump(valid_y, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/code/data/processed/train_y_with_notes.pickle', 'rb') as f:\n",
    "#     train_y = pickle.load(f)\n",
    "\n",
    "# with open('/code/data/processed/valid_y_with_notes.pickle', 'rb') as f:\n",
    "#     valid_y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ready\n"
     ]
    }
   ],
   "source": [
    "print('ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment('target_hosp_3_day_with_note_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y[np.isnan(train_y)] = 0\n",
    "valid_y[np.isnan(valid_y)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying hyperparamters: {'class_weight': None, 'feat_select_threshold': '64*median', 'max_features': 'auto', 'min_samples_leaf': 400, 'n_estimators': 48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 48 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 48building tree 2 of 48\n",
      "\n",
      "building tree 3 of 48\n",
      "building tree 4 of 48\n",
      "building tree 5 of 48\n",
      "building tree 6 of 48\n",
      "building tree 7 of 48\n",
      "building tree 8 of 48\n",
      "building tree 9 of 48\n",
      "building tree 10 of 48\n",
      "building tree 11 of 48\n",
      "building tree 12 of 48\n",
      "building tree 13 of 48\n",
      "building tree 14 of 48\n",
      "building tree 15 of 48\n",
      "building tree 16 of 48\n",
      "building tree 17 of 48\n",
      "building tree 18 of 48building tree 19 of 48\n",
      "\n",
      "building tree 20 of 48building tree 21 of 48\n",
      "building tree 22 of 48\n",
      "building tree 23 of 48\n",
      "building tree 24 of 48\n",
      "building tree 25 of 48\n",
      "building tree 26 of 48\n",
      "building tree 27 of 48\n",
      "building tree 28 of 48\n",
      "building tree 29 of 48\n",
      "building tree 30 of 48\n",
      "building tree 31 of 48\n",
      "building tree 32 of 48\n",
      "building tree 33 of 48\n",
      "building tree 34 of 48\n",
      "building tree 35 of 48\n",
      "building tree 36 of 48\n",
      "building tree 37 of 48\n",
      "building tree 38 of 48\n",
      "building tree 39 of 48\n",
      "building tree 40 of 48\n",
      "building tree 41 of 48\n",
      "building tree 42 of 48\n",
      "\n",
      "building tree 43 of 48\n",
      "building tree 44 of 48\n",
      "building tree 45 of 48\n",
      "building tree 46 of 48\n",
      "building tree 47 of 48\n",
      "building tree 48 of 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   4 out of  48 | elapsed:   45.7s remaining:  8.4min\n",
      "[Parallel(n_jobs=-1)]: Done  21 out of  48 | elapsed:   48.3s remaining:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done  38 out of  48 | elapsed:   50.8s remaining:   13.4s\n",
      "[Parallel(n_jobs=-1)]: Done  48 out of  48 | elapsed:   54.3s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 48 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 48\n",
      "building tree 2 of 48\n",
      "building tree 3 of 48\n",
      "building tree 4 of 48\n",
      "building tree 5 of 48\n",
      "building tree 6 of 48\n",
      "building tree 7 of 48\n",
      "building tree 8 of 48\n",
      "building tree 9 of 48\n",
      "building tree 10 of 48\n",
      "building tree 11 of 48\n",
      "building tree 12 of 48\n",
      "building tree 13 of 48\n",
      "building tree 14 of 48\n",
      "building tree 15 of 48\n",
      "building tree 16 of 48\n",
      "building tree 17 of 48\n",
      "building tree 18 of 48\n",
      "building tree 19 of 48\n",
      "building tree 20 of 48\n",
      "building tree 21 of 48\n",
      "building tree 22 of 48\n",
      "building tree 23 of 48\n",
      "building tree 24 of 48\n",
      "building tree 25 of 48\n",
      "building tree 26 of 48\n",
      "building tree 27 of 48\n",
      "building tree 28 of 48\n",
      "building tree 29 of 48\n",
      "building tree 30 of 48\n",
      "building tree 31 of 48\n",
      "building tree 32 of 48\n",
      "building tree 33 of 48\n",
      "building tree 34 of 48\n",
      "building tree 35 of 48\n",
      "building tree 36 of 48\n",
      "building tree 37 of 48\n",
      "building tree 38 of 48\n",
      "building tree 39 of 48\n",
      "building tree 40 of 48building tree 41 of 48\n",
      "building tree 42 of 48building tree 43 of 48\n",
      "\n",
      "\n",
      "building tree 44 of 48\n",
      "building tree 45 of 48\n",
      "building tree 46 of 48\n",
      "building tree 47 of 48\n",
      "building tree 48 of 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   4 out of  48 | elapsed:   45.5s remaining:  8.3min\n",
      "[Parallel(n_jobs=-1)]: Done  21 out of  48 | elapsed:   49.0s remaining:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done  38 out of  48 | elapsed:   50.7s remaining:   13.3s\n",
      "[Parallel(n_jobs=-1)]: Done  48 out of  48 | elapsed:   52.8s finished\n",
      "[Parallel(n_jobs=48)]: Using backend ThreadingBackend with 48 concurrent workers.\n",
      "[Parallel(n_jobs=48)]: Done   4 out of  48 | elapsed:    0.5s remaining:    5.0s\n",
      "[Parallel(n_jobs=48)]: Done  21 out of  48 | elapsed:    0.5s remaining:    0.6s\n",
      "[Parallel(n_jobs=48)]: Done  38 out of  48 | elapsed:    0.5s remaining:    0.1s\n",
      "[Parallel(n_jobs=48)]: Done  48 out of  48 | elapsed:    0.6s finished\n",
      "[Parallel(n_jobs=48)]: Using backend ThreadingBackend with 48 concurrent workers.\n",
      "[Parallel(n_jobs=48)]: Done   4 out of  48 | elapsed:    0.1s remaining:    0.7s\n",
      "[Parallel(n_jobs=48)]: Done  21 out of  48 | elapsed:    0.1s remaining:    0.1s\n",
      "[Parallel(n_jobs=48)]: Done  38 out of  48 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=48)]: Done  48 out of  48 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "for config in ParameterGrid(param_grid):\n",
    "    print(f'Trying hyperparamters: {config}')\n",
    "    \n",
    "    with mlflow.start_run():\n",
    "        feat_est = forest.RandomForestClassifier(\n",
    "            n_estimators=config['n_estimators'],\n",
    "            max_features=config['max_features'],\n",
    "            min_samples_leaf=config['min_samples_leaf'],\n",
    "            class_weight=config['class_weight'],\n",
    "            n_jobs=-1,\n",
    "            verbose=3\n",
    "        )\n",
    "\n",
    "        feat_selector = SelectFromModel(feat_est, threshold=config['feat_select_threshold'])\n",
    "        train_x_new = feat_selector.fit_transform(train_x, train_y)\n",
    "\n",
    "        clf = forest.RandomForestClassifier(\n",
    "            n_estimators=config['n_estimators'],\n",
    "            max_features=config['max_features'],\n",
    "            min_samples_leaf=config['min_samples_leaf'],\n",
    "            class_weight=config['class_weight'],\n",
    "            n_jobs=-1,\n",
    "            verbose=3\n",
    "        )\n",
    "\n",
    "        clf.fit(train_x_new, train_y)\n",
    "\n",
    "        valid_x_new = feat_selector.transform(valid_x)\n",
    "\n",
    "        train_preds = clf.predict_proba(train_x_new)\n",
    "        valid_preds = clf.predict_proba(valid_x_new)\n",
    "\n",
    "        for param in config:\n",
    "            log_param(param, config[param])\n",
    "\n",
    "        log_metric('train_aucroc', roc_auc_score(train_y, [pred[1] for pred in train_preds]))\n",
    "        log_metric('train_ap', average_precision_score(train_y, [pred[1] for pred in train_preds]))\n",
    "        log_metric('valid_aucroc', roc_auc_score(valid_y, [pred[1] for pred in valid_preds]))\n",
    "        log_metric('valid_ap', average_precision_score(valid_y, [pred[1] for pred in valid_preds]))\n",
    "\n",
    "        log_model(feat_selector, 'feat_selector')\n",
    "        log_model(clf, \"model\")\n",
    "\n",
    "        feature_selected_features = pd.DataFrame(zip(train_x.columns[feat_selector.get_support()], clf.feature_importances_), columns=['feature', 'rf_importance']).sort_values('rf_importance', ascending=False)\n",
    "        feature_selected_features.to_csv('./feature_selected_features.csv', index=False)\n",
    "        log_artifact('./feature_selected_features.csv')\n",
    "\n",
    "        input_features = pd.DataFrame(train_x.columns, columns=['feature'])\n",
    "        input_features.to_csv('./input_features.csv', index=False)\n",
    "        log_artifact('./input_features.csv')\n",
    "        \n",
    "        with open('./na_filler.pickle','wb') as f: pickle.dump(na_filler, f, protocol=4)\n",
    "        log_artifact('./na_filler.pickle')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_est = forest.RandomForestClassifier(\n",
    "    n_estimators=1000,\n",
    "    max_features='auto',\n",
    "    min_samples_leaf=200,\n",
    "    class_weight=None,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "feat_selector = SelectFromModel(feat_est, threshold='32*median')\n",
    "train_x_new = feat_selector.fit_transform(train_x, train_y)\n",
    "\n",
    "clf = forest.RandomForestClassifier(\n",
    "    n_estimators=1000,\n",
    "    max_features='auto',\n",
    "    min_samples_leaf=200,\n",
    "    class_weight=None,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "clf.fit(train_x_new, train_y)\n",
    "\n",
    "valid_x_new = feat_selector.transform(valid_x)\n",
    "valid_preds = clf.predict_proba(valid_x_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_precision_score(valid_y, [pred[1] for pred in valid_preds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_x_shap = pd.concat([valid_x.loc[:, feat_selector.get_support()].reset_index(drop=True), valid_idens.reset_index(drop=True)], axis=1)\n",
    "valid_x_shap['preds'] = [pred[1] for pred in valid_preds]\n",
    "valid_x_shap['target'] = valid_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_x_shap = valid_x_shap[valid_x_shap.censusdate == pd.to_datetime('2019-02-20')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_valid = valid_x_shap.sort_values(by='preds', ascending=False)\n",
    "sorted_valid_dr = sorted_valid.drop(columns=['preds', 'masterpatientid', 'censusdate'])\n",
    "idens = sorted_valid.loc[:, ['masterpatientid', 'censusdate', 'preds', 'target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = sorted_valid_dr.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = explainer.shap_values(tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(explainer.expected_value[1], shap_values[1][0], tt.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = []\n",
    "\n",
    "for i in range(0,10):\n",
    "    shaps = pd.DataFrame({'feature_name':tt.columns, 'shap_value': shap_values[1][i], 'feature_value': tt.iloc[i]}).sort_values(by='shap_value', ascending=False)\n",
    "    shaps = shaps.head(n=10)\n",
    "    shaps['masterpatientid'] = hash(str(idens.iloc[i].masterpatientid))\n",
    "    shaps['censusdate'] = idens.iloc[i].censusdate\n",
    "    shaps['prediction'] = idens.iloc[i].preds\n",
    "    shaps['rehosped'] = idens.iloc[i].target\n",
    "    out.append(shaps)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(out).to_csv('/code/data/copd_model_2019-02-20.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'feature_name':tt.columns, 'shap_value': shap_values[1][0], 'feature_value': tt.iloc[0]}).sort_values(by='shap_value', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
