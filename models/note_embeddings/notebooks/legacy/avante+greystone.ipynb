{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "import re\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.engine.url import URL\n",
    "\n",
    "from sklearn.ensemble import forest, gradient_boosting\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.feature_selection import SelectFromModel, SelectKBest\n",
    "\n",
    "import mlflow\n",
    "from mlflow import log_metric, log_param, log_artifact\n",
    "from mlflow.sklearn import log_model\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "#import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_start_date = '2017-01-01'\n",
    "train_end_date = '2019-03-01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path('/code/data/raw')\n",
    "data_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Avante Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secret_name = \"cust_db_credentials\"\n",
    "region_name = \"us-east-1\"\n",
    "\n",
    "# Create a Secrets Manager client\n",
    "session = boto3.session.Session()\n",
    "client = session.client(service_name=\"secretsmanager\", region_name=region_name)\n",
    "\n",
    "get_secret_value_response = client.get_secret_value(SecretId=secret_name)\n",
    "\n",
    "db_info = json.loads(get_secret_value_response[\"SecretString\"])\n",
    "\n",
    "avan_connect_url = URL(\n",
    "    drivername=\"mssql+pyodbc\",\n",
    "    username=db_info[\"username\"],\n",
    "    password=db_info[\"password\"],\n",
    "    host=db_info[\"host\"],\n",
    "    database=\"AVAN\",\n",
    "    query={'driver': 'ODBC Driver 17 for SQL Server'}\n",
    ")\n",
    "'D1'\n",
    "avan_engine = create_engine(avan_connect_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avan_engine.execute('select 1').fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f'''\n",
    "select patientid, facilityid, masterpatientid\n",
    "from view_ods_facility_patient\n",
    "where facilityid in (select facilityid from view_ods_facility where lineofbusiness = 'SNF')\n",
    "'''\n",
    "\n",
    "master_patient_lookup = pd.read_sql(query, con=avan_engine)\n",
    "master_patient_lookup['masterpatientid'] = 'avante_' + master_patient_lookup.masterpatientid.astype(str)\n",
    "master_patient_lookup.to_parquet(data_path/'avante_master_patient_lookup.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_patient_lookup.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f'''\n",
    "select patientid, facilityid, dateoftransfer, transferreason\n",
    "from view_ods_hospital_transfers_transfer_log_v2\n",
    "where dateoftransfer between '{train_start_date}' and '{train_end_date}'\n",
    "and facilityid in (select facilityid from view_ods_facility where lineofbusiness = 'SNF')\n",
    "'''\n",
    "\n",
    "patient_rehosps = pd.read_sql(query, con=avan_engine)\n",
    "patient_rehosps = patient_rehosps.merge(master_patient_lookup, on=['patientid', 'facilityid'])\n",
    "patient_rehosps.to_parquet(data_path/'avante_patient_rehosps.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_rehosps.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f'''\n",
    "select masterpatientid, gender, dateofbirth, primarylanguage\n",
    "from view_ods_master_patient\n",
    "'''\n",
    "\n",
    "patient_demographics = pd.read_sql(query, con=avan_engine)\n",
    "patient_demographics['masterpatientid'] = 'avante_' + patient_demographics.masterpatientid.astype(str)\n",
    "patient_demographics.to_parquet(data_path/'avante_patient_demographics.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_demographics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f'''\n",
    "select clientid as patientid, censusdate, facilityid, bedid\n",
    "from view_ods_daily_census_v2\n",
    "where censusdate between '{train_start_date}' and '{train_end_date}'\n",
    "and facilityid in (select facilityid from view_ods_facility where lineofbusiness = 'SNF')\n",
    "'''\n",
    "\n",
    "patient_census = pd.read_sql(query, con=avan_engine)\n",
    "patient_census = patient_census.merge(master_patient_lookup, on=['patientid','facilityid'])\n",
    "patient_census.to_parquet(data_path/'avante_patient_census.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_census.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f'''\n",
    "select patientid, facilityid, activitydate, principal, admission, other1, other2, other3, other4, other5, other6,\n",
    "other7, other8, other9, other10, other11, other12, other13, other14, other15, other16, other17, other18, other19,\n",
    "other20, other21\n",
    "from view_ods_patient_diagnosis_sheet\n",
    "where activitydate between '{train_start_date}' and '{train_end_date}'\n",
    "and facilityid in (select facilityid from view_ods_facility where lineofbusiness = 'SNF')\n",
    "'''\n",
    "\n",
    "patient_diagnosis = pd.read_sql(query, con=avan_engine)\n",
    "patient_diagnosis = patient_diagnosis.merge(master_patient_lookup, on=['patientid','facilityid'])\n",
    "patient_diagnosis.to_parquet(data_path/'avante_patient_diagnosis.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_diagnosis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f'''\n",
    "select clientid as patientid, facilityid, date, vitalsdescription, value, diastolicvalue\n",
    "from view_ods_Patient_weights_vitals\n",
    "where date between '{train_start_date}' and '{train_end_date}'\n",
    "and facilityid in (select facilityid from view_ods_facility where lineofbusiness = 'SNF')\n",
    "and clientid in (select distinct clientid from view_ods_daily_census_v2 where censusdate between '{train_start_date}' and '{train_end_date}')\n",
    "'''\n",
    "\n",
    "patient_vitals = pd.read_sql(query, con=avan_engine)\n",
    "patient_vitals = patient_vitals.merge(master_patient_lookup, on=['patientid', 'facilityid'])\n",
    "patient_vitals.to_parquet(data_path/'avante_patient_vitals.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_vitals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f'''\n",
    "select distinct patientid, facilityid, orderdate, gpiclassdescription, gpisubclassdescription, orderdescription\n",
    "from view_ods_physician_order_list_v2 a\n",
    "inner join view_ods_physician_order_list_med b\n",
    "on a.PhysicianOrderID = b.PhysiciansOrderID \n",
    "where orderdate between '{train_start_date}' and '{train_end_date}';\n",
    "'''\n",
    "\n",
    "patient_meds = pd.read_sql(query, con=avan_engine)\n",
    "patient_meds = patient_meds.merge(master_patient_lookup, on=['patientid', 'facilityid'])\n",
    "patient_meds.to_parquet(data_path/'avante_patient_meds.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_meds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f'''\n",
    "select distinct patientid, facilityid, orderdate, ordercategory, ordertype, orderdescription, diettype, diettexture, dietsupplement\n",
    "from view_ods_physician_order_list_v2\n",
    "where orderdate between '{train_start_date}' and '{train_end_date}'\n",
    "and ordercategory in ('Diagnostic', 'Enteral - Feed', 'Dietary - Diet', 'Dietary - Supplements')\n",
    "'''\n",
    "\n",
    "patient_orders = pd.read_sql(query, con=avan_engine)\n",
    "patient_orders = patient_orders.merge(master_patient_lookup, on=['patientid','facilityid'])\n",
    "patient_orders.to_parquet(data_path/'avante_patient_orders.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_orders.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f'''\n",
    "select patientid, facilityid, progressnotetype, createddate, sectionsequence, section, notetextorder, notetext\n",
    "from view_ods_progress_note\n",
    "where createddate between '{train_start_date}' and '{train_end_date}'\n",
    "'''\n",
    "\n",
    "patient_progress_notes = pd.read_sql(query, con=avan_engine)\n",
    "patient_progress_notes = patient_progress_notes.merge(master_patient_lookup, on=['patientid','facilityid'])\n",
    "patient_progress_notes.to_parquet(data_path/'avante_patient_progress_notes.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_progress_notes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detailed results seem to only start on 2019-03-03 and onwards - commenting out for now, revisit when we have more data\n",
    "\n",
    "#query = f'''\n",
    "#select c.patientid, a.resultdate, a.profiledescription, a.referencerange, a.result, a.abnormalityid, e.abnormalitydescription, b.reportdesciption, b.severityid, d.severitydescription from view_ods_result_lab_report_detail a\n",
    "#left join view_ods_result_lab_report b on a.LabReportID = b.LabReportID\n",
    "#left join view_ods_result_order_source c on b.ResultOrderSourceID = c.ResultOrderSourceID\n",
    "#left join view_ods_result_lab_report_severity d on b.SeverityID = d.SeverityID\n",
    "#left join view_ods_result_lab_test_abnormality e on a.AbnormalityID = e.AbnormalityID\n",
    "#'''\n",
    "\n",
    "#patient_lab_results = pd.read_sql(query, con=avan_engine)\n",
    "#patient_lab_results.to_parquet(data_path/'patient_detailed_lab_results.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#query = f'''\n",
    "#select patientid, facilityid, createddate, stdalertid, alertdescription, a.triggereditemtype, description\n",
    "#from [view_ods_cr_alert] a left join view_ods_cr_alert_triggered_item_type b\n",
    "#on a.triggereditemtype = b.triggereditemtype\n",
    "#where createddate between '{train_start_date}' and '{train_end_date}' and \n",
    "#((triggereditemid is not null) or (a.triggereditemtype is not null))\n",
    "#'''\n",
    "#\n",
    "#patient_alerts = pd.read_sql(query, con=avan_engine)\n",
    "#patient_alerts = patient_alerts.merge(master_patient_lookup, on=['patientid','facilityid'])\n",
    "#patient_alerts.to_parquet(data_path/'patient_alerts.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Greystone Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grey_connect_url = URL(\n",
    "    drivername=\"mssql+pyodbc\",\n",
    "    username='saiva',\n",
    "    password='Saiva27360!',\n",
    "    host='172.0.95.223',\n",
    "    database=\"pcc_replica_saiva\",\n",
    "    port=1433,\n",
    "    query={'driver': 'ODBC Driver 17 for SQL Server'}\n",
    ")\n",
    "\n",
    "grey_engine = create_engine(grey_connect_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grey_engine.execute('select 1').fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f'''\n",
    "select patientid, facilityid, masterpatientid\n",
    "from dbo.ods_facility_patient\n",
    "where facilityid in (select facilityid from dbo.ods_facility where lineofbusiness = 'SNF')\n",
    "'''\n",
    "\n",
    "master_patient_lookup = pd.read_sql(query, con=grey_engine)\n",
    "master_patient_lookup['masterpatientid'] = 'greystone_' + master_patient_lookup.masterpatientid.astype(str)\n",
    "master_patient_lookup.to_parquet(data_path/'grey_master_patient_lookup.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_patient_lookup.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f'''\n",
    "select patientid, facilityid, dateoftransfer, transferreason\n",
    "from dbo.ods_hospital_transfers_transfer_log_v2\n",
    "where dateoftransfer between '{train_start_date}' and '{train_end_date}'\n",
    "and facilityid in (select facilityid from dbo.ods_facility where lineofbusiness = 'SNF')\n",
    "'''\n",
    "\n",
    "patient_rehosps = pd.read_sql(query, con=grey_engine)\n",
    "patient_rehosps = patient_rehosps.merge(master_patient_lookup, on=['patientid', 'facilityid'])\n",
    "patient_rehosps.to_parquet(data_path/'grey_patient_rehosps.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_rehosps.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f'''\n",
    "select masterpatientid, gender, dateofbirth, primarylanguage\n",
    "from dbo.ods_master_patient\n",
    "'''\n",
    "\n",
    "patient_demographics = pd.read_sql(query, con=grey_engine)\n",
    "patient_demographics['masterpatientid'] = 'greystone_' + patient_demographics.masterpatientid.astype(str)\n",
    "patient_demographics.to_parquet(data_path/'grey_patient_demographics.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_demographics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f'''\n",
    "select patientid, censusdate, facilityid, bedid\n",
    "from dbo.ods_daily_census_lite\n",
    "where censusdate between '{train_start_date}' and '{train_end_date}'\n",
    "and facilityid in (select facilityid from dbo.ods_facility where lineofbusiness = 'SNF')\n",
    "'''\n",
    "\n",
    "patient_census = pd.read_sql(query, con=grey_engine)\n",
    "patient_census = patient_census.merge(master_patient_lookup, on=['patientid','facilityid'])\n",
    "patient_census.to_parquet(data_path/'grey_patient_census.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_census.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f'''\n",
    "select patientid, facilityid, activitydate, principal, admission, other1, other2, other3, other4, other5, other6,\n",
    "other7, other8, other9, other10, other11, other12, other13, other14, other15, other16, other17, other18, other19,\n",
    "other20, other21\n",
    "from dbo.ods_patient_diagnosis_sheet\n",
    "where activitydate between '{train_start_date}' and '{train_end_date}'\n",
    "and facilityid in (select facilityid from dbo.ods_facility where lineofbusiness = 'SNF')\n",
    "'''\n",
    "\n",
    "patient_diagnosis = pd.read_sql(query, con=grey_engine)\n",
    "patient_diagnosis = patient_diagnosis.merge(master_patient_lookup, on=['patientid','facilityid'])\n",
    "patient_diagnosis.to_parquet(data_path/'grey_patient_diagnosis.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_diagnosis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f'''\n",
    "select clientid as patientid, facilityid, date, vitalsdescription, value, diastolicvalue\n",
    "from dbo.ods_Patient_weights_vitals\n",
    "where date between '{train_start_date}' and '{train_end_date}'\n",
    "and facilityid in (select facilityid from dbo.ods_facility where lineofbusiness = 'SNF')\n",
    "and clientid in (select distinct patientid from dbo.ods_daily_census_lite where censusdate between '{train_start_date}' and '{train_end_date}')\n",
    "'''\n",
    "\n",
    "patient_vitals = pd.read_sql(query, con=grey_engine)\n",
    "patient_vitals = patient_vitals.merge(master_patient_lookup, on=['patientid', 'facilityid'])\n",
    "patient_vitals.to_parquet(data_path/'grey_patient_vitals.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_vitals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f'''\n",
    "select distinct patientid, facilityid, orderdate, gpiclassdescription, gpisubclassdescription, orderdescription\n",
    "from dbo.ods_physician_order_list_v2 a\n",
    "inner join dbo.ods_physician_order_list_med b\n",
    "on a.PhysicianOrderID = b.PhysiciansOrderID \n",
    "where orderdate between '{train_start_date}' and '{train_end_date}';\n",
    "'''\n",
    "\n",
    "patient_meds = pd.read_sql(query, con=grey_engine)\n",
    "patient_meds = patient_meds.merge(master_patient_lookup, on=['patientid', 'facilityid'])\n",
    "patient_meds.to_parquet(data_path/'grey_patient_meds.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_meds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f'''\n",
    "select distinct patientid, facilityid, orderdate, ordercategory, ordertype, orderdescription, diettype, diettexture, dietsupplement\n",
    "from dbo.ods_physician_order_list_v2\n",
    "where orderdate between '{train_start_date}' and '{train_end_date}'\n",
    "and ordercategory in ('Diagnostic', 'Enteral - Feed', 'Dietary - Diet', 'Dietary - Supplements')\n",
    "'''\n",
    "\n",
    "patient_orders = pd.read_sql(query, con=grey_engine)\n",
    "patient_orders = patient_orders.merge(master_patient_lookup, on=['patientid','facilityid'])\n",
    "patient_orders.to_parquet(data_path/'grey_patient_orders.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_orders.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f'''\n",
    "select patientid, facilityid, progressnotetype, createddate, sectionsequence, section, notetextorder, notetext\n",
    "from dbo.ods_progress_note\n",
    "where createddate between '{train_start_date}' and '{train_end_date}'\n",
    "'''\n",
    "\n",
    "patient_progress_notes = pd.read_sql(query, con=grey_engine)\n",
    "patient_progress_notes = patient_progress_notes.merge(master_patient_lookup, on=['patientid','facilityid'])\n",
    "patient_progress_notes.to_parquet(data_path/'grey_patient_progress_notes.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_progress_notes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_demographics = pd.concat(\n",
    "    [pd.read_parquet(data_path/'avante_patient_demographics.parquet'), \n",
    "     pd.read_parquet(data_path/'grey_patient_demographics.parquet')], axis=0\n",
    ")\n",
    "patient_orders = pd.concat(\n",
    "    [pd.read_parquet(data_path/'avante_patient_orders.parquet'), \n",
    "     pd.read_parquet(data_path/'grey_patient_orders.parquet')], axis=0\n",
    ")\n",
    "patient_vitals = pd.concat(\n",
    "    [pd.read_parquet(data_path/'avante_patient_vitals.parquet'), \n",
    "     pd.read_parquet(data_path/'grey_patient_vitals.parquet')], axis=0\n",
    ")\n",
    "patient_rehosps = pd.concat(\n",
    "    [pd.read_parquet(data_path/'avante_patient_rehosps.parquet'), \n",
    "     pd.read_parquet(data_path/'grey_patient_rehosps.parquet')], axis=0\n",
    ")\n",
    "patient_census = pd.concat(\n",
    "    [pd.read_parquet(data_path/'avante_patient_census.parquet'), \n",
    "     pd.read_parquet(data_path/'grey_patient_census.parquet')], axis=0\n",
    ")\n",
    "patient_diagnosis = pd.concat(\n",
    "    [pd.read_parquet(data_path/'avante_patient_diagnosis.parquet'), \n",
    "     pd.read_parquet(data_path/'grey_patient_diagnosis.parquet')], axis=0\n",
    ")\n",
    "patient_meds = pd.concat(\n",
    "    [pd.read_parquet(data_path/'avante_patient_meds.parquet'), \n",
    "     pd.read_parquet(data_path/'grey_patient_meds.parquet')], axis=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorter(df, sort_keys=[]):\n",
    "    return df.sort_values(by=sort_keys)\n",
    "\n",
    "def deduper(df, unique_keys=[]):\n",
    "    df = df.drop_duplicates(subset=unique_keys, keep='last')\n",
    "    assert df.duplicated(subset=unique_keys).sum() == 0, f'''Still have dupes!'''\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_census = sorter(patient_census, sort_keys=['masterpatientid', 'censusdate'])\n",
    "patient_vitals = sorter(patient_vitals, sort_keys=['masterpatientid', 'date'])\n",
    "patient_orders = sorter(patient_orders, sort_keys=['masterpatientid', 'orderdate'])\n",
    "patient_rehosps = sorter(patient_rehosps, sort_keys=['masterpatientid', 'dateoftransfer'])\n",
    "patient_meds = sorter(patient_meds, sort_keys=['masterpatientid', 'orderdate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_census = deduper(patient_census, unique_keys=['masterpatientid', 'censusdate'])\n",
    "patient_demographics = deduper(patient_demographics, unique_keys=['masterpatientid'])\n",
    "patient_vitals = deduper(patient_vitals, unique_keys=['masterpatientid', 'date', 'vitalsdescription'])\n",
    "patient_orders = deduper(patient_orders, unique_keys=['masterpatientid', 'orderdate', 'orderdescription'])\n",
    "patient_rehosps = deduper(patient_rehosps, unique_keys=['masterpatientid', 'dateoftransfer'])\n",
    "patient_meds = deduper(patient_meds, unique_keys=['masterpatientid', 'orderdate', 'gpisubclassdescription'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = pd.DataFrame({'censusdate': pd.date_range(start=train_start_date, end=train_end_date)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_rehosps['dateoftransfer'] = pd.to_datetime(patient_rehosps.dateoftransfer.dt.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base2 = base.merge(patient_census, how='left', on=['censusdate'])\n",
    "base3 = base2.merge(patient_demographics, how='left', on=['masterpatientid'])\n",
    "\n",
    "del base2;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vitals = patient_vitals.set_index(keys=['masterpatientid','facilityid','date']).drop(columns='patientid')\n",
    "\n",
    "diastolic = vitals.pop('diastolicvalue')\n",
    "diastolic = diastolic.dropna()\n",
    "\n",
    "vitals = vitals.reset_index()\n",
    "diastolic = diastolic.reset_index()\n",
    "\n",
    "vitals['date'] = vitals.pop('date').dt.date\n",
    "diastolic['date'] = diastolic.pop('date').dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vitals.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggs=['median','std', 'max', 'min']\n",
    "vitals_pivoted = vitals.loc[:,['masterpatientid', 'date', 'vitalsdescription', 'value']].pivot_table(index=['masterpatientid', 'date'], values='value', columns='vitalsdescription', aggfunc=aggs).reset_index()\n",
    "diastolic_pivoted = diastolic.loc[:,['masterpatientid', 'date', 'diastolicvalue']].pivot_table(index=['masterpatientid', 'date'], values='diastolicvalue', aggfunc=aggs).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_multi_columns(cols):\n",
    "    new_cols = []\n",
    "    \n",
    "    for col in cols:\n",
    "        if col[1] == '':\n",
    "            new_cols.append(col[0])\n",
    "        else:\n",
    "            new_cols.append('_'.join(col))\n",
    "            \n",
    "    return new_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vitals_pivoted.columns = clean_multi_columns(vitals_pivoted.columns)\n",
    "diastolic_pivoted.columns = clean_multi_columns(diastolic_pivoted.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vitals_pivoted = vitals_pivoted.drop_duplicates(subset=['masterpatientid','date'], keep='last')\n",
    "diastolic_pivoted = diastolic_pivoted.drop_duplicates(subset=['masterpatientid','date'], keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vitals_final = vitals_pivoted.merge(diastolic_pivoted, how='left', on=['masterpatientid', 'date'])\n",
    "vitals_final.columns = 'vtl_' + vitals_final.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vitals_final.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base4 = base3.merge(vitals_final, how='left', left_on=['masterpatientid','censusdate'], right_on=['vtl_masterpatientid','vtl_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base4.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del vitals; del vitals_pivoted; del diastolic_pivoted; del vitals_final; del base3;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_ccs = pd.read_csv('/code/data/lookup_tables/ccs_dx_icd10cm_2019_1.csv')\n",
    "lookup_ccs.columns = lookup_ccs.columns.str.replace(\"'\",\"\")\n",
    "lookup_ccs = lookup_ccs.apply(lambda x: x.str.replace(\"'\",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnosis_long = patient_diagnosis.melt(id_vars=['masterpatientid','activitydate'], value_vars=['principal', 'admission', 'other1', 'other2', 'other3', 'other4',\n",
    "       'other5', 'other6', 'other7', 'other8', 'other9', 'other10', 'other11',\n",
    "       'other12', 'other13', 'other14', 'other15', 'other16', 'other17',\n",
    "       'other18', 'other19', 'other20', 'other21'])\n",
    "\n",
    "diagnosis_long['value'] = diagnosis_long.value.str.replace('.','')\n",
    "diagnosis_long['activitydate'] = pd.to_datetime(diagnosis_long.activitydate).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnosis_with_ccs = diagnosis_long.merge(lookup_ccs, how='inner', left_on=['value'], right_on=['ICD-10-CM CODE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnosis_with_ccs['ccs_label'] = diagnosis_with_ccs['MULTI CCS LVL 1 LABEL'] + ' - ' + diagnosis_with_ccs['MULTI CCS LVL 2 LABEL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnosis_with_ccs['indicator'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnosis_pivoted = diagnosis_with_ccs.loc[:,['masterpatientid', 'activitydate', 'ccs_label', 'indicator']].pivot_table(index=['masterpatientid', 'activitydate'], columns=['ccs_label'], values='indicator', fill_value=0).reset_index()\n",
    "diagnosis_pivoted['activitydate'] = pd.to_datetime(diagnosis_pivoted.activitydate)\n",
    "diagnosis_pivoted.columns = 'dx_' + diagnosis_pivoted.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base5 = base4.merge(diagnosis_pivoted, how='left', left_on=['masterpatientid','censusdate'], right_on=['dx_masterpatientid','dx_activitydate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del base4; del diagnosis_pivoted;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_meds.loc[patient_meds.gpisubclassdescription.isna(), 'gpisubclassdescription'] = patient_meds.loc[patient_meds.gpisubclassdescription.isna(), 'gpiclassdescription']\n",
    "patient_meds['orderdate'] = patient_meds.orderdate.dt.date\n",
    "patient_meds['indicator'] = 1\n",
    "meds_pivoted = patient_meds.loc[:,['masterpatientid', 'orderdate', 'gpisubclassdescription', 'indicator']].pivot_table(index=['masterpatientid', 'orderdate'], columns=['gpisubclassdescription'], values='indicator', fill_value=0).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meds_pivoted.columns = 'med_' + meds_pivoted.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meds_pivoted = meds_pivoted.drop_duplicates(subset=['med_masterpatientid','med_orderdate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meds_pivoted['med_orderdate'] = pd.to_datetime(meds_pivoted.med_orderdate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base6 = base5.merge(meds_pivoted, how='left', left_on=['masterpatientid', 'censusdate'], right_on=['med_masterpatientid', 'med_orderdate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del base5; del meds_pivoted;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostic_orders = patient_orders.loc[patient_orders.ordercategory == 'Diagnostic']\n",
    "diagnostic_orders['orderdate'] = diagnostic_orders.orderdate.dt.date\n",
    "diagnostic_orders['count_indicator_diagnostic_orders'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostic_pivoted = diagnostic_orders.loc[:,['masterpatientid', 'orderdate', 'count_indicator_diagnostic_orders']].pivot_table(index=['masterpatientid', 'orderdate'], values=['count_indicator_diagnostic_orders'], aggfunc=sum).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostic_pivoted['orderdate'] = pd.to_datetime(diagnostic_pivoted.orderdate)\n",
    "diagnostic_pivoted.columns = 'order_' + diagnostic_pivoted.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base7 = base6.merge(diagnostic_pivoted, how='left', left_on=['masterpatientid','censusdate'], right_on=['order_masterpatientid','order_orderdate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del base6; del diagnostic_pivoted; del diagnostic_orders;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diet_orders = patient_orders[patient_orders.ordercategory == 'Dietary - Diet']\n",
    "diet_orders['orderdate'] = diet_orders.orderdate.dt.date\n",
    "diet_orders['indicator'] = 1\n",
    "diet_orders = diet_orders.drop_duplicates(subset=['masterpatientid', 'orderdate', 'diettype', 'diettexture'])\n",
    "\n",
    "diet_type_pivoted = diet_orders.loc[:,['masterpatientid', 'orderdate', 'diettype', 'indicator']].pivot_table(index=['masterpatientid', 'orderdate'], columns=['diettype'], values='indicator', aggfunc=min).reset_index()\n",
    "#diet_type_pivoted.columns = clean_multi_columns(diet_type_pivoted)\n",
    "diet_type_pivoted.head()\n",
    "diet_type_pivoted['orderdate'] = pd.to_datetime(diet_type_pivoted.orderdate)\n",
    "diet_type_pivoted.columns = 'order_' + diet_type_pivoted.columns\n",
    "\n",
    "diet_texture_pivoted = diet_orders.loc[:,['masterpatientid', 'orderdate', 'diettexture', 'indicator']].pivot_table(index=['masterpatientid', 'orderdate'], columns=['diettexture'], values='indicator', aggfunc=min).reset_index()\n",
    "#diet_texture_pivoted.columns = clean_multi_columns(diet_texture_pivoted)\n",
    "diet_texture_pivoted['orderdate'] = pd.to_datetime(diet_texture_pivoted.orderdate)\n",
    "diet_texture_pivoted.columns = 'order_' + diet_texture_pivoted.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base8 = base7.merge(diet_type_pivoted, how='left', left_on=['masterpatientid','censusdate'], right_on=['order_masterpatientid','order_orderdate'])\n",
    "base8 = base7.merge(diet_texture_pivoted, how='left', left_on=['masterpatientid','censusdate'], right_on=['order_masterpatientid','order_orderdate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del base7;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diet_supplements = patient_orders[patient_orders.ordercategory == 'Dietary - Supplements']\n",
    "diet_supplements['orderdate'] = diet_supplements.orderdate.dt.date\n",
    "diet_supplements['indicator'] = 1\n",
    "diet_supplements = diet_supplements.drop_duplicates(subset=['masterpatientid', 'orderdate', 'dietsupplement'])\n",
    "                                                    \n",
    "diet_supplements_pivoted = diet_supplements.loc[:,['masterpatientid', 'orderdate', 'dietsupplement', 'indicator']].pivot_table(index=['masterpatientid', 'orderdate'], columns='dietsupplement', values='indicator', aggfunc=min).reset_index()\n",
    "diet_supplements_pivoted['orderdate'] = pd.to_datetime(diet_supplements_pivoted.orderdate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diet_supplements_counts = diet_supplements.groupby(['masterpatientid', 'facilityid', 'orderdate']).dietsupplement.count().reset_index().rename(columns={'dietsupplement':'count_indicator_dietsupplement'})\n",
    "diet_supplements_counts['orderdate'] = pd.to_datetime(diet_supplements_counts.orderdate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diet_supplements_pivoted.columns = 'order_' + diet_supplements_pivoted.columns\n",
    "diet_supplements_counts.columns = 'order_' + diet_supplements_counts.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base9 = base8.merge(diet_supplements_pivoted, how='left', left_on=['masterpatientid','censusdate'], right_on=['order_masterpatientid','order_orderdate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base9 = base9.merge(diet_supplements_counts, how='left', left_on=['masterpatientid','censusdate'], right_on=['order_masterpatientid','order_orderdate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del base8;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rehosp = patient_rehosps.merge(patient_census, on=['masterpatientid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_hosp = rehosp[rehosp.dateoftransfer < rehosp.censusdate]\n",
    "last_hosp['count_prior_hosp'] = last_hosp.groupby(['masterpatientid', 'censusdate']).dateoftransfer.cumcount() + 1\n",
    "last_hosp = last_hosp.groupby(['masterpatientid','censusdate']).tail(n=1).loc[:,['masterpatientid', 'censusdate', 'dateoftransfer', 'count_prior_hosp']].rename(columns={'dateoftransfer': 'last_hosp_date'})\n",
    "last_hosp['days_since_last_hosp'] = (last_hosp.censusdate - last_hosp.last_hosp_date).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_hosp = rehosp[rehosp.dateoftransfer > rehosp.censusdate].groupby(['masterpatientid','censusdate']).head(n=1).loc[:,['masterpatientid', 'censusdate', 'dateoftransfer']].rename(columns={'dateoftransfer': 'next_hosp_date'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_hosp['target_3_day_hosp'] = (next_hosp.next_hosp_date - next_hosp.censusdate) <= pd.to_timedelta('4 days')\n",
    "next_hosp['target_7_day_hosp'] = (next_hosp.next_hosp_date - next_hosp.censusdate) <= pd.to_timedelta('8 days')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_hosp.columns = 'hosp_' + last_hosp.columns\n",
    "next_hosp.columns = 'hosp_' + next_hosp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base10 = base9.merge(last_hosp, how='left', left_on=['masterpatientid','censusdate'], right_on=['hosp_masterpatientid', 'hosp_censusdate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base10 = base10.merge(next_hosp, how='left', left_on=['masterpatientid','censusdate'], right_on=['hosp_masterpatientid', 'hosp_censusdate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base10 = base10.loc[:,~base10.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_path = Path('/code/data/processed')\n",
    "processed_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base10 = base10.loc[:, base10.columns[~base10.columns.str.contains('_masterpatientid|_facilityid|vtl_date|activitydate|orderdate|createddate|_x$|_y$')].tolist()]\n",
    "base10 = base10.drop_duplicates(subset=['masterpatientid', 'censusdate'], keep='last')\n",
    "base10 = base10.loc[base10.masterpatientid.notna()]\n",
    "base10.to_parquet(processed_path/'combined.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = pd.read_parquet(processed_path/'combined.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = combined.drop(columns=['hosp_last_hosp_date', 'hosp_next_hosp_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vtl_cols = [col for col in combined.columns if col.startswith('vtl')]\n",
    "dx_cols = [col for col in combined.columns if col.startswith('dx')]\n",
    "med_cols = [col for col in combined.columns if col.startswith('med')]\n",
    "order_cols = [col for col in combined.columns if col.startswith('order')]\n",
    "alert_cols = [col for col in combined.columns if col.startswith('alert')]\n",
    "hosp_cols = [col for col in combined.columns if col.startswith('hosp')]\n",
    "ignore_cols = [col for col in combined.columns if 'target' in col] + ['masterpatientid', 'patientid','censusdate', 'facilityid', 'bedid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_datepart(df, fldname, drop=True, time=False, errors=\"raise\"):\n",
    "    fld = df[fldname]\n",
    "    fld_dtype = fld.dtype\n",
    "    if isinstance(fld_dtype, pd.core.dtypes.dtypes.DatetimeTZDtype):\n",
    "        fld_dtype = np.datetime64\n",
    "\n",
    "    if not np.issubdtype(fld_dtype, np.datetime64):\n",
    "        df[fldname] = fld = pd.to_datetime(fld, infer_datetime_format=True, errors=errors)\n",
    "    attr = ['Year','Month', 'Week', 'Day', 'Dayofweek',\n",
    "            'Is_month_end', 'Is_month_start', 'Is_quarter_end', 'Is_quarter_start', 'Is_year_end', 'Is_year_start']\n",
    "    if time: attr = attr + ['Hour', 'Minute', 'Second']\n",
    "    for n in attr: df[fldname + '_' + n] = getattr(fld.dt, n.lower())\n",
    "    if drop: df.drop(fldname, axis=1, inplace=True)\n",
    "        \n",
    "def add_na_indicators(df, ignore_cols):\n",
    "    missings = df.drop(columns=ignore_cols).isna()\n",
    "    missings.columns = 'na_indictator_' + missings.columns\n",
    "    missings_sums = missings.sum()\n",
    "    \n",
    "    return pd.concat([df, missings.loc[:, (missings_sums > 0)]], axis=1)\n",
    "\n",
    "        \n",
    "def proc_vitals(df, vtl_cols):\n",
    "    ffilled = df.groupby('masterpatientid')[vtl_cols].fillna(method='ffill').reset_index()\n",
    "    ffilled['masterpatientid'] = df.masterpatientid\n",
    "    \n",
    "    diff_1_day = ffilled.groupby('masterpatientid')[vtl_cols].diff()\n",
    "    diff_1_day.columns = 'diff_1_day_' + diff_1_day.columns\n",
    "    \n",
    "    diff_7_day = ffilled.groupby('masterpatientid')[vtl_cols].diff(periods=7)\n",
    "    diff_7_day.columns = 'diff_7_day_' + diff_7_day.columns\n",
    "    \n",
    "    rolling_avg_7_day = ffilled.groupby('masterpatientid')[vtl_cols].rolling(7, min_periods=1).mean().reset_index(0, drop=True)\n",
    "    rolling_avg_7_day.columns = 'rol_avg_7_day_' + rolling_avg_7_day.columns\n",
    "    \n",
    "    rolling_avg_14_day = ffilled.groupby('masterpatientid')[vtl_cols].rolling(14, min_periods=1).mean().reset_index(0, drop=True)\n",
    "    rolling_avg_14_day.columns = 'rol_avg_14_day_' + rolling_avg_14_day.columns\n",
    "    \n",
    "    rolling_std_7_day = ffilled.groupby('masterpatientid')[vtl_cols].rolling(7, min_periods=1).std().reset_index(0, drop=True)\n",
    "    rolling_std_7_day.columns = 'rol_std_7_day_' + rolling_std_7_day.columns\n",
    "    \n",
    "    rolling_std_14_day = ffilled.groupby('masterpatientid')[vtl_cols].rolling(14, min_periods=1).std().reset_index(0, drop=True)\n",
    "    rolling_std_14_day.columns = 'rol_std_14_day_' + rolling_std_14_day.columns\n",
    "    \n",
    "    df.loc[:,vtl_cols] = ffilled.loc[:, vtl_cols]\n",
    "    \n",
    "    df = pd.concat([df, diff_1_day, diff_7_day], axis=1) # diffs all indexed the same as original in the same order\n",
    "    \n",
    "    rollings = pd.concat([rolling_avg_7_day, rolling_avg_14_day, rolling_std_7_day, rolling_std_14_day], axis=1)\n",
    "    df = df.merge(rollings, how='left', left_index=True, right_index=True) # rollings were sorted so we explictly join via index\n",
    "    \n",
    "    return df\n",
    "    \n",
    "def proc_dx_meds_alerts_orders(df, dx_cols, med_cols, order_cols):\n",
    "    cols = dx_cols + med_cols + order_cols\n",
    "    filled = df.groupby('masterpatientid')[cols].fillna(0).reset_index()\n",
    "    filled['masterpatientid'] = df.masterpatientid\n",
    "    \n",
    "    cumsum_all_time = filled.groupby('masterpatientid')[cols].cumsum()\n",
    "    cumsum_all_time.columns = 'cumsum_all_' + cumsum_all_time.columns\n",
    "    \n",
    "    cumsum_7_day = filled.groupby('masterpatientid')[cols].rolling(7, min_periods=1).sum().reset_index(0, drop=True)\n",
    "    cumsum_7_day.columns = 'cumsum_7_day_' + cumsum_7_day.columns\n",
    "    \n",
    "    cumsum_15_day = filled.groupby('masterpatientid')[cols].rolling(15, min_periods=1).sum().reset_index(0, drop=True)\n",
    "    cumsum_15_day.columns = 'cumsum_15_day_' + cumsum_15_day.columns\n",
    "    \n",
    "    #cumsum_30_day = filled.groupby('masterpatientid')[cols].rolling(30, min_periods=1).sum().reset_index(0, drop=True)\n",
    "    #cumsum_30_day.columns = 'cumsum_30_day_' + cumsum_30_day.columns\n",
    "    \n",
    "    df = df.drop(columns=cols)\n",
    "    df = pd.concat([df, cumsum_all_time], axis=1) # cumsum is indexed the same as original in the same order\n",
    "    \n",
    "    rollings = pd.concat([cumsum_7_day, cumsum_15_day], axis=1)\n",
    "    df = df.merge(rollings, how='left', left_index=True, right_index=True) # rollings were sorted so we explictly join via index\n",
    "    \n",
    "    return df\n",
    "\n",
    "def proc_demo(df):\n",
    "    df['demo_gender'] = df.gender == 'M'\n",
    "    df['demo_age_in_days'] = (df.censusdate - df.dateofbirth).dt.days\n",
    "    df = pd.concat([df.drop(columns='primarylanguage'), pd.get_dummies(df.primarylanguage, prefix='demo_primarylanguage')], axis=1)\n",
    "    df = pd.concat([df.drop(columns='facilityid'), pd.get_dummies(df.facilityid, prefix='demo_facility')], axis=1)\n",
    "    \n",
    "    return df\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = add_na_indicators(combined, ignore_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_datepart(combined, 'censusdate', drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_datepart(combined, 'dateofbirth', drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = proc_vitals(combined, vtl_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = proc_dx_meds_alerts_orders(combined, dx_cols, med_cols, order_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.to_parquet(processed_path/'after_vtl_and_cumsums.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = pd.read_parquet(processed_path/'after_vtl_and_cumsums.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = proc_demo(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = combined.drop_duplicates(subset=['masterpatientid', 'censusdate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.to_parquet(processed_path/'final_processed.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_path = Path('/code/data/processed')\n",
    "final = pd.read_parquet(processed_path/'final_processed.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['bedid', 'beddescription', 'roomratetypedescription', 'payercode', 'patientid', \n",
    "             'gender', 'dateofbirth', 'citizenship', 'state', 'hosp_target_7_day_hosp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = final.drop(columns=drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = final.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual check to make sure we're not including any columns that could leak data\n",
    "with open('/code/columns.txt','w') as f:\n",
    "    for col in final.columns:\n",
    "        f.write(col + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_day = final.loc[:,'censusdate'].iloc[round(final.shape[0] * (1-.2))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = final.loc[final.censusdate <= split_day]\n",
    "valid = final.loc[final.censusdate > split_day]\n",
    "\n",
    "train.to_pickle(processed_path/'train.pickle')\n",
    "valid.to_pickle(processed_path/'valid.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_path = Path('/code/data/processed')\n",
    "train = pd.read_pickle(processed_path/'train.pickle')\n",
    "valid = pd.read_pickle(processed_path/'valid.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask = train['cumsum_all_dx_Diseases of the respiratory system - Chronic obstructive pulmonary disease and bronchiectasis [127.]'] > 0\n",
    "valid_mask = valid['cumsum_all_dx_Diseases of the respiratory system - Chronic obstructive pulmonary disease and bronchiectasis [127.]'] > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{len(train[train_mask].masterpatientid.unique())} patients out of {len(train.masterpatientid.unique())} have COPD associated with them')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_patients = train[train_mask].masterpatientid.unique()\n",
    "valid_patients = valid[valid_mask].masterpatientid.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[train.masterpatientid.isin(train_patients)]\n",
    "valid = valid[valid.masterpatientid.isin(valid_patients)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_na_train(df):\n",
    "    has_na = df.isna().sum() > 0\n",
    "    d = df.loc[:, has_na].median()\n",
    "    df = df.fillna(d)\n",
    "    \n",
    "    return df, d\n",
    "\n",
    "def fill_na_valid(df, na_filler):\n",
    "    return df.fillna(na_filler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in any remaining na's - now that we're not forwardfilling past info it's not correct to use a global imputation\n",
    "# hence we impute on the train and apply to the valid\n",
    "train, na_filler = fill_na_train(train)\n",
    "valid = fill_na_valid(valid, na_filler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_pickle(processed_path/'copd_train_filled.pickle')\n",
    "valid.to_pickle(processed_path/'copd_valid_filled.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_path = Path('/code/data/processed')\n",
    "train = pd.read_pickle(processed_path/'copd_train_filled.pickle')\n",
    "valid = pd.read_pickle(processed_path/'copd_valid_filled.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.censusdate.min(), train.censusdate.max(), train.hosp_target_3_day_hosp.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid.censusdate.min(), valid.censusdate.max(), valid.hosp_target_3_day_hosp.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep(df):\n",
    "    drop_cols = ['censusdate', 'masterpatientid']\n",
    "    drop_cols = drop_cols + [col for col in df.columns if 'target' in col]\n",
    "    \n",
    "    y = df.hosp_target_3_day_hosp.astype('float32').values\n",
    "    x = df.drop(columns=drop_cols).reset_index(drop=True).astype('float32')\n",
    "    idens = df.loc[:,['masterpatientid','censusdate']]\n",
    "    \n",
    "    return x, y, idens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y, train_idens = prep(train)\n",
    "valid_x, valid_y, valid_idens = prep(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle;\n",
    "with open('/code/data/processed/copd_train_x.pickle','wb') as f: pickle.dump(train_x, f, protocol=4)\n",
    "with open('/code/data/processed/copd_train_y.pickle','wb') as f: pickle.dump(train_y, f, protocol=4)\n",
    "with open('/code/data/processed/copd_train_idens.pickle','wb') as f: pickle.dump(train_idens, f, protocol=4)\n",
    "with open('/code/data/processed/copd_valid_x.pickle','wb') as f: pickle.dump(valid_x, f, protocol=4)\n",
    "with open('/code/data/processed/copd_valid_y.pickle','wb') as f: pickle.dump(valid_y, f, protocol=4)\n",
    "with open('/code/data/processed/copd_valid_idens.pickle','wb') as f: pickle.dump(valid_idens, f, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle;\n",
    "with open('/code/data/processed/copd_train_x.pickle','rb') as f: train_x = pickle.load(f)\n",
    "with open('/code/data/processed/copd_train_y.pickle','rb') as f: train_y = pickle.load(f)\n",
    "with open('/code/data/processed/copd_train_idens.pickle','rb') as f: train_idens = pickle.load(f)\n",
    "with open('/code/data/processed/copd_valid_x.pickle','rb') as f: valid_x = pickle.load(f)\n",
    "with open('/code/data/processed/copd_valid_y.pickle','rb') as f: valid_y =pickle.load(f)\n",
    "with open('/code/data/processed/copd_valid_idens.pickle','rb') as f: valid_idens =pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators':[500],\n",
    "    'feat_select_threshold': ['16*median','32*median'],\n",
    "    'max_features': ['auto'], \n",
    "    'min_samples_leaf': [100, 150], \n",
    "    'class_weight': [None],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment('copd_target_hosp_3_day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for config in ParameterGrid(param_grid):\n",
    "    print(f'Trying hyperparamters: {config}')\n",
    "    \n",
    "    with mlflow.start_run():\n",
    "        feat_est = forest.RandomForestClassifier(\n",
    "            n_estimators=config['n_estimators'],\n",
    "            max_features=config['max_features'],\n",
    "            min_samples_leaf=config['min_samples_leaf'],\n",
    "            class_weight=config['class_weight'],\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "        feat_selector = SelectFromModel(feat_est, threshold=config['feat_select_threshold'])\n",
    "        train_x_new = feat_selector.fit_transform(train_x, train_y)\n",
    "\n",
    "        clf = forest.RandomForestClassifier(\n",
    "            n_estimators=config['n_estimators'],\n",
    "            max_features=config['max_features'],\n",
    "            min_samples_leaf=config['min_samples_leaf'],\n",
    "            class_weight=config['class_weight'],\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "        clf.fit(train_x_new, train_y)\n",
    "\n",
    "        valid_x_new = feat_selector.transform(valid_x)\n",
    "\n",
    "        train_preds = clf.predict_proba(train_x_new)\n",
    "        valid_preds = clf.predict_proba(valid_x_new)\n",
    "\n",
    "        for param in config:\n",
    "            log_param(param, config[param])\n",
    "\n",
    "        #log_metric('train_aucroc', roc_auc_score(train_y, [pred[1] for pred in train_preds]))\n",
    "        #log_metric('train_ap', average_precision_score(train_y, [pred[1] for pred in train_preds]))\n",
    "        log_metric('valid_aucroc', roc_auc_score(valid_y, [pred[1] for pred in valid_preds]))\n",
    "        log_metric('valid_ap', average_precision_score(valid_y, [pred[1] for pred in valid_preds]))\n",
    "\n",
    "        log_model(feat_selector, 'feat_selector')\n",
    "        log_model(clf, \"model\")\n",
    "\n",
    "        feature_selected_features = pd.DataFrame(zip(train_x.columns[feat_selector.get_support()], clf.feature_importances_), columns=['feature', 'rf_importance']).sort_values('rf_importance', ascending=False)\n",
    "        feature_selected_features.to_csv('./feature_selected_features.csv', index=False)\n",
    "        log_artifact('./feature_selected_features.csv')\n",
    "\n",
    "        input_features = pd.DataFrame(train_x.columns, columns=['feature'])\n",
    "        input_features.to_csv('./input_features.csv', index=False)\n",
    "        log_artifact('./input_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_est = forest.RandomForestClassifier(\n",
    "    n_estimators=1000,\n",
    "    max_features='auto',\n",
    "    min_samples_leaf=200,\n",
    "    class_weight=None,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "feat_selector = SelectFromModel(feat_est, threshold='32*median')\n",
    "train_x_new = feat_selector.fit_transform(train_x, train_y)\n",
    "\n",
    "clf = forest.RandomForestClassifier(\n",
    "    n_estimators=1000,\n",
    "    max_features='auto',\n",
    "    min_samples_leaf=200,\n",
    "    class_weight=None,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "clf.fit(train_x_new, train_y)\n",
    "\n",
    "valid_x_new = feat_selector.transform(valid_x)\n",
    "valid_preds = clf.predict_proba(valid_x_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_precision_score(valid_y, [pred[1] for pred in valid_preds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_x_shap = pd.concat([valid_x.loc[:, feat_selector.get_support()].reset_index(drop=True), valid_idens.reset_index(drop=True)], axis=1)\n",
    "valid_x_shap['preds'] = [pred[1] for pred in valid_preds]\n",
    "valid_x_shap['target'] = valid_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_x_shap = valid_x_shap[valid_x_shap.censusdate == pd.to_datetime('2019-02-20')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_valid = valid_x_shap.sort_values(by='preds', ascending=False)\n",
    "sorted_valid_dr = sorted_valid.drop(columns=['preds', 'masterpatientid', 'censusdate'])\n",
    "idens = sorted_valid.loc[:, ['masterpatientid', 'censusdate', 'preds', 'target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = sorted_valid_dr.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = explainer.shap_values(tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(explainer.expected_value[1], shap_values[1][0], tt.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = []\n",
    "\n",
    "for i in range(0,10):\n",
    "    shaps = pd.DataFrame({'feature_name':tt.columns, 'shap_value': shap_values[1][i], 'feature_value': tt.iloc[i]}).sort_values(by='shap_value', ascending=False)\n",
    "    shaps = shaps.head(n=10)\n",
    "    shaps['masterpatientid'] = hash(str(idens.iloc[i].masterpatientid))\n",
    "    shaps['censusdate'] = idens.iloc[i].censusdate\n",
    "    shaps['prediction'] = idens.iloc[i].preds\n",
    "    shaps['rehosped'] = idens.iloc[i].target\n",
    "    out.append(shaps)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(out).to_csv('/code/data/copd_model_2019-02-20.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'feature_name':tt.columns, 'shap_value': shap_values[1][0], 'feature_value': tt.iloc[0]}).sort_values(by='shap_value', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
