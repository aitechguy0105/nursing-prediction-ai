{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, '/src')\n",
    "from eliot import to_file\n",
    "to_file(sys.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "processed_path = Path('/data/processed')\n",
    "raw_path = Path('/data/raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "base_df = pd.read_parquet(processed_path/'04-result.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove features which have 98% 0 values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def na_analysis(df):\n",
    "    lst = []\n",
    "    cols = []\n",
    "    total_rows = df.shape[0]\n",
    "    for col in df.columns:\n",
    "        # Sum of NaN values in a column\n",
    "        na_values = df[col].eq(0).sum()\n",
    "        lst.extend([[col,total_rows,na_values,(na_values/total_rows)*100]])\n",
    "        if ((na_values/total_rows)*100) > 98 and (col not in cols):\n",
    "            cols.append(col)\n",
    "    \n",
    "#     print(cols)\n",
    "    return lst\n",
    "\n",
    "\n",
    "df_na = pd.DataFrame(\n",
    "    na_analysis(base_df),\n",
    "    columns=['column_name','total_count','null_values','%_null_values']\n",
    ")\n",
    "\n",
    "df_na.sort_values(['%_null_values'],ascending=False,inplace=True)\n",
    "\n",
    "df_na.head(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "drop_cols = df_na[\n",
    "    (df_na['%_null_values'] == 100) & (~df_na['column_name'].str.startswith('hosp_target'))\n",
    "]['column_name']\n",
    "base_df.drop(drop_cols,\n",
    "        axis=1,\n",
    "        inplace=True\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['censusdate', 'facilityid', 'bedid', 'masterpatientid',\n",
       "       'vtl_Pain Level_3_day_count', 'vtl_Pain Level_3_day_min',\n",
       "       'vtl_Pain Level_3_day_max', 'vtl_Pain Level_3_day_mean',\n",
       "       'vtl_Weight_3_day_count', 'vtl_Weight_3_day_min',\n",
       "       ...\n",
       "       'e_pn_unit_290_ewm', 'e_pn_unit_291_ewm', 'e_pn_unit_292_ewm',\n",
       "       'e_pn_unit_293_ewm', 'e_pn_unit_294_ewm', 'e_pn_unit_295_ewm',\n",
       "       'e_pn_unit_296_ewm', 'e_pn_unit_297_ewm', 'e_pn_unit_298_ewm',\n",
       "       'e_pn_unit_299_ewm'],\n",
       "      dtype='object', length=5305)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df.to_parquet(processed_path/'05-result.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## =======================END====================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df = pd.read_parquet(processed_path/'05-result.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif, mutual_info_regression\n",
    "from sklearn.feature_selection import SelectKBest, SelectPercentile\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import SelectKBest, SelectPercentile\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process the dataframe before running feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "output_cols = [col for col in base_df.columns if 'hosp_target' in col]\n",
    "x_df = base_df[base_df.columns.difference(output_cols)]\n",
    "y_df = base_df[output_cols]\n",
    "print(x_df.shape)\n",
    "print(y_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_cols = ['masterpatientid','censusdate', 'facilityid', 'bedid', 'client']\n",
    "\n",
    "x_df = x_df[x_df.columns.difference(exclude_cols)]\n",
    "x_df.shape\n",
    "\n",
    "y_df = y_df.fillna(False)\n",
    "y_df['hosp_target_3_day_hosp'] = y_df['hosp_target_3_day_hosp'].astype('float32')\n",
    "target_3_day = y_df['hosp_target_3_day_hosp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_na_train(df):\n",
    "    # Fill Median value for all NaN's in the respective columns\n",
    "    has_na = df.isna().sum() > 0\n",
    "    d = df.loc[:, has_na].median()\n",
    "    df = df.fillna(d)\n",
    "    \n",
    "    return df, d\n",
    "\n",
    "def fill_na_valid_or_test(df, na_filler):\n",
    "    return df.fillna(na_filler)\n",
    "\n",
    "\n",
    "x_df, na_filler = fill_na_train(x_df)\n",
    "x_df = x_df.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_df.shape)\n",
    "print(y_df.shape)\n",
    "print(len(target_3_day))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df.to_parquet(processed_path/'x_df.parquet')\n",
    "with open(processed_path/'target_3_day.pickle','wb') as f: pickle.dump(target_3_day, f, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df = pd.read_parquet(processed_path/'x_df.parquet')\n",
    "with open(processed_path/'target_3_day.pickle','rb') as f: target_3_day = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Correlation for all features with the target\n",
    "\n",
    "corr_matrix = x_df.corrwith(y_df['hosp_target_3_day_hosp'])\n",
    "\n",
    "_df = pd.DataFrame({'cols':corr_matrix.index, 'value':corr_matrix.values})\n",
    "_df.sort_values(by='value',ascending=False).head(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "## Remove constant features\n",
    "\n",
    "constant_features = []\n",
    "for feat in x_df.columns:\n",
    "    # convert all features to Float32\n",
    "    \n",
    "    if x_df[feat].std() == 0:\n",
    "        constant_features.append(feat)\n",
    "\n",
    "print(constant_features)\n",
    "\n",
    "# x_df.drop(labels=constant_features, axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Remove duplicated features\n",
    "\n",
    "duplicated_features = []\n",
    "for i in range(0, len(x_df.columns)):\n",
    "    col_1 = x_df.columns[i]\n",
    "\n",
    "    for col_2 in x_df.columns[i + 1:]:\n",
    "        if x_df[col_1].equals(x_df[col_2]):\n",
    "            duplicated_features.append(col_2)\n",
    "\n",
    "print(duplicated_features)\n",
    "\n",
    "# x_df.drop(labels=duplicated_features, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "# calculate the mutual information between the variables and the target\n",
    "# this returns the mutual information value of each feature.\n",
    "# the smaller the value the less information the feature has about the target\n",
    "\n",
    "\n",
    "mi = mutual_info_classif(x_df.fillna(0), target_3_day)\n",
    "print(mi)\n",
    "\n",
    "# let's add the variable names and order the features\n",
    "# according to the MI for clearer visualisation\n",
    "mi = pd.Series(mi)\n",
    "mi.index = x_df.columns\n",
    "mi = mi.sort_values(ascending=False)\n",
    "mi.to_csv('mi-date_cols.csv', header=True)\n",
    "# and now let's plot the ordered MI values per feature\n",
    "mi.sort_values(ascending=False).plot.bar(figsize=(20, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# here I will select the top 10 features\n",
    "# which are shown below\n",
    "sel_ = SelectKBest(mutual_info_classif, k=10).fit(x_df.fillna(0), target_3_day)\n",
    "x_df.columns[sel_.get_support()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# calculate the chi2 p_value between each of the variables\n",
    "# and the target\n",
    "# it returns 2 arrays, one contains the F-Scores which are then\n",
    "# evaluated against the chi2 distribution to obtain the pvalue\n",
    "# the pvalues are in the second array, see below\n",
    "\n",
    "f_score = chi2(x_df, target_3_day)\n",
    "f_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind, that contrarily to MI, where we were interested in the higher MI values,\n",
    "for Fisher score, the smaller the p_value, the more significant the feature is to predict the target.\n",
    "\n",
    "One thing to keep in mind when using Fisher score or univariate selection methods,\n",
    "is that in very big datasets, most of the features will show a small p_value,\n",
    "and therefore look like they are highly predictive.\n",
    "This is in fact an effect of the sample size. So care should be taken when selecting features\n",
    "using these procedures. An ultra tiny p_value does not highlight an ultra-important feature,\n",
    "it rather indicates that the dataset contains too many samples.\n",
    "\n",
    "If the dataset contained several categorical variables, we could then combine this procedure with\n",
    "SelectKBest or SelectPercentile, as I did in the previous lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# let's add the variable names and order it for clearer visualisation\n",
    "\n",
    "pvalues = pd.Series(f_score[1])\n",
    "pvalues.index = x_df.columns\n",
    "pvalues.sort_values(ascending=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# LASSO Regularization\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "logistic = LogisticRegression(C=1, penalty='l1',solver='liblinear',random_state=7).fit(x_df,target_3_day)\n",
    "model = SelectFromModel(logistic, prefit=True)\n",
    "\n",
    "# x_new_df = model.transform(x_df)\n",
    "\n",
    "# this command let's me visualise those features that were kept\n",
    "model.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Now I make a list with the selected features\n",
    "selected_feat = x_df.columns[(model.get_support())]\n",
    "\n",
    "print('total features: {}'.format((x_df.shape[1])))\n",
    "print('selected features: {}'.format(len(selected_feat)))\n",
    "print('features with coefficients shrank to zero: {}'.format(\n",
    "    np.sum(model.estimator_.coef_ == 0)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "\n",
    "# step forward feature selection\n",
    "# I indicate that I want to select 10 features from\n",
    "# the total, and that I want to select those features\n",
    "# based on the optimal roc_auc\n",
    "\n",
    "sfs1 = SFS(RandomForestRegressor(),\n",
    "           k_features=20,\n",
    "           forward=True,\n",
    "           floating=False,\n",
    "           verbose=2,\n",
    "           scoring='r2',\n",
    "           cv=3)\n",
    "\n",
    "sfs1 = sfs1.fit(np.array(x_df), target_3_day)\n",
    "selected_feat= x_df.columns[list(sfs1.k_feature_idx_)]\n",
    "selected_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mlxtend"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
