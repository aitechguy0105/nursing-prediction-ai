{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import timeit\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dataset are from TRAIN_START_DATE = '2017-01-01' & TRAIN_END_DATE = '2020-03-10'.\n",
    "We split this to training set, test set & validation set\n",
    "\"\"\"\n",
    "\n",
    "train_end_date = '2020-09-01'\n",
    "valid_end_date = '2020-11-29'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_path = Path('/data/processed')\n",
    "processed_path.mkdir(parents=True, exist_ok=True)\n",
    "filename = '05-result.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final = pd.read_parquet(processed_path/'02-result.parquet')\n",
    "final = pd.read_parquet(processed_path/f'{filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column processing\n",
    "\n",
    "final['client'] = final['masterpatientid'].apply(lambda z: z.split('_')[0])\n",
    "final[\"facilityid\"] = final[\"client\"] + \"_\" + final[\"facilityid\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually fill in target columns with 0 so we don't also get na indicators for them\n",
    "final['hosp_target_3_day_hosp'] = final.hosp_target_3_day_hosp.fillna(False)\n",
    "final['hosp_target_7_day_hosp'] = final.hosp_target_7_day_hosp.fillna(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual check to make sure we're not including any columns that could leak data\n",
    "with open('/data/processed/columns.txt','w') as f:\n",
    "    for col in final.columns:\n",
    "        f.write(col + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = final.loc[final.censusdate <= train_end_date]\n",
    "valid = final.loc[(final.censusdate > train_end_date) & (final.censusdate <= valid_end_date)]\n",
    "test = final.loc[final.censusdate > valid_end_date]\n",
    "\n",
    "print(train.shape)\n",
    "print(valid.shape)\n",
    "print(test.shape)\n",
    "\n",
    "del final\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start of basic tests - assert we have disjoint sets over time\n",
    "assert train.censusdate.max() < valid.censusdate.min()\n",
    "assert valid.censusdate.max() < test.censusdate.min()\n",
    "assert train.hosp_target_3_day_hosp.mean() < train.hosp_target_7_day_hosp.mean()\n",
    "assert valid.hosp_target_3_day_hosp.mean() < valid.hosp_target_7_day_hosp.mean()\n",
    "# assert test.hosp_target_3_day_hosp.mean() < test.hosp_target_7_day_hosp.mean()\n",
    "print('Success...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Train set covers {train.censusdate.min()} to {train.censusdate.max()} with 3_day_hosp percentage {train.hosp_target_3_day_hosp.mean()} and 7_day_hosp percentage {train.hosp_target_7_day_hosp.mean()}')\n",
    "print(f'Valid set covers {valid.censusdate.min()} to {valid.censusdate.max()} with 3_day_hosp percentage {valid.hosp_target_3_day_hosp.mean()} and 7_day_hosp percentage {valid.hosp_target_7_day_hosp.mean()}')\n",
    "print(f'Test set covers {test.censusdate.min()} to {test.censusdate.max()} with 3_day_hosp percentage {test.hosp_target_3_day_hosp.mean()} and 7_day_hosp percentage {test.hosp_target_7_day_hosp.mean()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_na_train(df):\n",
    "    \"\"\" Get Median value for all columns that contain NaN and all vital columns.\n",
    "    Store these Median values in a file in S3 & use them when ever a column has NaN during prediction\n",
    "    \"\"\"\n",
    "    nan_cols = [col for col in train.columns if (train[col].isnull().any()) or (col.startswith('vtl'))]\n",
    "    \n",
    "    d = df.loc[:, nan_cols].median()\n",
    "    df = df.fillna(d)\n",
    "    \n",
    "    return df, d\n",
    "\n",
    "def fill_na_valid_or_test(df, na_filler):\n",
    "    return df.fillna(na_filler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# fill in any remaining na's - now that we're not forwardfilling past info it's not correct to use a global imputation\n",
    "# hence we impute on the train and apply to the valid and test\n",
    "# We save these na filler values to use them during predictions\n",
    "\n",
    "train, na_filler = fill_na_train(train)\n",
    "valid = fill_na_valid_or_test(valid, na_filler)\n",
    "test = fill_na_valid_or_test(test, na_filler)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the Target values & identification columns\n",
    "def prep(df):\n",
    "    drop_cols = ['censusdate', 'masterpatientid', 'facilityid', 'bedid', 'client']\n",
    "    drop_cols = drop_cols + [col for col in df.columns if 'target' in col]\n",
    "\n",
    "    target_3_day = df.hosp_target_3_day_hosp.astype('float32').values\n",
    "    target_7_day = df.hosp_target_7_day_hosp.astype('float32').values\n",
    "    x = df.drop(columns=drop_cols).reset_index(drop=True).astype('float32')\n",
    "    idens = df.loc[:,['masterpatientid','censusdate', 'facilityid', 'bedid', 'client']]\n",
    "    \n",
    "    return x, target_3_day, target_7_day, idens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Seperate target, x-frame and identification columns\n",
    "train_x, train_target_3_day, train_target_7_day, train_idens = prep(train)\n",
    "del train\n",
    "valid_x, valid_target_3_day, valid_target_7_day, valid_idens = prep(valid)\n",
    "del valid\n",
    "test_x, test_target_3_day, test_target_7_day, test_idens = prep(test)\n",
    "del test\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure for that x's, targets, an idens all have the same # of rows\n",
    "assert train_x.shape[0] == train_target_3_day.shape[0] == train_target_7_day.shape[0] == train_idens.shape[0]\n",
    "assert valid_x.shape[0] == valid_target_3_day.shape[0] == valid_target_7_day.shape[0] == valid_idens.shape[0]\n",
    "assert test_x.shape[0] == test_target_3_day.shape[0] == test_target_7_day.shape[0] == test_idens.shape[0]\n",
    "\n",
    "# make sure that train, valid, and test have the same # of columns\n",
    "assert train_x.shape[1] == valid_x.shape[1] == test_x.shape[1] \n",
    "\n",
    "# make sure that the idens all have the same # of columns\n",
    "assert train_idens.shape[1] == valid_idens.shape[1] == test_idens.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Save train, test and validation datasets in local folder\n",
    "\n",
    "import pickle;\n",
    "with open(processed_path/'final-train_x.pickle','wb') as f: pickle.dump(train_x, f, protocol=4)\n",
    "with open(processed_path/'final-train_target_3_day.pickle','wb') as f: pickle.dump(train_target_3_day, f, protocol=4)\n",
    "with open(processed_path/'final-train_target_7_day.pickle','wb') as f: pickle.dump(train_target_7_day, f, protocol=4)\n",
    "with open(processed_path/'final-train_idens.pickle','wb') as f: pickle.dump(train_idens, f, protocol=4)\n",
    "\n",
    "with open(processed_path/'final-valid_x.pickle','wb') as f: pickle.dump(valid_x, f, protocol=4)\n",
    "with open(processed_path/'final-valid_target_3_day.pickle','wb') as f: pickle.dump(valid_target_3_day, f, protocol=4)\n",
    "with open(processed_path/'final-valid_target_7_day.pickle','wb') as f: pickle.dump(valid_target_7_day, f, protocol=4)\n",
    "with open(processed_path/'final-valid_idens.pickle','wb') as f: pickle.dump(valid_idens, f, protocol=4)\n",
    "    \n",
    "with open(processed_path/'final-test_x.pickle','wb') as f: pickle.dump(test_x, f, protocol=4)\n",
    "with open(processed_path/'final-test_target_3_day.pickle','wb') as f: pickle.dump(test_target_3_day, f, protocol=4)\n",
    "with open(processed_path/'final-test_target_7_day.pickle','wb') as f: pickle.dump(test_target_7_day, f, protocol=4)\n",
    "with open(processed_path/'final-test_idens.pickle','wb') as f: pickle.dump(test_idens, f, protocol=4)\n",
    "    \n",
    "with open(processed_path/'final-na_filler.pickle', 'wb') as f: pickle.dump(na_filler, f, protocol=4)\n",
    "    \n",
    "print(\"--------------Completed--------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
