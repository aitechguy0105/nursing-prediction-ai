{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import sys\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import timedelta, datetime\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from saiva.model.shared.constants import MODEL_TYPE\n",
    "from saiva.model.shared.utils import get_client_class, url_encode_cols\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from saiva.model.shared.constants import LOCAL_TRAINING_CONFIG_PATH\n",
    "from saiva.training.utils import load_config\n",
    "\n",
    "config = load_config(LOCAL_TRAINING_CONFIG_PATH)\n",
    "training_config = config.training_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_TYPE = MODEL_TYPE.lower()\n",
    "print('MODEL:', MODEL_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    " ## =========== Set HYPER_PARAMETER_TUNING in constants.py ==========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIENT = \"+\".join([config.organization_id for config in training_config.organization_configs])\n",
    "\n",
    "EXPERIMENT_DATES = training_config.training_metadata.experiment_dates\n",
    "HYPER_PARAMETER_TUNING = training_config.training_metadata.hyper_parameter_tuning\n",
    "\n",
    "# starting training from day 31 so that cumsum window 2,7,14,30 are all initial correct.\n",
    "# One day will be added to `censusdate` later in the code, so that the first date in\n",
    "# `train` will be `EXPERIMENT_DATES['train_start_date'] + 1 day`, that's why here we\n",
    "# add 31 days but not 30\n",
    "EXPERIMENT_DATES['train_start_date'] = str((pd.to_datetime(EXPERIMENT_DATES['train_start_date']) +  pd.DateOffset(days=31)).date())\n",
    "\n",
    "if not HYPER_PARAMETER_TUNING:\n",
    "    EXPERIMENT_DATES['train_end_date'] = (datetime.strptime(EXPERIMENT_DATES['validation_end_date'], '%Y-%m-%d') - timedelta(days=2)).strftime('%Y-%m-%d')\n",
    "    EXPERIMENT_DATES['validation_start_date'] = (datetime.strptime(EXPERIMENT_DATES['validation_end_date'], '%Y-%m-%d') - timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "    \n",
    "print(CLIENT)\n",
    "print(HYPER_PARAMETER_TUNING)\n",
    "print(EXPERIMENT_DATES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "processed_path = Path('/data/processed')\n",
    "processed_path.mkdir(parents=True, exist_ok=True)\n",
    "filename = 'final_cleaned_df.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "final = pd.read_parquet(processed_path/f'{filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = url_encode_cols(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert f'target_3_day_{MODEL_TYPE}' in final.columns, f\"There is no target for training `{MODEL_TYPE}` model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDEN_COLS = ['censusdate', 'facilityid', 'masterpatientid', 'LFS', 'primaryphysicianid',\n",
    "         'payername', 'to_from_type', 'client', 'admissionstatus',\n",
    "         f'positive_date_{MODEL_TYPE}']\n",
    "\n",
    "# UPT model doesn't need the rows that with payername contains 'hospice'\n",
    "if MODEL_TYPE=='model_upt':\n",
    "    final = final[~(final['payername'].str.contains('hospice', case=False, regex=True, na=False))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# column processing\n",
    "final['client'] = final['masterpatientid'].apply(lambda z: z.split('_')[0])\n",
    "final[\"facilityid\"] = final[\"client\"] + \"_\" + final[\"facilityid\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final['LFS'] = final['days_since_last_admission']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" We increment the census date by 1, since the prediction day always includes data upto last night.\n",
    "This means for every census date the data is upto previous night. \n",
    "\"\"\"\n",
    "print(final.shape)\n",
    "\n",
    "# Increment censusdate by 1\n",
    "final['censusdate'] = (pd.to_datetime(final['censusdate']) + timedelta(days=1))\n",
    "\n",
    "print(final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def drop_unwanted_columns(df):   \n",
    "    positive_date = f'positive_date_{MODEL_TYPE}'\n",
    "    target_3_day = f'target_3_day_{MODEL_TYPE}'\n",
    "    drop_columns = ['dateofadmission']\n",
    "    dates = list(df.columns[df.columns.str.contains('positive_date_')])\n",
    "    targets = list(df.columns[df.columns.str.contains('target_3_day_')])\n",
    "    for date in dates: \n",
    "        if date!=positive_date:\n",
    "            drop_columns.append(date)\n",
    "    for target in targets:\n",
    "        if target!= target_3_day:\n",
    "            drop_columns.append(target)\n",
    "    df = df.drop(columns=drop_columns, errors='ignore')\n",
    "    return df\n",
    "\n",
    "# drop unwanted columns for this model\n",
    "final = drop_unwanted_columns(final)\n",
    "    \n",
    "final[f'target_3_day_{MODEL_TYPE}'] = final[f'target_3_day_{MODEL_TYPE}'].fillna(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "train = final.loc[(final.censusdate >= EXPERIMENT_DATES['train_start_date']) & (final.censusdate <= EXPERIMENT_DATES['train_end_date'])]\n",
    "valid = final.loc[(final.censusdate >= EXPERIMENT_DATES['validation_start_date']) & (final.censusdate <= EXPERIMENT_DATES['validation_end_date'])]\n",
    "test = final.loc[final.censusdate >= EXPERIMENT_DATES['test_start_date']]\n",
    "\n",
    "def sort_group(group):\n",
    "    return group.sort_values('masterpatientid')\n",
    "\n",
    "valid = valid.groupby(['facilityid', 'censusdate']).apply(sort_group)\n",
    "valid.reset_index(drop=True)\n",
    "\n",
    "test = test.groupby(['facilityid', 'censusdate']).apply(sort_group)\n",
    "test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(final.shape)\n",
    "print(train.shape)\n",
    "print(valid.shape)\n",
    "print(test.shape)\n",
    "\n",
    "del final\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in train.columns:\n",
    "    if 'target_3_day' in col:\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'train - target_3_day_{MODEL_TYPE}', train[f'target_3_day_{MODEL_TYPE}'].value_counts())\n",
    "print(f'valid - target_3_day_{MODEL_TYPE}', valid[f'target_3_day_{MODEL_TYPE}'].value_counts())\n",
    "print(f'test - target_3_day_{MODEL_TYPE}', test[f'target_3_day_{MODEL_TYPE}'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# start of basic tests - assert we have disjoint sets over time\n",
    "assert train.censusdate.max() < valid.censusdate.min()\n",
    "assert valid.censusdate.max() < test.censusdate.min()\n",
    "print('Success...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f'Train set covers {train.censusdate.min()} to {train.censusdate.max()} with 3_day_{MODEL_TYPE} percentage {train[f\"target_3_day_{MODEL_TYPE}\"].mean()}')\n",
    "print(f'Valid set covers {valid.censusdate.min()} to {valid.censusdate.max()} with 3_day_{MODEL_TYPE} percentage {valid[f\"target_3_day_{MODEL_TYPE}\"].mean()}')\n",
    "print(f'Test set covers {test.censusdate.min()} to {test.censusdate.max()} with 3_day_{MODEL_TYPE} percentage {test[f\"target_3_day_{MODEL_TYPE}\"].mean()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for col in train.columns:\n",
    "    if train[col].dtypes=='datetime64[ns]':\n",
    "        if col not in IDEN_COLS:\n",
    "            print(col, train[col].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Remove the Target values & identification columns\n",
    "# Keep facilityid in idens and add a duplicate field as facility for featurisation\n",
    "def prep(df, feature_names=None, category_columns=None, pandas_categorical=None):\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    drop_cols = IDEN_COLS + [col for col in df.columns if 'target' in col]\n",
    "    drop_cols += [col for col in df.columns if 'positive_date_' in col]\n",
    "\n",
    "    target_3_day = df[f'target_3_day_{MODEL_TYPE}'].astype('float32').values\n",
    "    \n",
    "    df['facility'] = df['facilityid']  \n",
    "    df['facility'] = df['facility'].astype('category')\n",
    "    \n",
    "    x = df.drop(columns=drop_cols).reset_index(drop=True)\n",
    "    \n",
    "    if feature_names is None:\n",
    "        feature_names = x.columns.tolist()\n",
    "    elif (len(x.columns) != len(feature_names)):\n",
    "        raise ValueError(\"train and valid dataset feature names do not match\")\n",
    "    elif (x.columns != feature_names).any():\n",
    "        x = x.reindex(columns=feature_names)     \n",
    "    \n",
    "    if category_columns is None:\n",
    "        category_columns = x.dtypes[x.dtypes == 'category'].index.tolist()\n",
    "    \n",
    "    if pandas_categorical is None:\n",
    "        \n",
    "        pandas_categorical = [list(x[col].cat.categories) for col in category_columns]\n",
    "  \n",
    "    else:\n",
    "        if len(category_columns) != len(pandas_categorical):\n",
    "            raise ValueError(\"train and valid dataset categorical_feature do not match\")\n",
    "        for col, category in zip(category_columns, pandas_categorical):\n",
    "            if list(x[col].cat.categories) != list(category):\n",
    "                x[col] = x[col].cat.set_categories(category)\n",
    "    \n",
    "    idens = df.loc[:,IDEN_COLS]\n",
    "    #add 'long_short_term' column to indens\n",
    "    short_term_cond = ((x.payertype == 'Managed Care')|(x.payertype == 'Medicare A'))\n",
    "    idens.loc[short_term_cond,'long_short_term']='short'\n",
    "    \n",
    "    nonType_cond = (x.payertype == 'no payer info')\n",
    "    if nonType_cond.sum()>0:\n",
    "        print(f'{nonType_cond.sum()} patient days have no payer info')\n",
    "        idens.loc[nonType_cond,'long_short_term']='no payer info'\n",
    "    \n",
    "    idens.loc[~(short_term_cond|nonType_cond),'long_short_term']='long'\n",
    "    \n",
    "    # converting to numpy array    \n",
    "    x[category_columns] = x[category_columns].apply(lambda col: col.cat.codes).replace({-1: np.nan})\n",
    "    x = x.to_numpy(dtype=np.float32, na_value=np.nan)\n",
    "\n",
    "    return x, target_3_day, idens, feature_names, category_columns, pandas_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Seperate target, x-frame and identification columns\n",
    "train_x, train_target_3_day, train_idens, feature_names, cate_columns, pandas_categorical = prep(train)\n",
    "del train\n",
    "valid_x, valid_target_3_day, valid_idens, _, _, _ = prep(valid, feature_names, cate_columns, pandas_categorical)\n",
    "del valid\n",
    "test_x, test_target_3_day, test_idens, _, _, _ = prep(test, feature_names, cate_columns, pandas_categorical)\n",
    "del test\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# make sure for that x's, targets, an idens all have the same # of rows\n",
    "assert train_x.shape[0] == train_target_3_day.shape[0] == train_idens.shape[0]\n",
    "assert valid_x.shape[0] == valid_target_3_day.shape[0] == valid_idens.shape[0]\n",
    "assert test_x.shape[0] == test_target_3_day.shape[0] == test_idens.shape[0]\n",
    "\n",
    "# make sure that train, valid, and test have the same # of columns\n",
    "assert train_x.shape[1] == valid_x.shape[1] == test_x.shape[1]\n",
    "\n",
    "# make sure that the idens all have the same # of columns\n",
    "assert train_idens.shape[1] == valid_idens.shape[1] == test_idens.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Save train, test and validation datasets in local folder\n",
    "\n",
    "import pickle\n",
    "with open(processed_path/f'final-train_x_{MODEL_TYPE}.pickle','wb') as f: pickle.dump(train_x, f, protocol=4)\n",
    "with open(processed_path/f'final-train_target_3_day_{MODEL_TYPE}.pickle','wb') as f: pickle.dump(train_target_3_day, f, protocol=4)\n",
    "with open(processed_path/f'final-train_idens_{MODEL_TYPE}.pickle','wb') as f: pickle.dump(train_idens, f, protocol=4)\n",
    "\n",
    "with open(processed_path/f'final-valid_x_{MODEL_TYPE}.pickle','wb') as f: pickle.dump(valid_x, f, protocol=4)\n",
    "with open(processed_path/f'final-valid_target_3_day_{MODEL_TYPE}.pickle','wb') as f: pickle.dump(valid_target_3_day, f, protocol=4)\n",
    "with open(processed_path/f'final-valid_idens_{MODEL_TYPE}.pickle','wb') as f: pickle.dump(valid_idens, f, protocol=4)\n",
    "\n",
    "with open(processed_path/f'final-test_x_{MODEL_TYPE}.pickle','wb') as f: pickle.dump(test_x, f, protocol=4)\n",
    "with open(processed_path/f'final-test_target_3_day_{MODEL_TYPE}.pickle','wb') as f: pickle.dump(test_target_3_day, f, protocol=4)\n",
    "with open(processed_path/f'final-test_idens_{MODEL_TYPE}.pickle','wb') as f: pickle.dump(test_idens, f, protocol=4)\n",
    "\n",
    "with open(processed_path/'cate_columns.pickle', 'wb') as f: pickle.dump(cate_columns, f, protocol=4)\n",
    "with open(processed_path/'feature_names.pickle', 'wb') as f: pickle.dump(feature_names, f, protocol=4)\n",
    "with open(processed_path/'pandas_categorical.pickle', 'wb') as f: pickle.dump(pandas_categorical, f, protocol=4)\n",
    "    \n",
    "with open('./cate_columns.pickle', 'wb') as f: pickle.dump(cate_columns, f, protocol=4)\n",
    "\n",
    "print(\"--------------Completed--------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_x.shape)\n",
    "print(train_target_3_day.shape)\n",
    "print(valid_x.shape)\n",
    "print(valid_target_3_day.shape)\n",
    "print(test_x.shape)\n",
    "print(test_target_3_day.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
