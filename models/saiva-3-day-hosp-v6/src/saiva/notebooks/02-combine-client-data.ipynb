{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# import modin.pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import timeit\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from saiva.model.shared.load_raw_data import get_genric_file_names\n",
    "from eliot import to_file\n",
    "to_file(sys.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from saiva.model.shared.constants import saiva_api, LOCAL_TRAINING_CONFIG_PATH\n",
    "from saiva.training.utils import load_config\n",
    "\n",
    "config = load_config(LOCAL_TRAINING_CONFIG_PATH)\n",
    "training_config = config.training_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace this if necessary\n",
    "CLIENT = training_config.organization_configs[0].organization_id\n",
    "\n",
    "# Strip client name and get the actual dataframe names\n",
    "data_path = Path('/data/raw')\n",
    "client_file_types = get_genric_file_names(data_path=data_path, client=CLIENT)\n",
    "\n",
    "print(client_file_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- When using other clients data, merge respective files across all \n",
    "  given clients and rename them removing the client name\n",
    "- Append client_name to masterpatientid\n",
    "- Add a client column\n",
    "\"\"\"\n",
    "start_time = timeit.default_timer()\n",
    "for ft in client_file_types:\n",
    "    try:\n",
    "        # Fetch same file across client\n",
    "        client_files = data_path.glob(f'*_{ft}.parquet')\n",
    "\n",
    "        df = pd.DataFrame()\n",
    "        \n",
    "        # Loop through all the files and combine them\n",
    "        for f in client_files:\n",
    "            client = f.name.split('_')[0]\n",
    "            client_df = pd.read_parquet(f)\n",
    "            if len(client_df) == 0:\n",
    "                continue\n",
    "            client_df['masterpatientid'] = client_df['masterpatientid'].apply(lambda x: client + '_' + str(x))\n",
    "            client_df['client'] = client\n",
    "            df = df.append(client_df, ignore_index=True)\n",
    "            print(f, len(client_df), len(df))\n",
    "\n",
    "        if ft == 'patient_demographics':\n",
    "            df['dateofbirth'] = df['dateofbirth'].astype('datetime64[ms]')\n",
    "            # df['dateofbirth'] = pd.to_datetime(df['dateofbirth'], errors='coerce') # force convert and set invalid values to NaT\n",
    "            \n",
    "        if ft == 'patient_diagnosis':\n",
    "            df['onsetdate'] = df['onsetdate'].astype('datetime64[ms]')\n",
    "            \n",
    "        df.to_parquet(data_path/f'{ft}.parquet')\n",
    "        print('============================')\n",
    "    except Exception as e:\n",
    "        print(ft, 'failed:', e)\n",
    "print(f\"{timeit.default_timer() - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test + validation set is 25%\n",
    "## Note: Obtained training, test, validation dates needs to be added in client respective file under `get_experiment_dates` function!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# test + validation set is 25%\n",
    "# Note: Obtained test, validation dates needs to be added in client respective file under `get_experiment_dates` function\n",
    "\n",
    "def get_prior_date_as_str(date_as_str):\n",
    "    prior_date = pd.to_datetime(date_as_str) - timedelta(days=1)\n",
    "    prior_date_as_str = prior_date.date().strftime('%Y-%m-%d')\n",
    "    return prior_date_as_str\n",
    "\n",
    "\n",
    "data_path = Path('/data/raw')\n",
    "df = pd.read_parquet(data_path/'patient_census.parquet')\n",
    "\n",
    "df.drop_duplicates(\n",
    "    subset=['masterpatientid', 'censusdate'],\n",
    "    keep='last',\n",
    "    inplace=True\n",
    ")\n",
    "df.sort_values(by=['censusdate'], inplace=True)\n",
    "\n",
    "total_count = df.shape[0]\n",
    "test_count = int((total_count * 25) / 100)\n",
    "test_split_count = int((test_count * 50) / 100) # split between validation & test set\n",
    "\n",
    "test_df = df.tail(test_count) # cut last n rows\n",
    "validation_df = test_df.head(test_split_count)\n",
    "test_df = test_df.tail(test_split_count)\n",
    "\n",
    "train_start_date = df.censusdate.min().date().strftime('%Y-%m-%d')\n",
    "validation_start_date = validation_df.censusdate.min().date().strftime('%Y-%m-%d')\n",
    "test_start_date = test_df.censusdate.min().date().strftime('%Y-%m-%d')\n",
    "test_end_date = test_df.censusdate.max().date().strftime('%Y-%m-%d')\n",
    "\n",
    "train_end_date = get_prior_date_as_str(validation_start_date)\n",
    "validation_end_date = get_prior_date_as_str(test_start_date)\n",
    "\n",
    "print(f'train_start_date: {train_start_date}')\n",
    "print(f'train_end_date: {train_end_date}')\n",
    "print(f'validation_start_date: {validation_start_date}')\n",
    "print(f'validation_end_date: {validation_end_date}')\n",
    "print(f'test_start_date: {test_start_date}')\n",
    "print(f'test_end_date: {test_end_date}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double check that you have non-overlapping dates\n",
    "```    \n",
    "   Example:\n",
    "       return {\n",
    "            'train_start_date': '2019-09-01',\n",
    "            'train_end_date': '2021-02-21',\n",
    "            'validation_start_date': '2021-02-22',\n",
    "            'validation_end_date': '2021-06-06',\n",
    "            'test_start_date': '2021-06-07',\n",
    "            'test_end_date': '2021-09-15'\n",
    "        }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We update the experiment configuration in OrganizationMlModelConfig training_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_metadata = training_config.training_metadata\n",
    "training_metadata['experiment_dates'] = {\n",
    "    'train_start_date': train_start_date,\n",
    "    'train_end_date': train_end_date,\n",
    "    'validation_start_date': validation_start_date,\n",
    "    'validation_end_date': validation_end_date,\n",
    "    'test_start_date': test_start_date,\n",
    "    'test_end_date': test_end_date\n",
    "\n",
    "}\n",
    "\n",
    "print(training_metadata)\n",
    "\n",
    "conf = OmegaConf.create({'training_config': {'training_metadata': training_metadata}})\n",
    "OmegaConf.save(conf, f'{LOCAL_TRAINING_CONFIG_PATH}generated/training_metadata.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ======================== TESTING ================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load generic named Training data which is cached in local folders\n",
    "# from shared.load_raw_data import fetch_training_cache_data\n",
    "\n",
    "# result_dict = fetch_training_cache_data(client=CLIENT, generic=True)\n",
    "# for key, value in result_dict.items():\n",
    "#     print(f'{key} : {result_dict[key].info()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all newly generated parquet files\n",
    "\n",
    "# for ft in client_file_types:\n",
    "#     os.remove(data_path/f'{ft}.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
