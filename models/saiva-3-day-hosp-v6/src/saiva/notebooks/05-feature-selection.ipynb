{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "\n",
    "from eliot import to_file\n",
    "from saiva.model.shared.constants import MODEL_TYPE\n",
    "to_file(sys.stdout)\n",
    "\n",
    "MODEL_TYPE = MODEL_TYPE.lower()\n",
    "print('MODEL:', MODEL_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "processed_path = Path('/data/processed')\n",
    "raw_path = Path('/data/raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "base_df = pd.read_parquet(processed_path/'final_df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUMULATIVE_GROUP_MAPPING = {\n",
    "    r'^cumsum_2_day_alert_.*': 'cumsum_2_day_alert',\n",
    "    r'^cumsum_7_day_alert_.*': 'cumsum_7_day_alert',\n",
    "    r'^cumsum_14_day_alert_.*': 'cumsum_14_day_alert',\n",
    "    r'^cumsum_30_day_alert_.*': 'cumsum_30_day_alert',\n",
    "    r'^cumsum_all_alert_.*': 'cumsum_all_alert',\n",
    "    r'^cumsum_2_day_dx_.*': 'cumsum_2_day_dx',\n",
    "    r'^cumsum_7_day_dx_.*': 'cumsum_7_day_dx',\n",
    "    r'^cumsum_14_day_dx_.*': 'cumsum_14_day_dx',\n",
    "    r'^cumsum_30_day_dx_.*': 'cumsum_30_day_dx',\n",
    "    r'^cumsum_all_dx_.*': 'cumsum_all_dx',\n",
    "     r'^cumsum_2_day_med_.*': 'cumsum_2_day_med',\n",
    "    r'^cumsum_7_day_med_.*': 'cumsum_7_day_med',\n",
    "    r'^cumsum_14_day_med_.*': 'cumsum_14_day_med',\n",
    "    r'^cumsum_30_day_med_.*': 'cumsum_30_day_med',\n",
    "    r'^cumsum_all_med_.*': 'cumsum_all_med',\n",
    "    r'^cumsum_2_day_order_.*': 'cumsum_2_day_order',\n",
    "    r'^cumsum_7_day_order_.*': 'cumsum_7_day_order',\n",
    "    r'^cumsum_14_day_order_.*': 'cumsum_14_day_order',\n",
    "    r'^cumsum_30_day_order_.*': 'cumsum_30_day_order',\n",
    "    r'^cumsum_all_order_.*': 'cumsum_all_order',\n",
    "    r'^cumsum_2_day_labs_.*': 'cumsum_2_day_labs',\n",
    "    r'^cumsum_7_day_labs_.*': 'cumsum_7_day_labs',\n",
    "    r'^cumsum_14_day_labs_.*': 'cumsum_14_day_labs',\n",
    "    r'^cumsum_30_day_labs_.*': 'cumsum_30_day_labs',\n",
    "    r'^cumsum_all_labs_.*': 'cumsum_all_labs',\n",
    "    \n",
    "    r'^cumidx_2_day_alert_.*': 'cumidx_2_day_alert',\n",
    "    r'^cumidx_7_day_alert_.*': 'cumidx_7_day_alert',\n",
    "    r'^cumidx_14_day_alert_.*': 'cumidx_14_day_alert',\n",
    "    r'^cumidx_30_day_alert_.*': 'cumidx_30_day_alert',\n",
    "    r'^cumidx_all_alert_.*': 'cumidx_all_alert',\n",
    "    r'^cumidx_2_day_dx_.*': 'cumidx_2_day_dx',\n",
    "    r'^cumidx_7_day_dx_.*': 'cumidx_7_day_dx',\n",
    "    r'^cumidx_14_day_dx_.*': 'cumidx_14_day_dx',\n",
    "    r'^cumidx_30_day_dx_.*': 'cumidx_30_day_dx',\n",
    "    r'^cumidx_all_dx_.*': 'cumidx_all_dx',\n",
    "    r'^cumidx_2_day_med_.*': 'cumidx_2_day_med',\n",
    "    r'^cumidx_7_day_med_.*': 'cumidx_7_day_med',\n",
    "    r'^cumidx_14_day_med_.*': 'cumidx_14_day_med',\n",
    "    r'^cumidx_30_day_med_.*': 'cumidx_30_day_med',\n",
    "    r'^cumidx_all_med_.*': 'cumidx_all_med',\n",
    "    r'^cumidx_2_day_order_.*': 'cumidx_2_day_order',\n",
    "    r'^cumidx_7_day_order_.*': 'cumidx_7_day_order',\n",
    "    r'^cumidx_14_day_order_.*': 'cumidx_14_day_order',\n",
    "    r'^cumidx_30_day_order_.*': 'cumidx_30_day_order',\n",
    "    r'^cumidx_all_order_.*': 'cumidx_all_order',\n",
    "    r'^cumidx_2_day_labs_.*': 'cumidx_2_day_labs',\n",
    "    r'^cumidx_7_day_labs_.*': 'cumidx_7_day_labs',\n",
    "    r'^cumidx_14_day_labs_.*': 'cumidx_14_day_labs',\n",
    "    r'^cumidx_30_day_labs_.*': 'cumidx_30_day_labs',\n",
    "    r'^cumidx_all_labs_.*': 'cumidx_all_labs',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open ('./feature_groups.json', \"r\")\n",
    "feature_groups = json.loads(f.read())\n",
    "# Not the most efficient code but not optimizing since the cell runs pretty fast\n",
    "def get_feature_group_counts():\n",
    "    training_feats = base_df.columns\n",
    "    features = {}\n",
    "    for grp in feature_groups:\n",
    "        features[grp] = len([x for x in training_feats if x in feature_groups[grp]])\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cumulative_group_counts():\n",
    "    training_feats = pd.DataFrame({'feature': list(base_df.columns)})\n",
    "    training_feats['feature_group'] = training_feats.feature.replace(\n",
    "            CUMULATIVE_GROUP_MAPPING,\n",
    "            regex=True\n",
    "        )\n",
    "    features = training_feats.groupby('feature_group')['feature_group'].count().to_dict()\n",
    "    cumulative_cols = CUMULATIVE_GROUP_MAPPING.values()\n",
    "    features = {k: features.get(k, 0) for k in cumulative_cols}\n",
    "\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_drop_stats = {}\n",
    "cumulative_feature_drop_stats = {}\n",
    "\n",
    "feature_group_count = get_feature_group_counts()\n",
    "cumulative_group_count = get_cumulative_group_counts()\n",
    "for grp in feature_groups:\n",
    "    feature_drop_stats[grp] = {'before_drop_count': feature_group_count[grp]}\n",
    "    \n",
    "for grp in cumulative_group_count:\n",
    "    cumulative_feature_drop_stats[grp] = {'before_drop_count': cumulative_group_count[grp]}\n",
    "feature_drop_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop all columns with 100% Null values except for Idens Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDEN_COLS = ['censusdate', 'facilityid', 'masterpatientid', 'LFS', 'primaryphysicianid',\n",
    "         'payername', 'to_from_type', 'client', 'admissionstatus',\n",
    "         f'positive_date_{MODEL_TYPE}', f'target_3_day_{MODEL_TYPE}']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_idens_cols_all_null = [col for col in base_df.columns if base_df[col].isnull().all() and col not in IDEN_COLS]\n",
    "non_idens_cols_all_null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df.drop(non_idens_cols_all_null, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop columns with single value in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_with_single_value = []\n",
    "for col in base_df.columns:\n",
    "    if len(base_df[col].value_counts()) == 1 and base_df[col].value_counts().iloc[0] == len(base_df) and col not in IDEN_COLS:\n",
    "        cols_with_single_value.append(col)\n",
    "# cols_with_single_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df.drop(cols_with_single_value,inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = {'single_valued_columns': cols_with_single_value, 'all_null_columns': non_idens_cols_all_null}\n",
    "\n",
    "# Dump the merged dictionary into the JSON file\n",
    "with open('all_null_dropped_col_names.json', 'w') as json_file:\n",
    "    json.dump(cols_to_drop, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove features which have 100% 0 values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def na_analysis(df):\n",
    "    lst = []\n",
    "    cols = []\n",
    "    total_rows = df.shape[0]\n",
    "    cols = df.columns[df.columns.str.contains('cumidx|cumsum|days_since_last_event|na_indictator|vtl_|notes_')]\n",
    "    for col in cols:\n",
    "        # Sum of NaN values in a column\n",
    "        na_values = max(df[col].eq(0).sum(), df[col].eq(9999).sum(), df[col].isnull().sum())\n",
    "        lst.extend([[col,total_rows,na_values,(na_values/total_rows)*100]])\n",
    "        if ((na_values/total_rows)*100) >= 99 and (col not in cols):\n",
    "            cols.append(col)\n",
    "\n",
    "    return lst\n",
    "\n",
    "\n",
    "df_na = pd.DataFrame(\n",
    "    na_analysis(base_df),\n",
    "    columns=['column_name','total_count','null_values','%_null_values']\n",
    ")\n",
    "\n",
    "df_na.sort_values(['%_null_values'],ascending=False,inplace=True)\n",
    "\n",
    "df_na.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(base_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "drop_cols = df_na[\n",
    "    (df_na['%_null_values'] >=99.9) & (~df_na['column_name'].str.startswith('hosp_target'))\n",
    "]['column_name']\n",
    "base_df.drop(drop_cols,\n",
    "        axis=1,\n",
    "        inplace=True\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_cumulative_group_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_group_count = get_feature_group_counts()\n",
    "cumulative_group_count = get_cumulative_group_counts()\n",
    "\n",
    "total_before_drop = 0\n",
    "total_after_drop = 0\n",
    "for grp in feature_groups:\n",
    "    feature_drop_stats[grp]['after_drop_count'] = feature_group_count[grp]\n",
    "    dropped_percentage = (feature_drop_stats[grp]['before_drop_count'] - feature_drop_stats[grp]['after_drop_count'])/feature_drop_stats[grp]['before_drop_count']\n",
    "    feature_drop_stats[grp]['dropped_percentage'] = \"{:.0%}\".format(dropped_percentage)\n",
    "    total_before_drop += feature_drop_stats[grp]['before_drop_count']\n",
    "    total_after_drop += feature_drop_stats[grp]['after_drop_count']\n",
    "dropped_percentage = (total_before_drop-total_after_drop)/total_before_drop\n",
    "feature_drop_stats['Total'] = {'before_drop_count': total_before_drop, 'after_drop_count': total_after_drop, 'dropped_percentage': \"{:.0%}\".format(dropped_percentage)}\n",
    "\n",
    "total_before_drop = 0\n",
    "total_after_drop = 0\n",
    "for grp in cumulative_group_count:\n",
    "    cumulative_feature_drop_stats[grp]['after_drop_count'] = cumulative_group_count[grp]\n",
    "    if cumulative_feature_drop_stats[grp]['before_drop_count'] > 0:\n",
    "        dropped_percentage = (cumulative_feature_drop_stats[grp]['before_drop_count'] - cumulative_feature_drop_stats[grp]['after_drop_count'])/cumulative_feature_drop_stats[grp]['before_drop_count']\n",
    "    else:\n",
    "        dropped_percentage = 0\n",
    "    cumulative_feature_drop_stats[grp]['dropped_percentage'] = \"{:.0%}\".format(dropped_percentage)\n",
    "    total_before_drop += cumulative_feature_drop_stats[grp]['before_drop_count']\n",
    "    total_after_drop += cumulative_feature_drop_stats[grp]['after_drop_count']\n",
    "if total_before_drop > 0:\n",
    "    dropped_percentage = (total_before_drop-total_after_drop)/total_before_drop\n",
    "else:\n",
    "    dropped_percentage = 0\n",
    "cumulative_feature_drop_stats['Total'] = {'before_drop_count': total_before_drop, 'after_drop_count': total_after_drop, 'dropped_percentage': \"{:.0%}\".format(dropped_percentage)}\n",
    "\n",
    "print(cumulative_feature_drop_stats)\n",
    "\n",
    "with open('./feature_drop_stats.json', 'w') as outfile: json.dump(feature_drop_stats, outfile)\n",
    "with open('./cumulative_feature_drop_stats.json', 'w') as outfile: json.dump(cumulative_feature_drop_stats, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write feature_drop_stats and cumulative_feature_drop_stats as ascii tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = PrettyTable()\n",
    "x.title = 'Feature Group Drop Stats'\n",
    "x.field_names = [\"Feature Group\", \"Before Feature Reduction\", \"After Feature Reduction\", \"% of Dropped Features\"]\n",
    "# To make sure the groups are in alphabetical order\n",
    "grps = list(feature_drop_stats.keys())\n",
    "total = grps.pop()\n",
    "grps = sorted(grps) + [total]\n",
    "for grp in grps:\n",
    "        x.add_row([grp, feature_drop_stats[grp]['before_drop_count'], feature_drop_stats[grp]['after_drop_count'], feature_drop_stats[grp]['dropped_percentage']])\n",
    "\n",
    "with open('./feature_group_drop_stats.txt', 'w') as w:\n",
    "    w.write(str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = PrettyTable()\n",
    "x.title = 'Feature Cumulative Group Drop Stats'\n",
    "x.field_names = [\"Feature Group\", \"Before Feature Reduction\", \"After Feature Reduction\", \"% of Dropped Features\"]\n",
    "for grp in cumulative_feature_drop_stats:\n",
    "        x.add_row([grp, cumulative_feature_drop_stats[grp]['before_drop_count'], cumulative_feature_drop_stats[grp]['after_drop_count'], cumulative_feature_drop_stats[grp]['dropped_percentage']])\n",
    "        \n",
    "with open('./feature_cumulative_drop_stats.txt', 'w') as w:\n",
    "    w.write(str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(base_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df.to_parquet(processed_path/'final_cleaned_df.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## =======================END====================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_df = pd.read_parquet(processed_path/'05-result.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif, mutual_info_regression\n",
    "from sklearn.feature_selection import SelectKBest, SelectPercentile\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import SelectKBest, SelectPercentile\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process the dataframe before running feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "output_cols = [col for col in base_df.columns if 'hosp_target' in col]\n",
    "x_df = base_df[base_df.columns.difference(output_cols)]\n",
    "y_df = base_df[output_cols]\n",
    "print(x_df.shape)\n",
    "print(y_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_cols = ['masterpatientid','censusdate', 'facilityid', 'bedid', 'client']\n",
    "\n",
    "x_df = x_df[x_df.columns.difference(exclude_cols)]\n",
    "x_df.shape\n",
    "\n",
    "y_df = y_df.fillna(False)\n",
    "y_df['hosp_target_3_day_hosp'] = y_df['hosp_target_3_day_hosp'].astype('float32')\n",
    "target_3_day = y_df['hosp_target_3_day_hosp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_na_train(df):\n",
    "    # Fill Median value for all NaN's in the respective columns\n",
    "    has_na = df.isna().sum() > 0\n",
    "    d = df.loc[:, has_na].median()\n",
    "    df = df.fillna(d)\n",
    "    \n",
    "    return df, d\n",
    "\n",
    "def fill_na_valid_or_test(df, na_filler):\n",
    "    return df.fillna(na_filler)\n",
    "\n",
    "\n",
    "x_df, na_filler = fill_na_train(x_df)\n",
    "x_df = x_df.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_df.shape)\n",
    "print(y_df.shape)\n",
    "print(len(target_3_day))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df.to_parquet(processed_path/'x_df.parquet')\n",
    "with open(processed_path/'target_3_day.pickle','wb') as f: pickle.dump(target_3_day, f, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df = pd.read_parquet(processed_path/'x_df.parquet')\n",
    "with open(processed_path/'target_3_day.pickle','rb') as f: target_3_day = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Correlation for all features with the target\n",
    "\n",
    "corr_matrix = x_df.corrwith(y_df['hosp_target_3_day_hosp'])\n",
    "\n",
    "_df = pd.DataFrame({'cols':corr_matrix.index, 'value':corr_matrix.values})\n",
    "_df.sort_values(by='value',ascending=False).head(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "## Remove constant features\n",
    "\n",
    "constant_features = []\n",
    "for feat in x_df.columns:\n",
    "    # convert all features to Float32\n",
    "    \n",
    "    if x_df[feat].std() == 0:\n",
    "        constant_features.append(feat)\n",
    "\n",
    "print(constant_features)\n",
    "\n",
    "# x_df.drop(labels=constant_features, axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Remove duplicated features\n",
    "\n",
    "duplicated_features = []\n",
    "for i in range(0, len(x_df.columns)):\n",
    "    col_1 = x_df.columns[i]\n",
    "\n",
    "    for col_2 in x_df.columns[i + 1:]:\n",
    "        if x_df[col_1].equals(x_df[col_2]):\n",
    "            duplicated_features.append(col_2)\n",
    "\n",
    "print(duplicated_features)\n",
    "\n",
    "# x_df.drop(labels=duplicated_features, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "# calculate the mutual information between the variables and the target\n",
    "# this returns the mutual information value of each feature.\n",
    "# the smaller the value the less information the feature has about the target\n",
    "\n",
    "\n",
    "mi = mutual_info_classif(x_df.fillna(0), target_3_day)\n",
    "print(mi)\n",
    "\n",
    "# let's add the variable names and order the features\n",
    "# according to the MI for clearer visualisation\n",
    "mi = pd.Series(mi)\n",
    "mi.index = x_df.columns\n",
    "mi = mi.sort_values(ascending=False)\n",
    "mi.to_csv('mi-date_cols.csv', header=True)\n",
    "# and now let's plot the ordered MI values per feature\n",
    "mi.sort_values(ascending=False).plot.bar(figsize=(20, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# here I will select the top 10 features\n",
    "# which are shown below\n",
    "sel_ = SelectKBest(mutual_info_classif, k=10).fit(x_df.fillna(0), target_3_day)\n",
    "x_df.columns[sel_.get_support()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# calculate the chi2 p_value between each of the variables\n",
    "# and the target\n",
    "# it returns 2 arrays, one contains the F-Scores which are then\n",
    "# evaluated against the chi2 distribution to obtain the pvalue\n",
    "# the pvalues are in the second array, see below\n",
    "\n",
    "f_score = chi2(x_df, target_3_day)\n",
    "f_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind, that contrarily to MI, where we were interested in the higher MI values,\n",
    "for Fisher score, the smaller the p_value, the more significant the feature is to predict the target.\n",
    "\n",
    "One thing to keep in mind when using Fisher score or univariate selection methods,\n",
    "is that in very big datasets, most of the features will show a small p_value,\n",
    "and therefore look like they are highly predictive.\n",
    "This is in fact an effect of the sample size. So care should be taken when selecting features\n",
    "using these procedures. An ultra tiny p_value does not highlight an ultra-important feature,\n",
    "it rather indicates that the dataset contains too many samples.\n",
    "\n",
    "If the dataset contained several categorical variables, we could then combine this procedure with\n",
    "SelectKBest or SelectPercentile, as I did in the previous lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# let's add the variable names and order it for clearer visualisation\n",
    "\n",
    "pvalues = pd.Series(f_score[1])\n",
    "pvalues.index = x_df.columns\n",
    "pvalues.sort_values(ascending=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# LASSO Regularization\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "logistic = LogisticRegression(C=1, penalty='l1',solver='liblinear',random_state=7).fit(x_df,target_3_day)\n",
    "model = SelectFromModel(logistic, prefit=True)\n",
    "\n",
    "# x_new_df = model.transform(x_df)\n",
    "\n",
    "# this command let's me visualise those features that were kept\n",
    "model.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Now I make a list with the selected features\n",
    "selected_feat = x_df.columns[(model.get_support())]\n",
    "\n",
    "print('total features: {}'.format((x_df.shape[1])))\n",
    "print('selected features: {}'.format(len(selected_feat)))\n",
    "print('features with coefficients shrank to zero: {}'.format(\n",
    "    np.sum(model.estimator_.coef_ == 0)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "\n",
    "# step forward feature selection\n",
    "# I indicate that I want to select 10 features from\n",
    "# the total, and that I want to select those features\n",
    "# based on the optimal roc_auc\n",
    "\n",
    "sfs1 = SFS(RandomForestRegressor(),\n",
    "           k_features=20,\n",
    "           forward=True,\n",
    "           floating=False,\n",
    "           verbose=2,\n",
    "           scoring='r2',\n",
    "           cv=3)\n",
    "\n",
    "sfs1 = sfs1.fit(np.array(x_df), target_3_day)\n",
    "selected_feat= x_df.columns[list(sfs1.k_feature_idx_)]\n",
    "selected_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mlxtend"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
