{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "Before starting the training:\n",
    "1. Configure `CLIENT` in `constants.py`\n",
    "2. Create or edit the client-specific file in `src/clients/`; use `_template.py` and follow the instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import os\n",
    "import glob\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from saiva.model.shared.load_raw_data import fetch_training_data, fetch_training_cache_data\n",
    "from saiva.model.shared.database import DbEngine\n",
    "from saiva.model.shared.constants import LOCAL_TRAINING_CONFIG_PATH\n",
    "from saiva.training.utils import load_config\n",
    "\n",
    "from eliot import to_file\n",
    "to_file(sys.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config(LOCAL_TRAINING_CONFIG_PATH)\n",
    "training_config = config.training_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TRAIN_START_DATE:\", training_config.training_metadata.experiment_dates.train_start_date)\n",
    "print(\"TEST_END_DATE:\", training_config.training_metadata.experiment_dates.test_end_date)\n",
    "print(\"CLIENTS:\", [organization_config.organization_id for organization_config in training_config.organization_configs_setup])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ======================== Load Database ========================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = DbEngine()\n",
    "saiva_engine = engine.get_postgresdb_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ======================== Fetch data for all organizations ============================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the data from SQL db and store them in local directory as cache\n",
    "\n",
    "missing_datasets = set()\n",
    "\n",
    "for organization_config in training_config.organization_configs:\n",
    "    client_sql_engine = engine.get_sqldb_engine(\n",
    "        db_name=organization_config.datasource.source_database_name,\n",
    "        credentials_secret_id=organization_config.datasource.source_database_credentials_secret_id,\n",
    "        query={\"driver\": \"ODBC Driver 17 for SQL Server\"}\n",
    "    )\n",
    "    \n",
    "    # verify connectivity\n",
    "    engine.verify_connectivity(client_sql_engine)\n",
    "\n",
    "    result_dict = fetch_training_data(\n",
    "        client=organization_config.organization_id, \n",
    "        client_sql_engine=client_sql_engine, \n",
    "        train_start_date=training_config.training_metadata.experiment_dates.train_start_date,\n",
    "        test_end_date=training_config.training_metadata.experiment_dates.test_end_date,\n",
    "    )\n",
    "\n",
    "    for dataset in training_config.all_datasets:\n",
    "        if result_dict.get(dataset, pd.DataFrame()).empty:\n",
    "            missing_datasets.add(dataset)\n",
    "            continue\n",
    "        print(dataset,result_dict[dataset].shape)\n",
    "\n",
    "    df = result_dict['patient_demographics']\n",
    "    df['dateofbirth'] = pd.to_datetime(df['dateofbirth'], errors='coerce')\n",
    "    df.to_parquet(f'/data/raw/{organization_config.organization_id}_patient_demographics.parquet' , index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_metadata = training_config.training_metadata\n",
    "training_metadata['missing_datasets'] = list(missing_datasets)\n",
    "\n",
    "print(training_metadata)\n",
    "\n",
    "conf = OmegaConf.create({'training_config': {'training_metadata': training_metadata}})\n",
    "OmegaConf.save(conf, f'{LOCAL_TRAINING_CONFIG_PATH}generated/training_metadata.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ==================== If Multiple clients data need to be merged ===================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the data from SQL db for multiple clints and store them in local directory as cache\n",
    "\n",
    "# for client in ['avante','gulfshore','palmgarden']:\n",
    "#     print(f'*********************** Processing for {client} ******************************')\n",
    "#     clientClass = get_client_class(client)\n",
    "#     EXPERIMENT_DATES = getattr(clientClass(), 'get_experiment_dates')()\n",
    "#     TRAIN_START_DATE, TEST_END_DATE = EXPERIMENT_DATES['train_start_date'], EXPERIMENT_DATES['test_end_date']\n",
    "#     print(TRAIN_START_DATE, TEST_END_DATE)\n",
    "    \n",
    "#     engine = DbEngine()\n",
    "#     saiva_engine = engine.get_postgresdb_engine()\n",
    "#     client_sql_engine = engine.get_sqldb_engine(clientdb_name=client)\n",
    "#     engine.verify_connectivity(client_sql_engine)\n",
    "#     result_dict = fetch_training_data(client, client_sql_engine, TRAIN_START_DATE, TEST_END_DATE)\n",
    "    \n",
    "#     print('master_patient_lookup', result_dict['master_patient_lookup'].shape)\n",
    "#     print('patient_census',result_dict['patient_census'].shape)\n",
    "#     print('patient_rehosps',result_dict['patient_rehosps'].shape)\n",
    "#     print('patient_demographics',result_dict['patient_demographics'].shape)\n",
    "#     print('patient_diagnosis',result_dict['patient_diagnosis'].shape)\n",
    "#     print('patient_vitals',result_dict['patient_vitals'].shape)\n",
    "#     print('patient_meds',result_dict['patient_meds'].shape)\n",
    "#     print('patient_orders',result_dict['patient_orders'].shape)\n",
    "#     print('patient_alerts',result_dict['patient_alerts'].shape)\n",
    "#     print('patient_progress_notes',result_dict['patient_progress_notes'].shape)\n",
    "#     if not result_dict.get('patient_lab_results', pd.DataFrame()).empty:\n",
    "#         print('patient_lab_results',result_dict['patient_lab_results'].shape)\n",
    "#     print(result_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ======================== TESTING =========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once fetch_training_data loads the data, use the same cache \n",
    "\n",
    "# result_dict = fetch_training_cache_data(CLIENT)\n",
    "\n",
    "# print('master_patient_lookup', result_dict['master_patient_lookup'].shape)\n",
    "# print('patient_census',result_dict['patient_census'].shape)\n",
    "# print('patient_rehosps',result_dict['patient_rehosps'].shape)\n",
    "# print('patient_demographics',result_dict['patient_demographics'].shape)\n",
    "# print('patient_diagnosis',result_dict['patient_diagnosis'].shape)\n",
    "# print('patient_vitals',result_dict['patient_vitals'].shape)\n",
    "# print('patient_meds',result_dict['patient_meds'].shape)\n",
    "# print('patient_orders',result_dict['patient_orders'].shape)\n",
    "# print('patient_alerts',result_dict['patient_alerts'].shape)\n",
    "# print('patient_progress_notes',result_dict['patient_progress_notes'].shape)\n",
    "# if not result_dict.get('patient_lab_results', pd.DataFrame()).empty:\n",
    "#     print('patient_lab_results',result_dict['patient_lab_results'].shape)\n",
    "# print(result_dict.keys())\n",
    "\n",
    "# have a max of 15042 master_patient_lookup rows ie. Infinity-Infinity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING specific queries\n",
    "\n",
    "# query=f\"\"\"\n",
    "#         select distinct patientid, facilityid, orderdate, gpiclass, \n",
    "#         gpisubclassdescription, orderdescription, pharmacymedicationname, a.PhysiciansOrderID\n",
    "#         from view_ods_physician_order_list_v2 a\n",
    "#         inner join view_ods_physician_order_list_med b\n",
    "#         on a.PhysiciansOrderID = b.PhysiciansOrderID \n",
    "#         where orderdate between '{train_start_date}' and '{test_end_date}';\n",
    "#         \"\"\"\n",
    "\n",
    "# df = pd.read_sql(query, con=client_sql_engine)\n",
    "# print(df.shape)\n",
    "# df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
