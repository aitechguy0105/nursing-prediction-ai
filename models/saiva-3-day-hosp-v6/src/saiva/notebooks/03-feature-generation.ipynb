{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from saiva.model.shared.demographics import DemographicFeatures\n",
    "from saiva.model.shared.labs import LabFeatures\n",
    "from saiva.model.shared.meds import MedFeatures\n",
    "from saiva.model.shared.orders import OrderFeatures\n",
    "from saiva.model.shared.vitals import VitalFeatures\n",
    "from saiva.model.shared.alerts import AlertFeatures\n",
    "from saiva.model.shared.rehosp import RehospFeatures\n",
    "from saiva.model.shared.notes import NoteFeatures\n",
    "from saiva.model.shared.diagnosis import DiagnosisFeatures\n",
    "from saiva.model.shared.patient_census import PatientCensus\n",
    "from saiva.model.shared.admissions import AdmissionFeatures\n",
    "from saiva.model.shared.immunizations import ImmunizationFeatures\n",
    "from saiva.model.shared.risks import RiskFeatures\n",
    "from saiva.model.shared.assessments import AssessmentFeatures\n",
    "from saiva.model.shared.adt import AdtFeatures\n",
    "from saiva.model.shared.mds import MDSFeatures\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from saiva.model.shared.load_raw_data import fetch_training_cache_data\n",
    "from saiva.model.shared.utils import get_client_class, get_memory_usage\n",
    "from eliot import start_action, start_task, to_file, log_message\n",
    "to_file(sys.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from saiva.model.shared.constants import saiva_api, LOCAL_TRAINING_CONFIG_PATH\n",
    "from saiva.training.utils import load_config\n",
    "\n",
    "config = load_config(LOCAL_TRAINING_CONFIG_PATH)\n",
    "training_config = config.training_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from local directory cache \n",
    "\n",
    "processed_path = Path('/data/processed')\n",
    "processed_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Replace this if necessary\n",
    "CLIENT = training_config.organization_configs[0].organization_id\n",
    "\n",
    "config = load_config(\"/src/saiva/conf/training/\")\n",
    "\n",
    "result_dict = fetch_training_cache_data(client=CLIENT, generic=True)\n",
    "for key, value in result_dict.items():\n",
    "    print(f'{key} : {result_dict[key].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = True\n",
    "\n",
    "TRAIN_START_DATE = training_config.training_metadata.experiment_dates.train_start_date\n",
    "TEST_END_DATE = training_config.training_metadata.experiment_dates.test_end_date\n",
    "\n",
    "model_version = saiva_api.model_types.get_by_model_type_id(model_type_id=training_config.model_type, version=training_config.model_version)\n",
    "\n",
    "print(TRAIN_START_DATE)\n",
    "print(TEST_END_DATE)\n",
    "print(training)\n",
    "print(model_version.model_type_id, model_version.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_metadata = training_config.training_metadata\n",
    "training_metadata['model_type_version_id'] = model_version.id\n",
    "\n",
    "print(training_metadata)\n",
    "\n",
    "conf = OmegaConf.create({'training_config': {'training_metadata': training_metadata}})\n",
    "OmegaConf.save(conf, f'{LOCAL_TRAINING_CONFIG_PATH}generated/training_metadata.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from parquet file\n",
    "# census_df = pd.read_parquet(processed_path/'census_df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print(TRAIN_START_DATE)\n",
    "\n",
    "patient_census = PatientCensus(\n",
    "            census_df=result_dict.get('patient_census', None),\n",
    "            train_start_date=TRAIN_START_DATE,\n",
    "            test_end_date=TEST_END_DATE,\n",
    "        )\n",
    "census_df = patient_census.generate_features()\n",
    "\n",
    "# Write to new parquet file\n",
    "census_df.to_parquet(processed_path/'census_df.parquet')\n",
    "\n",
    "print(census_df.shape)\n",
    "census_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_feature_group(feature_group_class, feature_group_kwargs, processed_file_path, to_print=False):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    feature_group = feature_group_class(**feature_group_kwargs)\n",
    "    df = feature_group.generate_features()\n",
    "\n",
    "    # Write to new parquet file\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        df = df[0]\n",
    "    df.to_parquet(processed_file_path)\n",
    "    duration = time.time() - start_time\n",
    "    if to_print:\n",
    "        print(df.shape)\n",
    "        display(df.head(3))\n",
    "        print(f'Wall time for {feature_group_class.__name__}: {time.strftime(\"%H:%M:%S\", time.gmtime(duration))}','\\n')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_groups_spec = [\n",
    "    (DemographicFeatures, {'demo_df': 'patient_demographics'}, {}, 'demo_df.parquet'),\n",
    "    (VitalFeatures, {'vitals': 'patient_vitals'}, {'config': config}, 'vitals_df.parquet'),\n",
    "    (OrderFeatures, {'orders': 'patient_orders'}, {'config': config}, 'orders_df.parquet'),\n",
    "    (MedFeatures, {'meds': 'patient_meds'}, {'config': config}, 'meds_df.parquet'),\n",
    "    (AlertFeatures, {'alerts': 'patient_alerts'}, {'config': config}, 'alerts_df.parquet'),\n",
    "    (LabFeatures, {'labs': 'patient_lab_results'}, {'config': config}, 'labs_df.parquet'),\n",
    "    (RehospFeatures, {'rehosps': 'patient_rehosps', 'adt_df': 'patient_adt'}, {'config': config, 'train_start_date': TRAIN_START_DATE}, 'rehosp_df.parquet'),\n",
    "    (AdmissionFeatures, {'admissions': 'patient_admissions'}, {}, 'admissions_df.parquet'),\n",
    "    (DiagnosisFeatures, {'diagnosis': 'patient_diagnosis'}, {'diagnosis_lookup_ccs_s3_file_path': model_version.diagnosis_lookup_ccs_s3_uri, 'config': config}, 'diagnosis_df.parquet'),\n",
    "    (NoteFeatures, {'notes': 'patient_progress_notes'}, {'client': CLIENT, 'vector_model': training_metadata.vector_model}, 'notes_df.parquet'),\n",
    "    (ImmunizationFeatures, {'immuns_df': 'patient_immunizations'}, {'config': config}, 'immuns_df.parquet'),\n",
    "    (RiskFeatures, {'risks_df': 'patient_risks'}, {'config': config}, 'risks_df.parquet'),\n",
    "    (AssessmentFeatures, {'assessments_df': 'patient_assessments'}, {'config': config}, 'assessments_df.parquet'),\n",
    "    (AdtFeatures, {'adt_df': 'patient_adt'}, {'config': config}, 'adt_df.parquet'),\n",
    "    #(MDSFeatures, {'mds_df': 'patient_mds', 'adt_df': 'patient_adt'}, {'config': config}, 'mds_df.parquet'), # switched off by default\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "for feature_group_class, result_keys, params, file_name in feature_groups_spec:\n",
    "    \n",
    "    if any([result_dict.get(result_key, pd.DataFrame()).empty for result_key in result_keys.values()]):\n",
    "        continue\n",
    "    \n",
    "    kwargs = {key: result_dict.get(value, pd.DataFrame()) for key, value in result_keys.items()}\n",
    "    kwargs.update(params)\n",
    "    kwargs['census_df'] = census_df.copy()\n",
    "    kwargs['training'] = training\n",
    "    generate_feature_group(\n",
    "        feature_group_class,\n",
    "        kwargs,\n",
    "        processed_path/file_name,\n",
    "        True\n",
    "    )\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
