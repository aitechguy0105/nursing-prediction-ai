{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing of data into form suitable for input into RNNs. \n",
    "\n",
    "The basic form is: \n",
    "\n",
    "* A dataset is a list of tuples, with one tuple per stay. \n",
    "* Each stay tuple has a target Y, X_codes, X_pn, X_emar, X_fixed, which are themselves lists whose length is the number of prediction times in the stay. \n",
    "* Each element of X_fixed is the prediction time's demographics, stay, and vitals features. \n",
    "* Each element of X_codes is the prediction time's bag of codes, as indices into an embedding matrix starting at 1 (the 0 position is reserved for masking).  \n",
    "* Each element of X_pn and X_emar respectively are the prediction time's bag of words, as indices into embedding matrices for progress notes and eMAR notes respectively, again starting at 1 with 0 reserved for masking. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import scipy\n",
    "import gc\n",
    "\n",
    "from collections import namedtuple\n",
    "from multiprocessing import Pool\n",
    "\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import FastText\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/code')\n",
    "from edge import data\n",
    "from edge import patient_stays\n",
    "from edge import diagnosis\n",
    "from edge import meds\n",
    "from edge import vitals\n",
    "from edge import utils\n",
    "from edge import stays\n",
    "from edge import demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dir to put things\n",
    "os.makedirs('/data/processed/rnn', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dirs for pytorch data loader infrastructure\n",
    "os.makedirs('/data/processed/rnn/data/train', exist_ok=True)\n",
    "os.makedirs('/data/processed/rnn/data/val', exist_ok=True)\n",
    "os.makedirs('/data/processed/rnn/data/test', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14694483, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ptimes = pd.read_parquet('/data/raw/combined_ptimes.parquet')\n",
    "ptimes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(118127, 19)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stays = pd.read_parquet('/data/raw/combined_stays.parquet')\n",
    "stays.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116667\n"
     ]
    }
   ],
   "source": [
    "used_stay_indices = ptimes.StayRowIndex.unique()\n",
    "stays = stays.iloc[used_stay_indices]\n",
    "print(len(stays))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll keep using this... \n",
    "grouped_ptimes = ptimes.groupby('StayRowIndex')\n",
    "\n",
    "JOBS_DATA = []\n",
    "for group_idx, group in grouped_ptimes: \n",
    "    # All we need are the indices into ptimes... \n",
    "    JOBS_DATA.append(group.index.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Targets - 4 day rehosp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.015513509389884625"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_flat = utils.get_rehosp_target(ptimes, stays, num_days=4)\n",
    "np.mean(Y_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116667"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _getTargetWorker(indices):\n",
    "    return Y_flat[indices]\n",
    "\n",
    "with Pool(os.cpu_count() - 4) as pool: \n",
    "    Y_list = pool.map(_getTargetWorker, JOBS_DATA)\n",
    "len(Y_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data/processed/rnn/target_4day_rehosp.pkl', 'wb') as f_out: \n",
    "    pkl.dump(Y_list, file=f_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixed length features - stays, demographics, and vitals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<14694483x7 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 59069800 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stay_features = scipy.sparse.load_npz('/data/processed/combined_stay_features_csr.npz')\n",
    "stay_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<14694483x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 29383026 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_features = scipy.sparse.load_npz('/data/processed/combined_demo_features_csr.npz')\n",
    "demo_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<14694483x72 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 164064183 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vitals_features = scipy.sparse.load_npz('/data/processed/combined_vitals_features_csr.npz')\n",
    "vitals_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<14694483x83 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 252517009 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fixed_length_features = scipy.sparse.hstack([stay_features, demo_features, vitals_features])\n",
    "fixed_length_features = scipy.sparse.csr_matrix(fixed_length_features)\n",
    "\n",
    "del stay_features\n",
    "del demo_features\n",
    "del vitals_features\n",
    "gc.collect()\n",
    "\n",
    "fixed_length_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116667"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _getFixedLengthFeatures(indices): \n",
    "    return fixed_length_features[indices].todense()\n",
    "\n",
    "with Pool(os.cpu_count() - 4) as pool: \n",
    "    X_fixed = pool.map(_getFixedLengthFeatures, JOBS_DATA)\n",
    "\n",
    "len(X_fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data/processed/rnn/X_fixed.pkl', 'wb') as f_out: \n",
    "    pkl.dump(X_fixed, file=f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del fixed_length_features\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract embedding matrix and mapping of codes to indices.  \n",
    "Remember to leave index 0 for masking. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.fasttext.FastText at 0x7f40a8fc9850>"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_model = FastText.load('/data/model/ft_combined_codes_d200.model')\n",
    "code_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4785"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_vectors = code_model.wv\n",
    "vocab = sorted(list(code_vectors.vocab.keys()))\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done...\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 200\n",
    "\n",
    "num_codes = len(vocab)\n",
    "embedding_matrix = np.zeros((num_codes+1, embedding_dim))\n",
    "code_to_index = {k:(i+1) for i, k in enumerate(vocab)}\n",
    "for code in vocab: \n",
    "    embedding_idx = code_to_index[code]\n",
    "    wv = code_vectors[code]\n",
    "    embedding_matrix[embedding_idx] = wv\n",
    "\n",
    "np.save('/data/processed/rnn/code_embeddings.npy', embedding_matrix)\n",
    "with open('/data/processed/rnn/code_to_index.pkl', 'wb') as f_out: \n",
    "    pkl.dump(code_to_index, file=f_out)\n",
    "print('Done...')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now build X_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done...\n"
     ]
    }
   ],
   "source": [
    "dx_features = scipy.sparse.load_npz('/data/processed/combined_dx_features_csr.npz')\n",
    "rx_features = scipy.sparse.load_npz('/data/processed/combined_rx_features_csr.npz')\n",
    "with open('/data/processed/combined_dx_colnames.pkl', 'rb') as f_in: \n",
    "    dx_names = pkl.load(f_in)\n",
    "with open('/data/processed/combined_rx_colnames.pkl', 'rb') as f_in: \n",
    "    rx_names = pkl.load(f_in)\n",
    "rx_names = np.array([s.replace(' ', '_') for s in rx_names])\n",
    "\n",
    "code_features = scipy.sparse.csr_matrix(scipy.sparse.hstack([dx_features, rx_features]))\n",
    "code_names = np.concatenate([dx_names, rx_names])\n",
    "    \n",
    "print('Done...')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _codeFeatureWorker(indices): \n",
    "    features = code_features[indices]\n",
    "    rows, cols = np.nonzero(features > 0)\n",
    "    retval = []\n",
    "    for i in indices: \n",
    "        retval.append([])\n",
    "    for i, j in zip(rows, cols): \n",
    "        code = code_names[j]\n",
    "        if code in code_to_index: \n",
    "            code_idx = code_to_index[code]\n",
    "            retval[i].append(code_idx)\n",
    "    return retval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting jobs...\n",
      "Done...\n"
     ]
    }
   ],
   "source": [
    "print('Starting jobs...')\n",
    "with Pool(os.cpu_count() - 4) as pool: \n",
    "    X_codes = pool.map(_codeFeatureWorker, JOBS_DATA)\n",
    "print('Done...')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data/processed/rnn/X_codes.pkl', 'wb') as f_out: \n",
    "    pkl.dump(X_codes, file=f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean up a bit... \n",
    "del code_features\n",
    "del dx_features\n",
    "del rx_features\n",
    "del code_model\n",
    "del code_vectors\n",
    "del embedding_matrix\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eMAR notes\n",
    "Similar recipe as for codes, but larger vocab of course. Have to deal with transforming note text too. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got vocab of 26846 words...\n",
      "Building embedding matrix...\n",
      "Saving embedding matrix and word to index map...\n",
      "Done...\n"
     ]
    }
   ],
   "source": [
    "# Load model, get vocab, map to indices, and save all\n",
    "EMBEDDING_DIM = 200\n",
    "\n",
    "emar_model = FastText.load('/data/model/ft_combined_emar_notes_d200.model')\n",
    "word_vectors = emar_model.wv\n",
    "vocab = list(word_vectors.vocab.keys())\n",
    "print(f\"Got vocab of {len(vocab)} words...\")\n",
    "\n",
    "print(\"Building embedding matrix...\")\n",
    "num_words = len(vocab)\n",
    "embedding_matrix = np.zeros((num_words+1, EMBEDDING_DIM))\n",
    "word_to_index = {k:(i+1) for i, k in enumerate(vocab)}\n",
    "for word in vocab: \n",
    "    embedding_idx = word_to_index[word]\n",
    "    wv = word_vectors[word]\n",
    "    embedding_matrix[embedding_idx] = word_vectors[word]\n",
    "\n",
    "print(\"Saving embedding matrix and word to index map...\")\n",
    "np.save('/data/processed/rnn/emar_word_embeddings.npy', embedding_matrix)\n",
    "with open('/data/processed/rnn/emar_word_to_index.pkl', 'wb') as f_out: \n",
    "    pkl.dump(word_to_index, file=f_out)    \n",
    "    \n",
    "print('Done...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now build X_emar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done...\n"
     ]
    }
   ],
   "source": [
    "emar_notes = pd.read_parquet('/data/raw/combined_emar_notes.parquet')\n",
    "emar_notes['Date'] = emar_notes.CreatedDate.dt.date\n",
    "notes_subset = emar_notes[['MasterPatientID', 'FacilityID', 'ProgressNoteID', 'Date', 'NoteText']]\n",
    "grp_cols = ['MasterPatientID', 'Date']\n",
    "notes_merged_by_day = notes_subset.groupby(grp_cols).agg({'NoteText': lambda x: ' '.join(x)})\n",
    "print('Done...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14694483, 5), (14694483, 4))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ptimes_subset = ptimes[['MasterPatientID', 'FacilityID', 'StayRowIndex']].copy()\n",
    "ptimes_subset['Date'] = (ptimes.PredictionTimestamp - pd.to_timedelta('1 day')).dt.date\n",
    "\n",
    "merged_notes = ptimes_subset.merge(notes_merged_by_day, \n",
    "                                   how='left', \n",
    "                                   left_on=['MasterPatientID', 'Date'], \n",
    "                                   right_on=['MasterPatientID', 'Date'])\n",
    "\n",
    "merged_notes['NoteText'] = merged_notes.NoteText.fillna('')\n",
    "\n",
    "merged_notes.shape, ptimes_subset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _notesWorker(indices): \n",
    "    note_texts = merged_notes.iloc[indices].NoteText.values\n",
    "    retval = []\n",
    "    for text in note_texts: \n",
    "        tokens = simple_preprocess(text)\n",
    "        word_indices = [word_to_index[w] for w in tokens if w in word_to_index]\n",
    "        # NB - we only use unique word indices...\n",
    "        retval.append(list(set(word_indices)))\n",
    "    return retval\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(116667, 20)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with Pool(os.cpu_count() - 4) as pool: \n",
    "    X_emar = pool.map(_notesWorker, JOBS_DATA)\n",
    "len(X_emar), len(X_emar[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done...\n"
     ]
    }
   ],
   "source": [
    "with open('/data/processed/rnn/X_emar.pkl', 'wb') as f_out: \n",
    "    pkl.dump(X_emar, file=f_out)\n",
    "print('Done...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Progress Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got vocab of 97810 words...\n",
      "Building embedding matrix...\n",
      "Saving embedding matrix and word to index map...\n",
      "Done...\n"
     ]
    }
   ],
   "source": [
    "# Load model, get vocab, map to indices, and save all\n",
    "EMBEDDING_DIM = 200\n",
    "\n",
    "pn_model = FastText.load('/data/model/ft_combined_progress_notes_d200.model')\n",
    "word_vectors = pn_model.wv\n",
    "vocab = list(word_vectors.vocab.keys())\n",
    "print(f\"Got vocab of {len(vocab)} words...\")\n",
    "\n",
    "print(\"Building embedding matrix...\")\n",
    "num_words = len(vocab)\n",
    "embedding_matrix = np.zeros((num_words+1, EMBEDDING_DIM))\n",
    "word_to_index = {k:(i+1) for i, k in enumerate(vocab)}\n",
    "for word in vocab: \n",
    "    embedding_idx = word_to_index[word]\n",
    "    wv = word_vectors[word]\n",
    "    embedding_matrix[embedding_idx] = word_vectors[word]\n",
    "\n",
    "print(\"Saving embedding matrix and word to index map...\")\n",
    "np.save('/data/processed/rnn/pn_word_embeddings.npy', embedding_matrix)\n",
    "with open('/data/processed/rnn/pn_word_to_index.pkl', 'wb') as f_out: \n",
    "    pkl.dump(word_to_index, file=f_out)    \n",
    "    \n",
    "print('Done...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now build X_pn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done...\n"
     ]
    }
   ],
   "source": [
    "# Prepare progress notes by merging notes on same day for same patient... \n",
    "progress_notes = pd.read_parquet('/data/raw/combined_progress_notes.parquet')\n",
    "progress_notes['Date'] = progress_notes.CreatedDate.dt.date\n",
    "notes_subset = progress_notes[['MasterPatientID', 'FacilityID', 'ProgressNoteID', 'Date', 'NoteText']]\n",
    "grp_cols = ['MasterPatientID', 'Date']\n",
    "notes_merged_by_day = notes_subset.groupby(grp_cols).agg({'NoteText': lambda x: ' '.join(x)})\n",
    "print('Done...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14694483, 5), (14694483, 4))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ptimes_subset = ptimes[['MasterPatientID', 'FacilityID', 'StayRowIndex']].copy()\n",
    "ptimes_subset['Date'] = (ptimes.PredictionTimestamp - pd.to_timedelta('1 day')).dt.date\n",
    "\n",
    "merged_notes = ptimes_subset.merge(notes_merged_by_day, \n",
    "                                   how='left', \n",
    "                                   left_on=['MasterPatientID', 'Date'], \n",
    "                                   right_on=['MasterPatientID', 'Date'])\n",
    "merged_notes['NoteText'] = merged_notes.NoteText.fillna('')\n",
    "merged_notes.shape, ptimes_subset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _notesWorker(indices): \n",
    "    note_texts = merged_notes.iloc[indices].NoteText.values\n",
    "    retval = []\n",
    "    for text in note_texts: \n",
    "        tokens = simple_preprocess(text)\n",
    "        word_indices = [word_to_index[w] for w in tokens if w in word_to_index ]\n",
    "        # NB - we only use unique word indices...\n",
    "        retval.append(list(set(word_indices)))\n",
    "    return retval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(116667, 20)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with Pool(os.cpu_count() - 4) as pool: \n",
    "    X_pn = pool.map(_notesWorker, JOBS_DATA)\n",
    "len(X_pn), len(X_pn[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done...\n"
     ]
    }
   ],
   "source": [
    "with open('/data/processed/rnn/X_pn.pkl', 'wb') as f_out: \n",
    "    pkl.dump(X_pn, file=f_out)\n",
    "print('Done...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del merged_notes\n",
    "del ptimes_subset\n",
    "del progress_notes\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assemble tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(116667, 116667, 116667, 116667)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load raw data... \n",
    "with open('/data/processed/rnn/X_pn.pkl', 'rb') as f_in: \n",
    "    X_pn = pkl.load(f_in)\n",
    "with open('/data/processed/rnn/X_emar.pkl', 'rb') as f_in: \n",
    "    X_emar = pkl.load(f_in)\n",
    "with open('/data/processed/rnn/X_codes.pkl', 'rb') as f_in: \n",
    "    X_codes = pkl.load(f_in)\n",
    "with open('/data/processed/rnn/X_fixed.pkl', 'rb') as f_in: \n",
    "    X_fixed = pkl.load(f_in)\n",
    "len(X_pn), len(X_emar), len(X_codes), len(X_fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116667"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('/data/processed/rnn/target_4day_rehosp.pkl', 'rb') as f_in: \n",
    "    Y = pkl.load(f_in)\n",
    "len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116667"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ALL_DATA = []\n",
    "for Y_for_stay, X_for_stay in zip(Y, zip(X_fixed, X_codes, X_emar, X_pn)): \n",
    "    ALL_DATA.append((Y_for_stay, X_for_stay))\n",
    "len(ALL_DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splits\n",
    "Also, make tiny datasets for testing code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using min date 2017-04-01 00:00:00\n",
      "Keeping 94126 stays out of 116667\n",
      "Keeping 94126 stays\n",
      "94126\n"
     ]
    }
   ],
   "source": [
    "# Deal with left and right censoring. \n",
    "# Get rid of stays that start within 90 days of '2017-01-01'\n",
    "MIN_DATE = pd.Timestamp('2017-01-01') + pd.to_timedelta('90 days')\n",
    "print(f\"Using min date {MIN_DATE}\")\n",
    "\n",
    "left_mask = (stays.StartDate >= MIN_DATE).values\n",
    "print(f\"Keeping {np.sum(left_mask)} stays out of {len(stays)}\")\n",
    "\n",
    "indices_to_keep = np.nonzero(left_mask)[0]\n",
    "stays_to_use = stays.iloc[indices_to_keep]\n",
    "print(f\"Keeping {len(stays_to_use)} stays\")\n",
    "\n",
    "DATA = [ALL_DATA[idx] for idx in indices_to_keep]\n",
    "print(len(DATA))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deal with right censoring\n",
    "We should get rid of prediction times that are within 4 days of '2020-02-28', and that do not end in a rehosp event, but this is a bit complicated so defer until later.  This approximation should be fine.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78964 7371 7791\n"
     ]
    }
   ],
   "source": [
    "# train_start_date = '2017-01-01'\n",
    "# train_end_date = '2019-07-31'\n",
    "# val_end_date = '2019-11-30'\n",
    "# test_end_date = '2020-02-28'\n",
    "\n",
    "train_end = pd.Timestamp('2019-08-31')\n",
    "val_end = pd.Timestamp('2019-11-30')\n",
    "\n",
    "train_indices = np.nonzero((stays_to_use.StartDate <= train_end).values)[0]\n",
    "val_indices = np.nonzero((stays_to_use.StartDate > train_end).values & (stays_to_use.StartDate <= val_end).values)[0]\n",
    "test_indices = np.nonzero((stays_to_use.StartDate > val_end).values)[0]\n",
    "\n",
    "print(len(train_indices), len(val_indices), len(test_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78964 7371 7791\n"
     ]
    }
   ],
   "source": [
    "TRAIN_DATA = [DATA[idx] for idx in train_indices]\n",
    "VAL_DATA = [DATA[idx] for idx in val_indices]\n",
    "TEST_DATA = [DATA[idx] for idx in test_indices]\n",
    "print(len(TRAIN_DATA), len(VAL_DATA), len(TEST_DATA))\n",
    "\n",
    "with open('/data/processed/rnn/train_data.pkl', 'wb') as f_out: \n",
    "    pkl.dump(TRAIN_DATA, file=f_out)\n",
    "with open('/data/processed/rnn/val_data.pkl', 'wb') as f_out: \n",
    "    pkl.dump(VAL_DATA, file=f_out)\n",
    "with open('/data/processed/rnn/test_data.pkl', 'wb') as f_out: \n",
    "    pkl.dump(TEST_DATA, file=f_out)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-31 2019-09-01 2019-11-30 2019-12-01\n",
      "78964 7371 7791\n"
     ]
    }
   ],
   "source": [
    "# Save stays for splits too... \n",
    "train_stays = stays_to_use.iloc[train_indices]\n",
    "val_stays = stays_to_use.iloc[val_indices]\n",
    "test_stays = stays_to_use.iloc[test_indices]\n",
    "print(train_stays.StartDate.max(), val_stays.StartDate.min(), val_stays.StartDate.max(), test_stays.StartDate.min())\n",
    "\n",
    "with open('/data/processed/rnn/train_stays.pkl', 'wb') as f_out: \n",
    "    pkl.dump(train_stays, file=f_out)\n",
    "with open('/data/processed/rnn/val_stays.pkl', 'wb') as f_out: \n",
    "    pkl.dump(val_stays, file=f_out)\n",
    "with open('/data/processed/rnn/test_stays.pkl', 'wb') as f_out: \n",
    "    pkl.dump(test_stays, file=f_out)\n",
    "    \n",
    "print(len(train_stays), len(val_stays), len(test_stays))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "# Also, get a tiny dataset for testing... \n",
    "N_tiny = 100\n",
    "indices = np.random.choice(len(TRAIN_DATA), N_tiny, replace=False)\n",
    "tiny_train = [TRAIN_DATA[i] for i in indices]\n",
    "with open('/data/processed/rnn/tiny_train.pkl', 'wb') as f_out: \n",
    "    pkl.dump(tiny_train, file=f_out)\n",
    "print(len(tiny_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch DataLoader support\n",
    "Since it provides out of box support for threading, etc, write things out like this was an image problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def prepare_pytorch_files(split='train'): \n",
    "    print(\"Processing data...\")\n",
    "    filepath = f'/data/processed/rnn/{split}_data.pkl'\n",
    "    with open(filepath, 'rb') as f_in: \n",
    "        data = pkl.load(f_in)\n",
    "    for i, d_i in enumerate(data): \n",
    "        if (i+1) % 1000 == 0: \n",
    "            print(f\"Working on {i+1} / {len(data)}\")\n",
    "        Y, X = d_i\n",
    "        X_fixed, X_codes, X_emar, X_pn = X\n",
    "        X_fixed = np.array(X_fixed)\n",
    "        X_codes = np.array([codes if len(codes) > 0 else [0] for codes in X_codes])\n",
    "        X_emar = np.array([words if len(words) > 0 else [0] for words in X_emar])\n",
    "        X_pn = np.array([words if len(words) > 0 else [0] for words in X_pn])\n",
    "        dest_path = f'/data/processed/rnn/data/{split}/{i}.npz'\n",
    "        np.savez(dest_path, \n",
    "                 Y=Y, \n",
    "                 X_fixed=X_fixed, \n",
    "                 X_codes=X_codes, \n",
    "                 X_emar=X_emar, \n",
    "                 X_pn=X_pn)\n",
    "    \n",
    "    print(\"Copying stays file...\")\n",
    "    stays_path = f'/data/processed/rnn/{split}_stays.pkl'\n",
    "    new_stays_path = f'/data/processed/rnn/data/{split}/stays.pkl'\n",
    "    shutil.copyfile(stays_path, new_stays_path)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data...\n",
      "Working on 1000 / 78964\n",
      "Working on 2000 / 78964\n",
      "Working on 3000 / 78964\n",
      "Working on 4000 / 78964\n",
      "Working on 5000 / 78964\n",
      "Working on 6000 / 78964\n",
      "Working on 7000 / 78964\n",
      "Working on 8000 / 78964\n",
      "Working on 9000 / 78964\n",
      "Working on 10000 / 78964\n",
      "Working on 11000 / 78964\n",
      "Working on 12000 / 78964\n",
      "Working on 13000 / 78964\n",
      "Working on 14000 / 78964\n",
      "Working on 15000 / 78964\n",
      "Working on 16000 / 78964\n",
      "Working on 17000 / 78964\n",
      "Working on 18000 / 78964\n",
      "Working on 19000 / 78964\n",
      "Working on 20000 / 78964\n",
      "Working on 21000 / 78964\n",
      "Working on 22000 / 78964\n",
      "Working on 23000 / 78964\n",
      "Working on 24000 / 78964\n",
      "Working on 25000 / 78964\n",
      "Working on 26000 / 78964\n",
      "Working on 27000 / 78964\n",
      "Working on 28000 / 78964\n",
      "Working on 29000 / 78964\n",
      "Working on 30000 / 78964\n",
      "Working on 31000 / 78964\n",
      "Working on 32000 / 78964\n",
      "Working on 33000 / 78964\n",
      "Working on 34000 / 78964\n",
      "Working on 35000 / 78964\n",
      "Working on 36000 / 78964\n",
      "Working on 37000 / 78964\n",
      "Working on 38000 / 78964\n",
      "Working on 39000 / 78964\n",
      "Working on 40000 / 78964\n",
      "Working on 41000 / 78964\n",
      "Working on 42000 / 78964\n",
      "Working on 43000 / 78964\n",
      "Working on 44000 / 78964\n",
      "Working on 45000 / 78964\n",
      "Working on 46000 / 78964\n",
      "Working on 47000 / 78964\n",
      "Working on 48000 / 78964\n",
      "Working on 49000 / 78964\n",
      "Working on 50000 / 78964\n",
      "Working on 51000 / 78964\n",
      "Working on 52000 / 78964\n",
      "Working on 53000 / 78964\n",
      "Working on 54000 / 78964\n",
      "Working on 55000 / 78964\n",
      "Working on 56000 / 78964\n",
      "Working on 57000 / 78964\n",
      "Working on 58000 / 78964\n",
      "Working on 59000 / 78964\n",
      "Working on 60000 / 78964\n",
      "Working on 61000 / 78964\n",
      "Working on 62000 / 78964\n",
      "Working on 63000 / 78964\n",
      "Working on 64000 / 78964\n",
      "Working on 65000 / 78964\n",
      "Working on 66000 / 78964\n",
      "Working on 67000 / 78964\n",
      "Working on 68000 / 78964\n",
      "Working on 69000 / 78964\n",
      "Working on 70000 / 78964\n",
      "Working on 71000 / 78964\n",
      "Working on 72000 / 78964\n",
      "Working on 73000 / 78964\n",
      "Working on 74000 / 78964\n",
      "Working on 75000 / 78964\n",
      "Working on 76000 / 78964\n",
      "Working on 77000 / 78964\n",
      "Working on 78000 / 78964\n",
      "Copying stays file...\n",
      "Processing data...\n",
      "Working on 1000 / 7371\n",
      "Working on 2000 / 7371\n",
      "Working on 3000 / 7371\n",
      "Working on 4000 / 7371\n",
      "Working on 5000 / 7371\n",
      "Working on 6000 / 7371\n",
      "Working on 7000 / 7371\n",
      "Copying stays file...\n",
      "Processing data...\n",
      "Working on 1000 / 7791\n",
      "Working on 2000 / 7791\n",
      "Working on 3000 / 7791\n",
      "Working on 4000 / 7791\n",
      "Working on 5000 / 7791\n",
      "Working on 6000 / 7791\n",
      "Working on 7000 / 7791\n",
      "Copying stays file...\n"
     ]
    }
   ],
   "source": [
    "prepare_pytorch_files(split='train')\n",
    "prepare_pytorch_files(split='val')\n",
    "prepare_pytorch_files(split='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Dataset experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from edge.dnn import rnn_dataset\n",
    "\n",
    "val_dataset = rnn_dataset.SaivaDataset('/data/processed/rnn/data/val', max_length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y, (X_fixed, X_codes, X_emar, X_pn) = val_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14, 83])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_fixed.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([280]), torch.Size([14]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_codes_inputs, X_codes_offsets = X_codes\n",
    "X_codes_inputs.size(), X_codes_offsets.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collating function for DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y0, X0 = val_dataset[0]\n",
    "Y1, X1 = val_dataset[1]\n",
    "Y2, X2 = val_dataset[2]\n",
    "batch = [(Y0, X0), (Y1, X1), (Y2, X2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from edge import dnn \n",
    "\n",
    "Y, X_fixed, X_codes, X_emar, X_pn, sizes = dnn.rnn_dataset.data_loader_collate_fn(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([14, 13, 50], torch.Size([50, 3]), torch.Size([50, 3, 83]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sizes, Y.size(), X_fixed.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_fixed[:,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "batch_size = 4\n",
    "val_data = torch.utils.data.DataLoader(val_dataset, \n",
    "                                       batch_size=batch_size, \n",
    "                                       num_workers=4, \n",
    "                                       collate_fn=dnn.rnn_dataset.data_loader_collate_fn)\n",
    "for batch_i, (Y, X_fixed, X_codes, X_emar, X_pn, sizes) in enumerate(val_data): \n",
    "    if batch_i == 5: \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50, 50, 41, 50]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fixed_size = X_fixed.size()[-1]\n",
    "fixed_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scratch pad for testing model architecture, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BasicModel(\n",
       "  (code_embedding_bag): EmbeddingBag(10000, 200, mode=mean)\n",
       "  (emar_embedding_bag): EmbeddingBag(50000, 200, mode=mean)\n",
       "  (pn_embedding_bag): EmbeddingBag(100000, 200, mode=mean)\n",
       "  (rnn): GRU(683, 128, num_layers=2)\n",
       "  (dense_1): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (out_prob): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from edge.dnn.basic_model import BasicModel\n",
    "\n",
    "model = BasicModel(fixed_size=fixed_size, code_size=10000, emar_size=50000, pn_size=100000, embed_dim=200, rnn_dim=128)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got max length = 50 from sizes...\n",
      "code embeddings have size torch.Size([200, 200])\n",
      "new code embeddings have size torch.Size([50, 4, 200])\n",
      "emar embeddings have size torch.Size([200, 200])\n",
      "new emar embeddings have size torch.Size([50, 4, 200])\n",
      "pn embeddings have size torch.Size([200, 200])\n",
      "new pn embeddings have size torch.Size([50, 4, 200])\n",
      "Combined inputs shape: torch.Size([50, 4, 683])\n",
      "rnn output shape: torch.Size([50, 4, 128]), hidden shape: torch.Size([2, 4, 128])\n",
      "logits has shape torch.Size([50, 4])\n"
     ]
    }
   ],
   "source": [
    "dense_out, probs = model(X_fixed, X_codes, X_emar, X_pn, sizes, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> Output sizes! <<<"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50, 4]), torch.Size([50, 4]), torch.Size([50, 4]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_out.size(), probs.size(), Y.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test a training step... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try training for 3 steps... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _send_to_device(device, Y, X_fixed, X_codes, X_emar, X_pn): \n",
    "    Y.to(device)\n",
    "    X_fixed.to(device)\n",
    "    X_fixed.to(device)\n",
    "    X_codes[0].to(device)\n",
    "    X_codes[1].to(device)\n",
    "    X_emar[0].to(device)\n",
    "    X_emar[1].to(device)\n",
    "    X_pn[0].to(device)\n",
    "    X_pn[1].to(device)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from edge.dnn import rnn_dataset\n",
    "from edge.dnn.basic_model import BasicModel\n",
    "from edge.dnn import utils\n",
    "\n",
    "# Set seed! \n",
    "#torch.manual_seed(1)\n",
    "\n",
    "# These should be from cmd line... \n",
    "max_length = 100  # Defined by dataset, etc... \n",
    "data_root_dir = '/data/processed/rnn/data'\n",
    "batch_size = 32\n",
    "num_data_workers = 4\n",
    "lr = 0.001\n",
    "num_epochs = 50\n",
    "#num_epochs = 3\n",
    "\n",
    "# Set up datasets and loaders.  \n",
    "train_dataset = rnn_dataset.SaivaDataset(os.path.join(data_root_dir, 'train'), \n",
    "                                         max_length=max_length, \n",
    "                                         max_num=2000)\n",
    "val_dataset = rnn_dataset.SaivaDataset(os.path.join(data_root_dir, 'val'), \n",
    "                                       max_length=max_length, \n",
    "                                       max_num=500)\n",
    "test_dataset = rnn_dataset.SaivaDataset(os.path.join(data_root_dir, 'test'),\n",
    "                                        max_length=max_length)\n",
    "\n",
    "train_data = torch.utils.data.DataLoader(train_dataset, \n",
    "                                         batch_size=batch_size, \n",
    "                                         num_workers=num_data_workers, \n",
    "                                         collate_fn=rnn_dataset.data_loader_collate_fn)\n",
    "val_data = torch.utils.data.DataLoader(val_dataset, \n",
    "                                       batch_size=batch_size, \n",
    "                                       num_workers=num_data_workers, \n",
    "                                       collate_fn=rnn_dataset.data_loader_collate_fn)\n",
    "test_data = torch.utils.data.DataLoader(test_dataset, \n",
    "                                        batch_size=batch_size, \n",
    "                                        num_workers=num_data_workers, \n",
    "                                        collate_fn=rnn_dataset.data_loader_collate_fn)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device {device}\")\n",
    "\n",
    "fixed_size = 83\n",
    "embeddings_dir = '/data/processed/rnn'\n",
    "\n",
    "codes_file = os.path.join(embeddings_dir, 'code_embeddings.npy')\n",
    "emar_file = os.path.join(embeddings_dir, 'emar_word_embeddings.npy')\n",
    "pn_file = os.path.join(embeddings_dir, 'pn_word_embeddings.npy')\n",
    "model = BasicModel(fixed_size=fixed_size, \n",
    "                   codes_file=codes_file, \n",
    "                   emar_file=emar_file, \n",
    "                   pn_file=pn_file, \n",
    "                   rnn_dim=200)\n",
    "model.to(device)\n",
    "\n",
    "# Set up objective function and optimizer. \n",
    "pos_weight = torch.Tensor([5.])\n",
    "criterion = torch.nn.BCEWithLogitsLoss(reduction='none', pos_weight=pos_weight) # Other options available...\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, criterion, data, \n",
    "                device, print_every=10, max_iter=100): \n",
    "    \n",
    "    total_num_iter = 0\n",
    "    cumulative_loss = 0.    \n",
    "\n",
    "    model.train()\n",
    "    with torch.set_grad_enabled(True): \n",
    "        for batch_i, (Y, X_fixed, X_codes, X_emar, X_pn, sizes) in enumerate(data): \n",
    "            _send_to_device(device, Y, X_fixed, X_codes, X_emar, X_pn)\n",
    "            batch_size = Y.size()[1]\n",
    "            num_days = sum(sizes) \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            mask = utils.get_mask_from_sizes(sizes)\n",
    "            mask.to(device)  # Mask needs to be on device too I think?  \n",
    "\n",
    "            logits, probs = model(X_fixed, X_codes, X_emar, X_pn, sizes, batch_size)\n",
    "            loss = criterion(logits, Y)  # Loss is a Tensor of size (max_length, batch_size)...  \n",
    "            masked_loss = loss * mask  # This is why mask should be on device I think?  \n",
    "            scalar_loss = torch.sum(masked_loss) / torch.sum(mask)\n",
    "            scalar_loss.backward()     # Back propagate gradients. \n",
    "            optimizer.step()    # Take the step...      \n",
    "\n",
    "            cumulative_loss += scalar_loss.item() \n",
    "            total_num_iter += 1\n",
    "\n",
    "            if (batch_i+1) % print_every == 0: \n",
    "                loss_right_now = cumulative_loss / total_num_days\n",
    "                print(f\"On iteration {batch_i+1} => {loss_right_now:.4f}\")\n",
    "\n",
    "    \n",
    "    return cumulative_loss / total_num_iter\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns log loss, auroc, ap...  \n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "def evaluate_model(model, criterion, data, device): \n",
    "\n",
    "    Y_all = None\n",
    "    Yhat_all = None\n",
    "    total_num_iter = 0\n",
    "    cumulative_loss = 0.  \n",
    "\n",
    "    model.eval()\n",
    "    with torch.set_grad_enabled(False): \n",
    "        for batch_i, (Y, X_fixed, X_codes, X_emar, X_pn, sizes) in enumerate(data): \n",
    "            _send_to_device(device, Y, X_fixed, X_codes, X_emar, X_pn)\n",
    "            print(Y.device)\n",
    "            batch_size = Y.size()[1] # dims are time x batch\n",
    "            num_days = sum(sizes)\n",
    "            \n",
    "            mask = utils.get_mask_from_sizes(sizes)\n",
    "            mask.to(device)  # Mask needs to be on device too I think?              \n",
    "            \n",
    "            logits, probs = model(X_fixed, X_codes, X_emar, X_pn, sizes, batch_size)\n",
    "            loss = criterion(logits, Y) \n",
    "            masked_loss = loss * mask  \n",
    "            scalar_loss = torch.sum(masked_loss) / torch.sum(mask)            \n",
    "\n",
    "            # Accumulate loss\n",
    "            cumulative_loss += scalar_loss.item() \n",
    "            total_num_iter += 1\n",
    "            \n",
    "            # Accumulate Y and Yhat given sizes (i.e., disregarding mask...)\n",
    "            Y_batch = Y.detach().cpu().numpy()\n",
    "            Yhat_batch = probs.detach().cpu().numpy()\n",
    "\n",
    "            if Y_all is None: \n",
    "                Y_list, Yhat_list = [], []\n",
    "            else: \n",
    "                Y_list, Yhat_list = [Y_all], [Yhat_all]\n",
    "                \n",
    "            for idx in range(Y_batch.shape[1]): \n",
    "                Y_list.append(Y_batch[:sizes[idx], idx])\n",
    "                Yhat_list.append(Yhat_batch[:sizes[idx], idx])\n",
    "            Y_all = np.concatenate(Y_list)\n",
    "            Yhat_all = np.concatenate(Yhat_list)\n",
    "\n",
    "    data_loss = cumulative_loss / total_num_iter\n",
    "    auroc = roc_auc_score(Y_all, Yhat_all)\n",
    "    ap = average_precision_score(Y_all, Yhat_all)\n",
    "    return data_loss, auroc, ap, Y_all, Yhat_all\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_i = 0\n",
    "best_weights = None\n",
    "which_metric = 'roc'\n",
    "best_metric = np.Inf if which_metric == 'loss' else -np.Inf\n",
    "patience = 5  # Run this many epochs with no improvement in val loss\n",
    "epochs_since_improvement = 0\n",
    "which_metric = 'roc'\n",
    "\n",
    "def compare_stats(stats, which_metric, best_metric): \n",
    "    if which_metric == 'loss': \n",
    "        return stats[which_metric] < best_metric\n",
    "    else: \n",
    "        return stats[which_metric] > best_metric\n",
    "    \n",
    "for epoch_i in range(num_epochs): \n",
    "    \n",
    "    train_loss = train_epoch(model, optimizer, criterion, train_data, device, print_every=1000)\n",
    "    print(f\"Epoch {epoch_i+1} training loss: {train_loss:.4f}\")\n",
    "    \n",
    "    # Evaluate and collect evaluation metrics.\n",
    "#    val_loss, val_auroc, val_ap, Y, Yhat = evaluate_model(model, criterion, val_data, device)\n",
    "    val_loss, val_auroc, val_ap, Y, Yhat = evaluate_model(model, criterion, train_data, device)\n",
    "    \n",
    "    stats = {'loss': val_loss, \n",
    "             'roc': val_auroc, \n",
    "             'ap': val_ap}\n",
    "    print(f\"Epoch {epoch_i+1} val loss: {val_loss:.4f} AUROC: {val_auroc:.4f} aP: {val_ap:.4f}\")\n",
    "\n",
    "    if compare_stats(stats, which_metric, best_metric): \n",
    "        best_metric = stats[which_metric]\n",
    "        best_i = epoch_i + 1\n",
    "        best_weights = copy.deepcopy(model.state_dict())\n",
    "        epochs_since_improvement = 0\n",
    "    else: \n",
    "        epochs_since_improvement += 1\n",
    "    if epochs_since_improvement == patience: \n",
    "        break \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04292714534483457"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5914622550139174"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5775934713931352"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(Y, Yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.isdir('/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/code'"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join('/', 'code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/code'"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join('/', '/code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
