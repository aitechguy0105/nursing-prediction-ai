{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add in demographics, other features to Infinity model\n",
    "Use prior features for convenience. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "from pathlib import Path\n",
    "import gc\n",
    "import mlflow\n",
    "from mlflow import log_metric, log_param, log_artifact, set_tag\n",
    "import lightgbm as lgb \n",
    "from typing import Any\n",
    "from dataclasses import dataclass\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/src')\n",
    "from shared import perf_utils\n",
    "from shared.load_raw_data import load_raw_data_from_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = load_raw_data_from_files('/data/raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/data/processed/prediction_times.pkl\", 'rb') as f_in: \n",
    "    prediction_times = pkl.load(f_in)\n",
    "prediction_times.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data/processed/final_colnames.pkl', 'rb') as f_in: all_feature_names = pkl.load(f_in)\n",
    "all_features = scipy.sparse.load_npz('/data/processed/final_csr.npz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hstack returns a coo matrix, so convert back to csr. \n",
    "features = scipy.sparse.csr_matrix(all_features)\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign target values.\n",
    "4 day rehosp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign targets\n",
    "\n",
    "stays = data_dict['stays']\n",
    "\n",
    "indices = prediction_times.stayrowindex.values\n",
    "dates_of_transfer = stays.dateoftransfer.dt.date.values[indices]\n",
    "prediction_dates = prediction_times.predictiontimestamp.dt.date.values\n",
    "\n",
    "# substract PredictionTimestamp from DateOfTransfer \n",
    "time_diffs = [ transfer_date - predict_date for transfer_date, predict_date in zip(dates_of_transfer, prediction_dates)]\n",
    "time_diffs_in_days = [diff.days if type(diff) is datetime.timedelta else 1e6 for diff in time_diffs]\n",
    "\n",
    "time_diffs_in_days = np.array(time_diffs_in_days)\n",
    "print(np.sum(time_diffs_in_days <= 4))\n",
    "print(np.mean(time_diffs_in_days <= 4))\n",
    "\n",
    "mask = (time_diffs_in_days <= 4)\n",
    "target = np.zeros(len(time_diffs_in_days))\n",
    "target[mask] = 1\n",
    "print(np.mean(target))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do splits into train, val, test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_start_date = '2017-01-01'\n",
    "train_end_date = '2019-07-31'\n",
    "val_end_date = '2019-11-30'\n",
    "test_end_date = '2020-02-28'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deal with lookback period for stays features... \n",
    "We do a 90 day look back for the stays features, so we should be getting rid of prediction times with start dates before 2017-01-01 + 90 days... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stay_indices = prediction_times.stayrowindex.values\n",
    "start_dates = stays.startdate[stay_indices]\n",
    "td = pd.to_timedelta('90 days')\n",
    "min_date = pd.Timestamp(train_start_date) + td\n",
    "min_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Getting masks')\n",
    "train_mask = (prediction_times.predictiontimestamp.dt.date.values >= min_date) & \\\n",
    "             (prediction_times.predictiontimestamp.dt.date.values <= pd.to_datetime(train_end_date))\n",
    "val_mask = (prediction_times.predictiontimestamp.dt.date.values > pd.to_datetime(train_end_date)) & \\\n",
    "           (prediction_times.predictiontimestamp.dt.date.values <= pd.to_datetime(val_end_date))\n",
    "test_mask = prediction_times.predictiontimestamp.dt.date.values > pd.to_datetime(val_end_date)\n",
    "np.sum(train_mask), np.sum(val_mask), np.sum(test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = features[train_mask,:]\n",
    "train_y = target[train_mask]\n",
    "train_ptimes = prediction_times[train_mask]\n",
    "train_ptimes = train_ptimes.reset_index(drop=True)\n",
    "train_dict = {'X': train_x, 'Y': train_y, 'PredictionTimes': train_ptimes}\n",
    "print(train_x.shape)\n",
    "\n",
    "val_x = features[val_mask,:]\n",
    "val_y = target[val_mask]\n",
    "val_ptimes = prediction_times[val_mask]\n",
    "val_ptimes = val_ptimes.reset_index(drop=True)\n",
    "val_dict = {'X':val_x, 'Y': val_y, 'PredictionTimes': val_ptimes}\n",
    "print(val_x.shape)\n",
    "\n",
    "test_x = features[test_mask,:]\n",
    "test_y = target[test_mask]\n",
    "test_ptimes = prediction_times[test_mask]\n",
    "test_ptimes = test_ptimes.reset_index(drop=True)\n",
    "test_dict = {'X': test_x, 'Y': test_y, 'PredictionTimes': test_ptimes}\n",
    "print(test_x.shape)\n",
    "\n",
    "with open('/data/processed/more_features_train_dataset.pkl', 'wb') as f_out: \n",
    "    pkl.dump(train_dict, file=f_out)\n",
    "with open('/data/processed/more_features_val_dataset.pkl', 'wb') as f_out: \n",
    "    pkl.dump(val_dict, file=f_out)\n",
    "with open('/data/processed/more_features_test_dataset.pkl', 'wb') as f_out: \n",
    "    pkl.dump(test_dict, file=f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(train_y), np.mean(val_y), np.mean(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = lgb.Dataset(train_x, label=(train_y))\n",
    "valid_data = lgb.Dataset(val_x, label=(val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "    'boosting': 'gbdt', \n",
    "    'objective': 'binary',\n",
    "    'metric': ['auc', 'binary'],\n",
    "    'learning_rate': 0.02,\n",
    "    'n_estimators': 1000,\n",
    "    'max_depth': -1, \n",
    "    'num_threads': 32,\n",
    "    'num_leaves': 63, \n",
    "    'verbose': 3,\n",
    "    'two_round': True, \n",
    "    'early_stopping_rounds': 10,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an ML-flow experiment\n",
    "\n",
    "mlflow.set_tracking_uri('http://mlflow.saiva-dev')\n",
    "# Experiment name which appears in ML flow \n",
    "mlflow.set_experiment('edge-3-day-hosp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BaseModel:\n",
    "    model_name: str\n",
    "    model_type: str\n",
    "    model: Any \n",
    "\n",
    "base_models = []        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "    print(f'Training LGB models with parameter: {params}')\n",
    "    with mlflow.start_run():\n",
    "        run_uuid = mlflow.active_run().info.run_uuid\n",
    "        for param in params:\n",
    "            log_param(param, params[param])\n",
    "        set_tag('model', 'lgb')\n",
    "        print(\"=============================Training started...=============================\")\n",
    "        model = lgb.train(params, \n",
    "                          train_set=train_data, \n",
    "                          valid_sets=[valid_data])\n",
    "        \n",
    "        print(\"=============================Training completed...=============================\")\n",
    "        gc.collect()\n",
    "        \n",
    "        # ===========================Predict on valdation dataset=======================\n",
    "        test_yhat = model.predict(test_x)\n",
    "        print(test_yhat.shape)\n",
    "        print(\"=============================Prediction completed...=============================\")\n",
    "        \n",
    "        test_auroc = roc_auc_score(test_y, test_yhat)\n",
    "        test_ap = average_precision_score(test_y, test_yhat)\n",
    "        saiva_recall = perf_utils.saiva_recall_at_top_K(test_y, test_yhat, test_ptimes)\n",
    "        recall = perf_utils.recall_at_top_K(test_y, test_yhat, test_ptimes)\n",
    "\n",
    "        log_metric('Saiva_recall_at_top_15', saiva_recall)\n",
    "        log_metric('Recall_at_top_15', recall)\n",
    "        log_metric('Test_set_AUROC', test_auroc)\n",
    "        log_metric('Test_set_AUPRC', test_ap)\n",
    "        \n",
    "        \n",
    "        base_model = BaseModel(model_name=run_uuid, model_type='lgb', model=model)\n",
    "        base_models.append(base_model)\n",
    "        # ================= Save model related artifacts =========================\n",
    "        with open(f'./{run_uuid}.pickle', 'wb') as f: pkl.dump(base_model, f)\n",
    "        log_artifact(f'./{run_uuid}.pickle')\n",
    "\n",
    "        input_features = pd.DataFrame(all_feature_names, columns=['feature'])\n",
    "        input_features.to_csv(f'./input_features.csv', index=False)\n",
    "        log_artifact(f'./input_features.csv')\n",
    "\n",
    "        # =============== Save the code used to training in S3 ======================\n",
    "        for notebook in list(Path('/src/notebooks').glob('0*.ipynb')):\n",
    "            log_artifact(str(notebook))\n",
    "            \n",
    "        for shared_code in list(Path('/src/shared').glob('*.py')):\n",
    "            log_artifact(str(shared_code))\n",
    "            \n",
    "        for client_code in list(Path('/src/clients').glob('*.py')):\n",
    "            log_artifact(str(client_code))\n",
    "            \n",
    "        # =============== Save the diagnoses & meds used during training ======================\n",
    "        log_artifact(f'/data/processed/diagnoses_codes.pkl')\n",
    "        log_artifact(f'/data/processed/med_codes.pkl')\n",
    "        log_artifact(f'/data/processed/vital_bins.pkl')\n",
    "        \n",
    "objective(lgb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_by_patient, stats_patient = perf_utils.bootstrap_confidence_intervals(test_y, test_yhat, test_ptimes, B=500, sample_by='patient')\n",
    "stats_by_patient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### =======================================END=========================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results - ablations\n",
    "After getting baseline with all new features, take out stays, then notes. These cells are the results only. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results for all features except stay features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_yhat = model.predict(test_x)\n",
    "print(test_yhat.shape)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "test_auroc = roc_auc_score(test_y, test_yhat)\n",
    "test_ap = average_precision_score(test_y, test_yhat)\n",
    "saiva_recall = perf_utils.saiva_recall_at_top_K(test_y, test_yhat, test_ptimes)\n",
    "recall = perf_utils.recall_at_top_K(test_y, test_yhat, test_ptimes)\n",
    "\n",
    "print(f\"Saiva recall at top 15: {saiva_recall}\")\n",
    "print(f\"Recall at top 15: {recall}\")\n",
    "print(f'Test set AUROC: {test_auroc}')\n",
    "print(f'Test set AUPRC: {test_ap}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_by_patient, stats_patient = perf_utils.bootstrap_confidence_intervals(test_y, test_yhat, test_ptimes, B=200, sample_by='patient')\n",
    "stats_by_patient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results for all features minus notes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_yhat = model.predict(test_x)\n",
    "print(test_yhat.shape)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "test_auroc = roc_auc_score(test_y, test_yhat)\n",
    "test_ap = average_precision_score(test_y, test_yhat)\n",
    "saiva_recall = perf_utils.saiva_recall_at_top_K(test_y, test_yhat, test_ptimes)\n",
    "recall = perf_utils.recall_at_top_K(test_y, test_yhat, test_ptimes)\n",
    "\n",
    "print(f\"Saiva recall at top 15: {saiva_recall}\")\n",
    "print(f\"Recall at top 15: {recall}\")\n",
    "print(f'Test set AUROC: {test_auroc}')\n",
    "print(f'Test set AUPRC: {test_ap}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_by_patient, stats_patient = perf_utils.bootstrap_confidence_intervals(test_y, test_yhat, test_ptimes, B=500, sample_by='patient')\n",
    "stats_by_patient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results for just stays\n",
    "Cuz why now see... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_yhat = model.predict(test_x)\n",
    "print(test_yhat.shape)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "test_auroc = roc_auc_score(test_y, test_yhat)\n",
    "test_ap = average_precision_score(test_y, test_yhat)\n",
    "saiva_recall = perf_utils.saiva_recall_at_top_K(test_y, test_yhat, test_ptimes)\n",
    "recall = perf_utils.recall_at_top_K(test_y, test_yhat, test_ptimes)\n",
    "\n",
    "print(f\"Saiva recall at top 15: {saiva_recall}\")\n",
    "print(f\"Recall at top 15: {recall}\")\n",
    "print(f'Test set AUROC: {test_auroc}')\n",
    "print(f'Test set AUPRC: {test_ap}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_by_patient, stats_patient = perf_utils.bootstrap_confidence_intervals(test_y, test_yhat, test_ptimes, B=500, sample_by='patient')\n",
    "stats_by_patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}