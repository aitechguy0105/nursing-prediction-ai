{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import timeit\n",
    "import gc\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import mlflow\n",
    "from mlflow import log_metric, log_param, log_artifact, set_tag\n",
    "import lightgbm as lgb\n",
    "\n",
    "from hyperopt import tpe, fmin\n",
    "from dataclasses import dataclass\n",
    "from typing import Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_path = Path('/data/processed')\n",
    "processed_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(processed_path/'03-train_x.pickle','rb') as f: train_x = pickle.load(f)\n",
    "with open(processed_path/'03-train_target_3_day.pickle','rb') as f: train_target_3_day = pickle.load(f)\n",
    "with open(processed_path/'03-train_target_7_day.pickle','rb') as f: train_target_7_day = pickle.load(f)\n",
    "with open(processed_path/'03-train_idens.pickle','rb') as f: train_idens = pickle.load(f)\n",
    "\n",
    "with open(processed_path/'03-valid_x.pickle','rb') as f: valid_x = pickle.load(f)\n",
    "with open(processed_path/'03-valid_target_3_day.pickle','rb') as f: valid_target_3_day = pickle.load(f)\n",
    "with open(processed_path/'03-valid_target_7_day.pickle','rb') as f: valid_target_7_day = pickle.load(f)\n",
    "with open(processed_path/'03-valid_idens.pickle','rb') as f: valid_idens = pickle.load(f)\n",
    "    \n",
    "with open(processed_path/'03-test_x.pickle','rb') as f: test_x = pickle.load(f)\n",
    "with open(processed_path/'03-test_target_3_day.pickle','rb') as f: test_target_3_day = pickle.load(f)\n",
    "with open(processed_path/'03-test_target_7_day.pickle','rb') as f: test_target_7_day = pickle.load(f)\n",
    "with open(processed_path/'03-test_idens.pickle','rb') as f: test_idens = pickle.load(f)\n",
    "    \n",
    "with open(processed_path/'03-na_filler.pickle', 'rb') as f: na_filler = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = lgb.Dataset(train_x, label=train_target_3_day)\n",
    "valid_data = lgb.Dataset(valid_x, label=valid_target_3_day)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ============= Set the Experiment name correctly ============= "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an ML-flow experiment\n",
    "\n",
    "mlflow.set_tracking_uri('http://mlflow.saiva-dev')\n",
    "# Experiment name which appears in ML flow \n",
    "mlflow.set_experiment('trio_baseline_vital')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_at_k(group):\n",
    "    group.loc[:, \"hospitalized_cumsum\"] = group.hospitalized_within_pred_range.cumsum()\n",
    "    group.loc[:, \"total_relevant\"] = group.hospitalized_within_pred_range.sum()\n",
    "    group.loc[:, \"recall_at_k\"] = group.hospitalized_cumsum / group.total_relevant\n",
    "\n",
    "    return group.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile parameterTunningConfig.py\n",
    "# This cell just creates a python file containing the contents of this cell\n",
    "\n",
    "from hyperopt import hp\n",
    "\n",
    "# Parameter tunning\n",
    "lgb_param_space = {\n",
    " 'application': 'binary',\n",
    " 'objective': 'binary',\n",
    " 'metric': 'auc',\n",
    " #'boosting_type': hp.choice('boosting_type', ['gbdt']),\n",
    " #'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1),\n",
    " 'learning_rate': hp.uniform('learning_rate', 0.001, 0.05),\n",
    " #'max_depth': hp.quniform('max_depth', -1, 10, 1),\n",
    " #'min_child_samples': 20,\n",
    " #'min_child_weight': 0.001,\n",
    " #'min_split_gain': 0.0,\n",
    " 'n_estimators': hp.quniform('n_estimators',100,400,10),\n",
    " 'n_jobs': -1,\n",
    "#  'num_leaves': hp.quniform('num_leaves', 30, 300, 1),\n",
    " #'subsample': hp.uniform('subsample', 0, 1),\n",
    " #'subsample_for_bin': hp.quniform('subsample_for_bin', 200000, 500000, 1000),\n",
    " 'verbose': 3,\n",
    " 'is_unbalance': hp.choice('is_unbalance', [True, False]),\n",
    " #'max_bin': hp.quniform('max_bin', 100,1000, 100),\n",
    " 'early_stopping_round': None,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run parameterTunningConfig.py\n",
    "\n",
    "# Execute the python file stored earlier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BaseModel:\n",
    "    model_name: str\n",
    "    model_type: str\n",
    "    model: Any\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_models = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_beta_score(precision, recall, beta=2):\n",
    "    return ((1+beta**2)*(precision*recall)) / ((beta**2)*precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def objective(params):\n",
    "    print(f'Training LGB models with parameter: {params}')\n",
    "    with mlflow.start_run():\n",
    "        run_uuid = mlflow.active_run().info.run_uuid\n",
    "        \n",
    "        #params['num_leaves'] = int(params.get('num_leaves'))\n",
    "        params['n_estimators'] = int(params.get('n_estimators'))\n",
    "        #params['max_depth'] = int(params.get('max_depth'))\n",
    "        #params['subsample_for_bin'] = int(params.get('subsample_for_bin'))\n",
    "        #params['max_bin'] = int(params.get('max_bin'))\n",
    "\n",
    "        for param in params:\n",
    "            log_param(param, params[param])\n",
    "\n",
    "        set_tag('model', 'lgb')\n",
    "        print(\"=============================Training started...=============================\")\n",
    "        model = lgb.train(params, train_set=train_data, valid_sets=[valid_data])\n",
    "        \n",
    "        print(\"=============================Training completed...=============================\")\n",
    "        gc.collect()\n",
    "\n",
    "        # ===========================Predict on valdation dataset=======================\n",
    "        valid_preds = model.predict(valid_x)\n",
    "        print(\"=============================Prediction completed...=============================\")\n",
    "        \n",
    "        total_valid_aucroc = roc_auc_score(valid_target_3_day, valid_preds)\n",
    "        total_valid_aucroc_25_fpr = roc_auc_score(valid_target_3_day, valid_preds, max_fpr=0.25)\n",
    "        total_valid_ap = average_precision_score(valid_target_3_day, valid_preds)\n",
    "        agg_recall_to_optimize = None\n",
    "\n",
    "        log_metric('total_valid_aucroc', total_valid_aucroc)\n",
    "        log_metric('total_valid_ap', total_valid_ap)\n",
    "        log_metric('total_valid_aucroc_at_.25_fpr', total_valid_aucroc_25_fpr)\n",
    "\n",
    "        valid_base = valid_idens.copy()\n",
    "        valid_base['predictionvalue'] = valid_preds\n",
    "        valid_base['hospitalized_within_pred_range'] = valid_target_3_day\n",
    "        valid_base['predictionrank'] = valid_base.groupby(['censusdate', 'facilityid']).predictionvalue.rank(ascending=False)\n",
    "        valid_base = valid_base.sort_values('predictionrank', ascending=True)\n",
    "\n",
    "        performance_base = (\n",
    "            valid_base.groupby([\"facilityid\", \"censusdate\"])\n",
    "            .apply(precision_recall_at_k)\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "        \n",
    "        facility_pats = performance_base.groupby(['censusdate','facilityid']\n",
    "                        ).predictionrank.max().reset_index().groupby('facilityid').predictionrank.median().reset_index()\n",
    "\n",
    "        \n",
    "        for facilityid in sorted(valid_idens.facilityid.unique()):\n",
    "            mask = (valid_idens.facilityid == facilityid).values\n",
    "            k_at_10_percent = round(facility_pats.loc[facility_pats.facilityid == facilityid].predictionrank * .1).values[0]\n",
    "            k_at_15_percent = round(facility_pats.loc[facility_pats.facilityid == facilityid].predictionrank * .15).values[0]\n",
    "            k_at_20_percent = round(facility_pats.loc[facility_pats.facilityid == facilityid].predictionrank * .2).values[0]\n",
    "\n",
    "            rank_subset = performance_base.loc[(performance_base.facilityid==facilityid)]\n",
    "            try:\n",
    "                \n",
    "                log_metric(f'facility_{facilityid}_valid_aucroc', roc_auc_score(valid_target_3_day[mask], valid_preds[mask]))\n",
    "                log_metric(f'facility_{facilityid}_valid_aucroc_at_.25_fpr', roc_auc_score(valid_target_3_day[mask], valid_preds[mask], max_fpr=0.25))\n",
    "                log_metric(f'facility_{facilityid}_valid_ap', average_precision_score(valid_target_3_day[mask],valid_preds[mask]))\n",
    "\n",
    "                agg_recall_at_10_percent = (\n",
    "                    rank_subset.loc[rank_subset.predictionrank == k_at_10_percent].hospitalized_cumsum.sum() / rank_subset.loc[rank_subset.predictionrank == k_at_10_percent].total_relevant.sum()\n",
    "                )\n",
    "                agg_precision_at_10_percent = (\n",
    "                    rank_subset.loc[rank_subset.predictionrank == k_at_10_percent].hospitalized_cumsum.sum() / k_at_10_percent)\n",
    "                \n",
    "                agg_recall_at_15_percent = (\n",
    "                    rank_subset.loc[rank_subset.predictionrank == k_at_15_percent].hospitalized_cumsum.sum() / rank_subset.loc[rank_subset.predictionrank == k_at_15_percent].total_relevant.sum()\n",
    "                )\n",
    "                \n",
    "                agg_precision_at_15_percent = (\n",
    "                    rank_subset.loc[rank_subset.predictionrank == k_at_15_percent].hospitalized_cumsum.sum() / k_at_15_percent)\n",
    "\n",
    "                agg_recall_at_20_percent = (\n",
    "                    rank_subset.loc[rank_subset.predictionrank == k_at_20_percent].hospitalized_cumsum.sum() / rank_subset.loc[rank_subset.predictionrank == k_at_20_percent].total_relevant.sum()\n",
    "                )\n",
    "                \n",
    "                agg_precision_at_20_percent = (\n",
    "                    rank_subset.loc[rank_subset.predictionrank == k_at_20_percent].hospitalized_cumsum.sum() / k_at_20_percent\n",
    "                )\n",
    "                \n",
    "                log_metric(f'facility_{facilityid}_agg_recall_at_10_percent', agg_recall_at_10_percent)\n",
    "                log_metric(f'facility_{facilityid}_agg_b-score_at_10_percent', \n",
    "                           f_beta_score(agg_precision_at_10_percent, agg_recall_at_10_percent))\n",
    "                log_metric(f'facility_{facilityid}_agg_recall_at_15_percent', agg_recall_at_15_percent)\n",
    "                log_metric(f'facility_{facilityid}_agg_b-score_at_15_percent', \n",
    "                           f_beta_score(agg_precision_at_15_percent, agg_recall_at_15_percent))\n",
    "                log_metric(f'facility_{facilityid}_agg_recall_at_20_percent', agg_recall_at_20_percent)\n",
    "                log_metric(f'facility_{facilityid}_agg_b-score_at_20_percent', \n",
    "                           f_beta_score(agg_precision_at_20_percent, agg_recall_at_20_percent))\n",
    "                \n",
    "                if params.get('facility_to_optimize_for') == facilityid:\n",
    "                    agg_recall_to_optimize = agg_recall_at_15_percent\n",
    "                \n",
    "            except Exception as e:\n",
    "                # workaround for infinity-benchmark because you cannot calculate facility level\n",
    "                # metric for one facility.  This workaround will just skip calculating that\n",
    "                # facility level metric - it will print the exception, but continue\n",
    "                print(e)\n",
    "                continue\n",
    "\n",
    "\n",
    "        base_model = BaseModel(model_name=run_uuid, model_type='lgb', model=model)\n",
    "        base_models.append(base_model)\n",
    "        \n",
    "        # ================= Save model related artifacts =========================\n",
    "        with open(f'./{run_uuid}.pickle', 'wb') as f: pickle.dump(base_model, f)\n",
    "        log_artifact(f'./{run_uuid}.pickle')\n",
    "\n",
    "        input_features = pd.DataFrame(train_x.columns, columns=['feature'])\n",
    "        input_features.to_csv(f'./input_features.csv', index=False)\n",
    "        log_artifact(f'./input_features.csv')\n",
    "\n",
    "        with open('./na_filler.pickle','wb') as f: pickle.dump(na_filler, f, protocol=4)\n",
    "        log_artifact('./na_filler.pickle')\n",
    "\n",
    "        # =============== Save the code used to training in S3 ======================\n",
    "        for notebook in list(Path('/src/notebooks').glob('0*.ipynb')):\n",
    "            log_artifact(str(notebook))\n",
    "            \n",
    "        for shared_code in list(Path('/src/shared').glob('*.py')):\n",
    "            log_artifact(str(shared_code))\n",
    "            \n",
    "        for client_code in list(Path('/src/clients').glob('*.py')):\n",
    "            log_artifact(str(client_code))\n",
    " \n",
    "        log_artifact('./parameterTunningConfig.py')\n",
    "            \n",
    "        if agg_recall_to_optimize is not None:\n",
    "            return 1 - agg_recall_to_optimize\n",
    "        else:\n",
    "            return 1 - total_valid_aucroc\n",
    "\n",
    "start_time = timeit.default_timer()        \n",
    "best = fmin(fn=objective,\n",
    "        space=lgb_param_space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=10)\n",
    "print(f\"==============Time taken for training {timeit.default_timer() - start_time}======================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
