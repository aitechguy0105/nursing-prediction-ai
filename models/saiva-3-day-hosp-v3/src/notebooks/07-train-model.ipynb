{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import sys\n",
    "import timeit\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "from hyperopt import tpe, fmin\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "import mlflow\n",
    "from mlflow import log_metric, log_param, log_artifact, set_tag\n",
    "\n",
    "sys.path.insert(0, '/src')\n",
    "from dataclasses import dataclass\n",
    "from typing import Any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shared.constants import LOCAL_TRAINING_CONFIG_PATH\n",
    "from shared.utils import load_config\n",
    "\n",
    "config = load_config(LOCAL_TRAINING_CONFIG_PATH)\n",
    "training_config = config.training_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "processed_path = Path('/data/processed')\n",
    "processed_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(processed_path/'final-train_x.pickle','rb') as f: train_x = pickle.load(f)\n",
    "with open(processed_path/'final-train_target_3_day.pickle','rb') as f: train_target_3_day = pickle.load(f)\n",
    "with open(processed_path/'final-train_target_7_day.pickle','rb') as f: train_target_7_day = pickle.load(f)\n",
    "with open(processed_path/'final-train_idens.pickle','rb') as f: train_idens = pickle.load(f)\n",
    "\n",
    "with open(processed_path/'final-valid_x.pickle','rb') as f: valid_x = pickle.load(f)\n",
    "with open(processed_path/'final-valid_target_3_day.pickle','rb') as f: valid_target_3_day = pickle.load(f)\n",
    "with open(processed_path/'final-valid_target_7_day.pickle','rb') as f: valid_target_7_day = pickle.load(f)\n",
    "with open(processed_path/'final-valid_idens.pickle','rb') as f: valid_idens = pickle.load(f)\n",
    "\n",
    "with open(processed_path/'final-test_x.pickle','rb') as f: test_x = pickle.load(f)\n",
    "with open(processed_path/'final-test_target_3_day.pickle','rb') as f: test_target_3_day = pickle.load(f)\n",
    "with open(processed_path/'final-test_target_7_day.pickle','rb') as f: test_target_7_day = pickle.load(f)\n",
    "with open(processed_path/'final-test_idens.pickle','rb') as f: test_idens = pickle.load(f)\n",
    "\n",
    "with open(processed_path/'final-na_filler.pickle', 'rb') as f: na_filler = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('na_fillers column count: ', len(na_filler.keys()))\n",
    "print('feature column count: ', len(train_x.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_data = lgb.Dataset(train_x, label=train_target_3_day)\n",
    "valid_data = lgb.Dataset(valid_x, label=valid_target_3_day)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ============= Set the Experiment config correctly ============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "EXPERIMENT_DATES = training_config.training_metadata.experiment_dates\n",
    "CLIENT = \"+\".join([config.organization_id for config in training_config.ml_model_org_configs])\n",
    "vector_model = training_config.training_metadata.vector_model\n",
    "\n",
    "base_models = []\n",
    "MODEL_DESCRIPTION = f'{CLIENT}-3-day-hosp-v3'   # Name used to filter models in AWS quicksight\n",
    "TRAINING_DATA=CLIENT                            # trained on which data? e.g. avante + champion\n",
    "experiment_name = f'{CLIENT}-3-day-hosp-v3'     # ML Flow experiment name\n",
    "\n",
    "@dataclass\n",
    "class BaseModel:\n",
    "    model_name: str\n",
    "    model_type: str\n",
    "    model: Any\n",
    "\n",
    "EXPERIMENT_DATES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create an ML-flow experiment\n",
    "mlflow.set_tracking_uri('http://mlflow.saiva-dev')\n",
    "\n",
    "# Experiment name which appears in ML flow\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "EXPERIMENT = mlflow.get_experiment_by_name(experiment_name)\n",
    "MLFLOW_EXPERIMENT_ID = EXPERIMENT.experiment_id\n",
    "\n",
    "print(f'Experiment ID: {MLFLOW_EXPERIMENT_ID}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" Calculate how many transfers were caught up to a particular rank.\n",
    "hospital_cumsum - how many transfers caught upto a certain rank. Eg: Caught transfers till 10 th rank\n",
    "Relavant - total transfers per day per facility\n",
    "\"\"\"\n",
    "\n",
    "def precision_recall_at_k(group):\n",
    "    group.loc[:, \"hospitalized_cumsum\"] = group.hospitalized_within_pred_range.cumsum()\n",
    "    group.loc[:, \"total_relevant\"] = group.hospitalized_within_pred_range.sum()\n",
    "    group.loc[:, \"recall_at_k\"] = group.hospitalized_cumsum / group.total_relevant\n",
    "\n",
    "    return group.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile parameterTunningConfig.py\n",
    "# This cell just creates a python file containing the contents of this cell\n",
    "\n",
    "from hyperopt import hp\n",
    "\n",
    "# Parameter tunning\n",
    "lgb_param_space = {\n",
    " 'application': 'binary',\n",
    " 'objective': 'binary',\n",
    " 'metric': 'auc',\n",
    " #'boosting_type': hp.choice('boosting_type', ['gbdt']),\n",
    " #'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1),\n",
    " 'learning_rate': hp.uniform('learning_rate', 0.001, 0.05),\n",
    " #'max_depth': hp.quniform('max_depth', -1, 10, 1),\n",
    " #'min_child_samples': 20,\n",
    " #'min_child_weight': 0.001,\n",
    " #'min_split_gain': 0.0,\n",
    " 'n_estimators': hp.quniform('n_estimators',108,405,10),\n",
    " 'n_jobs': -1,\n",
    "#  'num_leaves': hp.quniform('num_leaves', 30, 300, 1),\n",
    " #'subsample': hp.uniform('subsample', 0, 1),\n",
    " #'subsample_for_bin': hp.quniform('subsample_for_bin', 200000, 500000, 1000),\n",
    " 'verbose': 3,\n",
    " 'is_unbalance': True,\n",
    " #'max_bin': hp.quniform('max_bin', 100,1000, 100),\n",
    " 'early_stopping_round': None,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%run parameterTunningConfig.py\n",
    "\n",
    "# Execute the python file stored earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def log_overall_model_auc(target_3_day, preds, dataset_type='valid'):\n",
    "    total_aucroc = roc_auc_score(target_3_day, preds)\n",
    "    total_aucroc_25_fpr = roc_auc_score(target_3_day, preds, max_fpr=0.25)\n",
    "    total_ap = average_precision_score(target_3_day, preds)\n",
    "\n",
    "    log_metric(f'total_{dataset_type}_aucroc', total_aucroc)\n",
    "    log_metric(f'total_{dataset_type}_ap', total_aucroc_25_fpr)\n",
    "    log_metric(f'total_{dataset_type}_aucroc_at_.25_fpr', total_ap)\n",
    "\n",
    "    return total_aucroc\n",
    "\n",
    "def performance_base_processing(idens, preds, target_3_day):\n",
    "    base = idens.copy()\n",
    "    base['predictionvalue'] = preds\n",
    "    base['hospitalized_within_pred_range'] = target_3_day\n",
    "    base['predictionrank'] = base.groupby(['censusdate', 'facilityid']).predictionvalue.rank(ascending=False)\n",
    "    base = base.sort_values('predictionrank', ascending=True)\n",
    "\n",
    "    performance_base = (\n",
    "        base.groupby([\"facilityid\", \"censusdate\"])\n",
    "        .apply(precision_recall_at_k)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # Get max rank per facility\n",
    "    facility_pats = performance_base.groupby(\n",
    "        ['censusdate','facilityid']\n",
    "    ).predictionrank.max().reset_index().groupby(\n",
    "        'facilityid'\n",
    "    ).predictionrank.median().reset_index()\n",
    "\n",
    "    return performance_base, facility_pats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def log_facility_wise_test_metrics(test_idens, facility_pats, performance_base):\n",
    "    total_facility_recall = 0\n",
    "    excluded_facility_count = 0\n",
    "    for facilityid in sorted(test_idens.facilityid.unique()):\n",
    "        rank_subset = performance_base.loc[(performance_base.facilityid==facilityid)]\n",
    "        facility_15_ranks = rank_subset.loc[rank_subset.predictionrank == 15]\n",
    "        # Facilities can have 0 transfers for the entire test-set which may result in 0 denominator\n",
    "        if facility_15_ranks.recall_at_k.count() > 0:\n",
    "            # Add all the recalls at a facility level\n",
    "            total_facility_recall += facility_15_ranks.recall_at_k.sum() / facility_15_ranks.recall_at_k.count()\n",
    "        else:\n",
    "            excluded_facility_count += 1\n",
    "\n",
    "    # Substract facilities which have 0 transfers for the entire test-set\n",
    "    recall = (total_facility_recall/(facility_pats.shape[0]-excluded_facility_count))\n",
    "    log_metric(f'total_test_recall_at_rank_15', recall)\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def log_facility_wise_valid_metrics(valid_idens, facility_pats, performance_base, valid_preds, params):\n",
    "    facility_auc_dict = {}\n",
    "    total_facility_recall = 0\n",
    "    excluded_facility_count = 0\n",
    "    agg_recall_to_optimize = None\n",
    "    for facilityid in sorted(valid_idens.facilityid.unique()):\n",
    "        mask = (valid_idens.facilityid == facilityid).values\n",
    "        k_at_10_percent = round(facility_pats.loc[facility_pats.facilityid == facilityid].predictionrank * .1).values[0]\n",
    "        k_at_15_percent = round(facility_pats.loc[facility_pats.facilityid == facilityid].predictionrank * .15).values[0]\n",
    "        k_at_20_percent = round(facility_pats.loc[facility_pats.facilityid == facilityid].predictionrank * .2).values[0]\n",
    "\n",
    "        rank_subset = performance_base.loc[(performance_base.facilityid==facilityid)]\n",
    "        try:\n",
    "            roc_auc = roc_auc_score(valid_target_3_day[mask], valid_preds[mask])\n",
    "\n",
    "            # Add facility wise AUC only if the facility is part of training client \n",
    "            if CLIENT in facilityid:\n",
    "                fid = int(facilityid.split('_')[1])\n",
    "                facility_auc_dict[fid] = roc_auc\n",
    "                \n",
    "            log_metric(f'facility_{facilityid}_valid_aucroc', roc_auc)\n",
    "            log_metric(f'facility_{facilityid}_valid_aucroc_at_.25_fpr', roc_auc_score(valid_target_3_day[mask], valid_preds[mask], max_fpr=0.25))\n",
    "            log_metric(f'facility_{facilityid}_valid_ap', average_precision_score(valid_target_3_day[mask],valid_preds[mask]))\n",
    "\n",
    "            # We know the total tranfers & caught tranfers upto a certain rank.\n",
    "            # Recall can be calculated as sum of all transfers upto a certian rank for entire given date range divided\n",
    "            # by sum of total tranfers per day for the entire given date range\n",
    "            facility_predicted_sum = rank_subset.loc[rank_subset.predictionrank == k_at_10_percent].hospitalized_cumsum.sum()\n",
    "            facility_transfered_sum = rank_subset.loc[rank_subset.predictionrank == k_at_10_percent].total_relevant.sum()\n",
    "            agg_recall_at_10_percent = facility_predicted_sum / facility_transfered_sum\n",
    "            agg_precision_at_10_percent = facility_predicted_sum / k_at_10_percent\n",
    "\n",
    "            facility_predicted_sum = rank_subset.loc[rank_subset.predictionrank == k_at_15_percent].hospitalized_cumsum.sum()\n",
    "            facility_transfered_sum = rank_subset.loc[rank_subset.predictionrank == k_at_15_percent].total_relevant.sum()\n",
    "            agg_recall_at_15_percent = facility_predicted_sum / facility_transfered_sum\n",
    "            agg_precision_at_15_percent = facility_predicted_sum / k_at_15_percent\n",
    "\n",
    "            facility_predicted_sum = rank_subset.loc[rank_subset.predictionrank == k_at_20_percent].hospitalized_cumsum.sum()\n",
    "            facility_transfered_sum = rank_subset.loc[rank_subset.predictionrank == k_at_20_percent].total_relevant.sum()\n",
    "            agg_recall_at_20_percent = facility_predicted_sum / facility_transfered_sum\n",
    "            agg_precision_at_20_percent = facility_predicted_sum / k_at_20_percent\n",
    "\n",
    "            facility_15_ranks = rank_subset.loc[rank_subset.predictionrank == 15]\n",
    "            # Facilities can have 0 transfers for the entire test-set which may result in 0 denominator\n",
    "            if facility_15_ranks.recall_at_k.count() > 0:\n",
    "                # Add all the recalls at a facility level\n",
    "                total_facility_recall += facility_15_ranks.recall_at_k.sum() / facility_15_ranks.recall_at_k.count()\n",
    "            else:\n",
    "                excluded_facility_count += 1\n",
    "\n",
    "            log_metric(f'facility_{facilityid}_agg_recall_at_10_percent', agg_recall_at_10_percent)\n",
    "            log_metric(f'facility_{facilityid}_agg_b-score_at_10_percent',\n",
    "                       f_beta_score(agg_precision_at_10_percent, agg_recall_at_10_percent))\n",
    "            log_metric(f'facility_{facilityid}_agg_recall_at_15_percent', agg_recall_at_15_percent)\n",
    "            log_metric(f'facility_{facilityid}_agg_b-score_at_15_percent',\n",
    "                       f_beta_score(agg_precision_at_15_percent, agg_recall_at_15_percent))\n",
    "            log_metric(f'facility_{facilityid}_agg_recall_at_20_percent', agg_recall_at_20_percent)\n",
    "            log_metric(f'facility_{facilityid}_agg_b-score_at_20_percent',\n",
    "                       f_beta_score(agg_precision_at_20_percent, agg_recall_at_20_percent))\n",
    "\n",
    "            if params.get('facility_to_optimize_for') == facilityid:\n",
    "                agg_recall_to_optimize = agg_recall_at_15_percent\n",
    "\n",
    "        except Exception as e:\n",
    "            # workaround for infinity-benchmark because you cannot calculate facility level\n",
    "            # metric for one facility.  This workaround will just skip calculating that\n",
    "            # facility level metric - it will print the exception, but continue\n",
    "            print(e)\n",
    "            continue\n",
    "\n",
    "    # divide total_facility_recall by total facilities\n",
    "    log_metric(f'total_valid_recall_at_rank_15', (total_facility_recall/(facility_pats.shape[0]-excluded_facility_count)))\n",
    "\n",
    "    return agg_recall_to_optimize, facility_auc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def f_beta_score(precision, recall, beta=2):\n",
    "    return ((1+beta**2)*(precision*recall)) / ((beta**2)*precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def objective(params):\n",
    "    print(f'Training LGB models with parameter: {params}')\n",
    "    with mlflow.start_run():\n",
    "        run_uuid = mlflow.active_run().info.run_uuid\n",
    "\n",
    "        #params['num_leaves'] = int(params.get('num_leaves'))\n",
    "        params['n_estimators'] = int(params.get('n_estimators'))\n",
    "        #params['max_depth'] = int(params.get('max_depth'))\n",
    "        #params['subsample_for_bin'] = int(params.get('subsample_for_bin'))\n",
    "        #params['max_bin'] = int(params.get('max_bin'))\n",
    "\n",
    "        # Log the train, validation & test date ranges\n",
    "        log_param('TRAIN_START_DATE', EXPERIMENT_DATES['train_start_date'])\n",
    "        log_param('TRAIN_END_DATE', EXPERIMENT_DATES['train_end_date'])\n",
    "        log_param('VALIDATION_START_DATE', EXPERIMENT_DATES['validation_start_date'])\n",
    "        log_param('VALIDATION_END_DATE', EXPERIMENT_DATES['validation_end_date'])\n",
    "        log_param('TEST_START_DATE', EXPERIMENT_DATES['test_start_date'])\n",
    "        log_param('TEST_END_DATE', EXPERIMENT_DATES['test_end_date'])\n",
    "\n",
    "        for param in params:\n",
    "            log_param(param, params[param])\n",
    "\n",
    "        set_tag('model', 'lgb')\n",
    "        print(\"=============================Training started...=============================\")\n",
    "        model = lgb.train(params, train_set=train_data, valid_sets=[valid_data])\n",
    "\n",
    "        print(\"=============================Training completed...=============================\")\n",
    "        gc.collect()\n",
    "\n",
    "        # ===========================Predict on valdation dataset=======================\n",
    "        valid_preds = model.predict(valid_x)\n",
    "        # ===========================Predict on test dataset=======================\n",
    "        test_preds = model.predict(test_x)\n",
    "        print(\"=============================Prediction completed...=============================\")\n",
    "\n",
    "        # =========================== TOTAL AUCROC on VALIDATION SET ===========================\n",
    "        total_valid_aucroc = log_overall_model_auc(valid_target_3_day, valid_preds, dataset_type='valid')\n",
    "        # =========================== TOTAL AUCROC on TEST SET ===========================\n",
    "        total_test_aucroc = log_overall_model_auc(test_target_3_day, test_preds, dataset_type='test')\n",
    "\n",
    "        performance_valid_base, facility_valid_pats = performance_base_processing(valid_idens, valid_preds, valid_target_3_day)\n",
    "        performance_test_base, facility_test_pats = performance_base_processing(test_idens, test_preds, test_target_3_day)\n",
    "\n",
    "        agg_recall_to_optimize, facility_auc_dict = log_facility_wise_valid_metrics(\n",
    "            valid_idens,\n",
    "            facility_valid_pats,\n",
    "            performance_valid_base,\n",
    "            valid_preds,\n",
    "            params\n",
    "        )\n",
    "        total_test_recall = log_facility_wise_test_metrics(\n",
    "            test_idens,\n",
    "            facility_test_pats,\n",
    "            performance_test_base\n",
    "        )\n",
    "        \n",
    "        model_config = {\n",
    "            'modelid':run_uuid,\n",
    "            'dayspredictionvalid':3,\n",
    "            'model_algo': 'lgbm',\n",
    "            'predictiontask': 'hospitalization',\n",
    "            'modeldescription' : MODEL_DESCRIPTION,\n",
    "            'client_data_trained_on' : TRAINING_DATA,\n",
    "            'vector_model' : vector_model,\n",
    "            'model_s3_folder': MLFLOW_EXPERIMENT_ID,\n",
    "            'prospectivedatestart':f'{datetime.now().date()}',\n",
    "            'training_start_date':EXPERIMENT_DATES['train_start_date'],\n",
    "            'training_end_date':EXPERIMENT_DATES['train_end_date'],\n",
    "            'validation_start_date': EXPERIMENT_DATES['validation_start_date'],\n",
    "            'validation_end_date': EXPERIMENT_DATES['validation_end_date'],\n",
    "            'test_start_date': EXPERIMENT_DATES['test_start_date'],\n",
    "            'test_end_date': EXPERIMENT_DATES['test_end_date'],\n",
    "            'test_auc': total_test_aucroc,\n",
    "            'test_recall_at_rank_15': total_test_recall,\n",
    "            'facility_wise_auc': facility_auc_dict,\n",
    "        }\n",
    "        \n",
    "        base_models.append(model_config)\n",
    "        \n",
    "        # ================= Save model related artifacts =========================\n",
    "        with open('./model_config.json', 'w') as outfile: json.dump(model_config, outfile)\n",
    "        log_artifact(f'./model_config.json')\n",
    "        \n",
    "        base_model = BaseModel(model_name=run_uuid, model_type='lgb', model=model)\n",
    "        with open(f'./{run_uuid}.pickle', 'wb') as f: pickle.dump(base_model, f)\n",
    "        log_artifact(f'./{run_uuid}.pickle')\n",
    "        \n",
    "        input_features = pd.DataFrame(train_x.columns, columns=['feature'])\n",
    "        input_features.to_csv(f'./input_features.csv', index=False)\n",
    "        log_artifact(f'./input_features.csv')\n",
    "        \n",
    "        with open('./na_filler.pickle','wb') as f: pickle.dump(na_filler, f, protocol=4)\n",
    "        log_artifact('./na_filler.pickle')\n",
    "\n",
    "        # =============== Save the code used to training in S3 ======================\n",
    "        for notebook in list(Path('/src/notebooks').glob('0*.ipynb')):\n",
    "            log_artifact(str(notebook))\n",
    "\n",
    "        for shared_code in list(Path('/src/shared').glob('*.py')):\n",
    "            log_artifact(str(shared_code))\n",
    "\n",
    "        for client_code in list(Path('/src/clients').glob('*.py')):\n",
    "            log_artifact(str(client_code))\n",
    "\n",
    "        log_artifact('./parameterTunningConfig.py')\n",
    "\n",
    "        if agg_recall_to_optimize is not None:\n",
    "            return 1 - agg_recall_to_optimize\n",
    "        else:\n",
    "            return 1 - total_valid_aucroc\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "best = fmin(fn=objective,\n",
    "        space=lgb_param_space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=9)\n",
    "print(f\"==============Time taken for training {timeit.default_timer() - start_time}======================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open('./model_config.json', 'w') as outfile: json.dump(base_models, outfile)\n",
    "    \n",
    "base_models    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
