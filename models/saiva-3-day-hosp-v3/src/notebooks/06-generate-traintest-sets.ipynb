{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.insert(0, '/src')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shared.constants import LOCAL_TRAINING_CONFIG_PATH\n",
    "from shared.utils import load_config\n",
    "\n",
    "config = load_config(LOCAL_TRAINING_CONFIG_PATH)\n",
    "training_config = config.training_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIENT = \"+\".join([config.organization_id for config in training_config.ml_model_org_configs])\n",
    "\n",
    "EXPERIMENT_DATES = training_config.training_metadata.experiment_dates\n",
    "\n",
    "print(CLIENT)\n",
    "print(EXPERIMENT_DATES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "processed_path = Path('/data/processed')\n",
    "processed_path.mkdir(parents=True, exist_ok=True)\n",
    "filename = 'final_cleaned_df.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# final = pd.read_parquet(processed_path/'02-result.parquet')\n",
    "final = pd.read_parquet(processed_path/f'{filename}')\n",
    "# Remove columns with names containing '\\t' or '\\n'\n",
    "cols_to_drop = [col for col in final.columns if '\\t' in col or '\\n' in col]\n",
    "final.drop(columns=cols_to_drop, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# column processing\n",
    "\n",
    "final['client'] = final['masterpatientid'].apply(lambda z: z.split('_')[0])\n",
    "final[\"facilityid\"] = final[\"client\"] + \"_\" + final[\"facilityid\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# manually fill in target columns with 0 so we don't also get na indicators for them\n",
    "final['hosp_target_3_day_hosp'] = final.hosp_target_3_day_hosp.fillna(False)\n",
    "final['hosp_target_7_day_hosp'] = final.hosp_target_7_day_hosp.fillna(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# manual check to make sure we're not including any columns that could leak data\n",
    "with open('/data/processed/columns.txt','w') as f:\n",
    "    for col in final.columns:\n",
    "        f.write(col + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_rows = final.shape[0]\n",
    "final_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.censusdate.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train = final.loc[final.censusdate < EXPERIMENT_DATES['validation_start_date']]\n",
    "valid = final.loc[(final.censusdate >= EXPERIMENT_DATES['validation_start_date']) & (final.censusdate <= EXPERIMENT_DATES['validation_end_date'])]\n",
    "test = final.loc[final.censusdate >= EXPERIMENT_DATES['test_start_date']]\n",
    "\n",
    "print(final.shape)\n",
    "print(train.shape)\n",
    "print(valid.shape)\n",
    "print(test.shape)\n",
    "# assert that the sum of rows of the 3 different dataframes is the same as the original\n",
    "assert final.shape[0] == (train.shape[0] + valid.shape[0] + test.shape[0])\n",
    "\n",
    "del final\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# start of basic tests - assert we have disjoint sets over time\n",
    "assert train.censusdate.max() < valid.censusdate.min()\n",
    "assert valid.censusdate.max() < test.censusdate.min()\n",
    "assert train.hosp_target_3_day_hosp.mean() < train.hosp_target_7_day_hosp.mean()\n",
    "assert valid.hosp_target_3_day_hosp.mean() < valid.hosp_target_7_day_hosp.mean()\n",
    "assert test.hosp_target_3_day_hosp.mean() < test.hosp_target_7_day_hosp.mean()\n",
    "print('Success...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f'Train set covers {train.censusdate.min()} to {train.censusdate.max()} with 3_day_hosp percentage {train.hosp_target_3_day_hosp.mean()} and 7_day_hosp percentage {train.hosp_target_7_day_hosp.mean()}')\n",
    "print(f'Valid set covers {valid.censusdate.min()} to {valid.censusdate.max()} with 3_day_hosp percentage {valid.hosp_target_3_day_hosp.mean()} and 7_day_hosp percentage {valid.hosp_target_7_day_hosp.mean()}')\n",
    "print(f'Test set covers {test.censusdate.min()} to {test.censusdate.max()} with 3_day_hosp percentage {test.hosp_target_3_day_hosp.mean()} and 7_day_hosp percentage {test.hosp_target_7_day_hosp.mean()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "import os\n",
    "\n",
    "def get_median(job_data):\n",
    "    col = job_data[0]\n",
    "    ser = job_data[1]\n",
    "    return ser.median()\n",
    "\n",
    "def fill_na_train(df):\n",
    "    \"\"\" Get Median value for all columns that contain NaN and all vital columns.\n",
    "    Store these Median values in a file in S3 & use them when ever a column has NaN during prediction\n",
    "    \"\"\"\n",
    "    nan_cols = []\n",
    "    for col in df.columns:\n",
    "        if col in ['bedid','admissionstatus','censusactioncode', 'payername', 'payercode','to_from_type']:\n",
    "            continue\n",
    "        if (df[col].isnull().any()) or (col.startswith((\"vtl\", \"hosp\"))):\n",
    "            nan_cols.append(col)\n",
    "\n",
    "    job_data = []\n",
    "    for col in nan_cols:\n",
    "        job_data.append((col,df[col]))\n",
    "\n",
    "    with Pool(min(os.cpu_count() - 4, 24)) as pool:\n",
    "        d = pool.map(get_median, job_data)\n",
    "\n",
    "    d = pd.Series(data=d, index=nan_cols)\n",
    "    df = df.fillna(d)\n",
    "\n",
    "    return df, d\n",
    "\n",
    "def fill_na_valid_or_test(df, na_filler):\n",
    "    return df.fillna(na_filler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# fill in any remaining na's - now that we're not forwardfilling past info it's not correct to use a global imputation\n",
    "# hence we impute on the train and apply to the valid and test\n",
    "# We save these na filler values to use them during predictions\n",
    "\n",
    "train, na_filler = fill_na_train(train)\n",
    "valid = fill_na_valid_or_test(valid, na_filler)\n",
    "test = fill_na_valid_or_test(test, na_filler)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Remove the Target values & identification columns\n",
    "def prep(df):\n",
    "    iden_cols = ['censusdate', 'masterpatientid', 'facilityid', 'bedid','admissionstatus',\n",
    "                 'censusactioncode', 'payername', 'payercode','to_from_type','client']\n",
    "    \n",
    "    drop_cols = iden_cols + [col for col in df.columns if 'target' in col]\n",
    "\n",
    "    target_3_day = df.hosp_target_3_day_hosp.astype('float32').values\n",
    "    target_7_day = df.hosp_target_7_day_hosp.astype('float32').values\n",
    "    x = df.drop(columns=drop_cols).reset_index(drop=True).astype('float32')\n",
    "    idens = df.loc[:,iden_cols]\n",
    "\n",
    "    return x, target_3_day, target_7_day, idens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Seperate target, x-frame and identification columns\n",
    "train_x, train_target_3_day, train_target_7_day, train_idens = prep(train)\n",
    "del train\n",
    "valid_x, valid_target_3_day, valid_target_7_day, valid_idens = prep(valid)\n",
    "del valid\n",
    "test_x, test_target_3_day, test_target_7_day, test_idens = prep(test)\n",
    "del test\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# make sure for that x's, targets, an idens all have the same # of rows\n",
    "assert train_x.shape[0] == train_target_3_day.shape[0] == train_target_7_day.shape[0] == train_idens.shape[0]\n",
    "assert valid_x.shape[0] == valid_target_3_day.shape[0] == valid_target_7_day.shape[0] == valid_idens.shape[0]\n",
    "assert test_x.shape[0] == test_target_3_day.shape[0] == test_target_7_day.shape[0] == test_idens.shape[0]\n",
    "\n",
    "# make sure that train, valid, and test have the same # of columns\n",
    "assert train_x.shape[1] == valid_x.shape[1] == test_x.shape[1]\n",
    "\n",
    "# make sure that the idens all have the same # of columns\n",
    "assert train_idens.shape[1] == valid_idens.shape[1] == test_idens.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Save train, test and validation datasets in local folder\n",
    "\n",
    "import pickle;\n",
    "with open(processed_path/'final-train_x.pickle','wb') as f: pickle.dump(train_x, f, protocol=4)\n",
    "with open(processed_path/'final-train_target_3_day.pickle','wb') as f: pickle.dump(train_target_3_day, f, protocol=4)\n",
    "with open(processed_path/'final-train_target_7_day.pickle','wb') as f: pickle.dump(train_target_7_day, f, protocol=4)\n",
    "with open(processed_path/'final-train_idens.pickle','wb') as f: pickle.dump(train_idens, f, protocol=4)\n",
    "\n",
    "with open(processed_path/'final-valid_x.pickle','wb') as f: pickle.dump(valid_x, f, protocol=4)\n",
    "with open(processed_path/'final-valid_target_3_day.pickle','wb') as f: pickle.dump(valid_target_3_day, f, protocol=4)\n",
    "with open(processed_path/'final-valid_target_7_day.pickle','wb') as f: pickle.dump(valid_target_7_day, f, protocol=4)\n",
    "with open(processed_path/'final-valid_idens.pickle','wb') as f: pickle.dump(valid_idens, f, protocol=4)\n",
    "\n",
    "with open(processed_path/'final-test_x.pickle','wb') as f: pickle.dump(test_x, f, protocol=4)\n",
    "with open(processed_path/'final-test_target_3_day.pickle','wb') as f: pickle.dump(test_target_3_day, f, protocol=4)\n",
    "with open(processed_path/'final-test_target_7_day.pickle','wb') as f: pickle.dump(test_target_7_day, f, protocol=4)\n",
    "with open(processed_path/'final-test_idens.pickle','wb') as f: pickle.dump(test_idens, f, protocol=4)\n",
    "\n",
    "with open(processed_path/'final-na_filler.pickle', 'wb') as f: pickle.dump(na_filler, f, protocol=4)\n",
    "\n",
    "print(\"--------------Completed--------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
